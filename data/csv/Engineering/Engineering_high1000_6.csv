Publication Type,Authors,Title,Abstract,DOI
J,"Liu, BD; Liu, YK",Expected value of fuzzy variable and fuzzy expected value models,"This paper will present a novel concept of expected values of fuzzy variables, which is essentially a type of Choquet integral and coincides with that of random variables. In order to calculate the expected value of general fuzzy variable, a fuzzy simulation technique is also designed. Finally, we construct a spectrum, of fuzzy expected value models,,and integrate fuzzy simulation, neural network, and genetic algorithms to produce a hybrid intelligent algorithm for solving general fuzzy expected value models.",10.1109/TFUZZ.2002.800692
J,"Broder, A; Kumar, R; Maghoul, F; Raghavan, P; Rajagopalan, S; Stata, R; Tomkins, A; Wiener, J",Graph structure in the Web,"The study of the Web as a graph is not only fascinating in its own right, but also yields valuable insight into Web algorithms for crawling, searching and community discovery, and the sociological phenomena which characterize its evolution. We report on experiments on local and global properties of the Web graph using two AltaVista crawls each with over 200 million pages and 1.5 billion links. Our study indicates that the macroscopic structure of the Web is considerably more intricate than suggested by earlier experiments on a smaller scale. (C) 2000 Published by Elsevier Science B.V. All rights reserved.",10.1016/S1389-1286(00)00083-9
J,"Arandjelovic, R; Gronat, P; Torii, A; Pajdla, T; Sivic, J",NetVLAD: CNN Architecture for Weakly Supervised Place Recognition,"We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following four principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the ""Vector of Locally Aggregated Descriptors"" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we create a new weakly supervised ranking loss, which enables end-to-end learning of the architecture's parameters from images depicting the same places over time downloaded from Google Street View Time Machine. Third, we develop an efficient training procedure which can be applied on very large-scale weakly labelled tasks. Finally, we show that the proposed architecture and training procedure significantly outperform non-learnt image representations and off-the-shelf CNN descriptors on challenging place recognition and image retrieval benchmarks.",10.1109/TPAMI.2017.2711011
J,"Taal, CH; Hendriks, RC; Heusdens, R; Jensen, J",An Algorithm for Intelligibility Prediction of Time-Frequency Weighted Noisy Speech,"In the development process of noise-reduction algorithms, an objective machine-driven intelligibility measure which shows high correlation with speech intelligibility is of great interest. Besides reducing time and costs compared to real listening experiments, an objective intelligibility measure could also help provide answers on how to improve the intelligibility of noisy unprocessed speech. In this paper, a short-time objective intelligibility measure (STOI) is presented, which shows high correlation with the intelligibility of noisy and time-frequency weighted noisy speech (e.g., resulting from noise reduction) of three different listening experiments. In general, STOI showed better correlation with speech intelligibility compared to five other reference objective intelligibility models. In contrast to other conventional intelligibility models which tend to rely on global statistics across entire sentences, STOI is based on shorter time segments (386 ms). Experiments indeed show that it is beneficial to take segment lengths of this order into account. In addition, a free Matlab implementation is provided.",10.1109/TASL.2011.2114881
J,"Zhu, QS; Mai, JM; Shao, L",A Fast Single Image Haze Removal Algorithm Using Color Attenuation Prior,"Single image haze removal has been a challenging problem due to its ill-posed nature. In this paper, we propose a simple but powerful color attenuation prior for haze removal from a single input hazy image. By creating a linear model for modeling the scene depth of the hazy image under this novel prior and learning the parameters of the model with a supervised learning method, the depth information can be well recovered. With the depth map of the hazy image, we can easily estimate the transmission and restore the scene radiance via the atmospheric scattering model, and thus effectively remove the haze from a single image. Experimental results show that the proposed approach outperforms state-of-the-art haze removal algorithms in terms of both efficiency and the dehazing effect.",10.1109/TIP.2015.2446191
J,"Warfield, SK; Zou, KH; Wells, WM",Simultaneous truth and performance level estimation (STAPLE): An algorithm for the validation of image segmentation,"Characterizing the performance of image segmentation approaches has been a persistent challenge. Performance analysis is important since segmentation algorithms often have limited accuracy and precision. Interactive drawing of the desired segmentation by human raters has often been the only acceptable approach, and yet suffers from intra-rater and inter-rater variability. Automated algorithms have been sought in order to remove the variability introduced by raters, but such algorithms must be assessed to ensure they are suitable for the task. The performance of raters (human or algorithmic) generating segmentations of medical images has been difficult to quantify because of the difficulty of obtaining or estimating a known true segmentation for clinical data. Although physical and digital phantoms can be constructed for which ground truth is known or readily estimated, such phantoms do not fully reflect clinical images due to the difficulty of constructing phantoms which reproduce the full range of imaging characteristics and normal and pathological anatomical variability observed in clinical data. Comparison to a collection of segmentations by raters is an attractive alternative since it can be carried out directly on the relevant clinical imaging data. However, the most appropriate measure or set of measures with which to compare such segmentations has not been clarified and several measures are used in practice. We present here an expectation-maximization algorithm for simultaneous truth and performance level estimation (STAPLE). The algorithm considers a collection of segmentations and computes a probabilistic estimate of the true segmentation and a measure of the performance level represented by each segmentation. The source of each segmentation in the collection may be an appropriately trained human rater or raters, or may be an automated segmentation algorithm. The probabilistic estimate of the true segmentation is formed by estimating an optimal combination of the segmentations, weighting each segmentation depending upon the estimated performance level, and incorporating a prior model for the spatial distribution of structures being segmented as well as spatial homogeneity constraints. STAPLE is straightforward to apply to clinical imaging data, it readily enables assessment of the performance of an automated image segmentation algorithm, and enables direct comparison of human rater and algorithm performance.",10.1109/TMI.2004.828354
J,"Song, CS","Global challenges and strategies for control, conversion and utilization of CO<sub>2</sub> for sustainable development involving energy, catalysis, adsorption and chemical processing","Utilization of carbon dioxide (CO2) has become an important global issue due to the significant and continuous rise in atmospheric CO2 concentrations, accelerated growth in the consumption of carbon-based energy worldwide, depletion of carbon-based energy resources, and low efficiency in current energy systems. The barriers for CO2 utilization include: (1) Costs Of CO2 capture, separation, purification, and transportation to user site; (2) energy requirements of CO2 chemical conversion (plus source and cost of co-reactants); (3) market size limitations, little investment-incentives and lack of industrial commitments for enhancing CO2-based chemicals; and (4) the lack of socio-economical driving forces. The strategic objectives may include: (1) use CO2 for environmentally-benign physical and chemical processing that adds value to the process; (2) use CO2 to produce industrially useful chemicals and materials that adds value to the products; (3) use CO2 as a beneficial fluid for processing or as a medium for energy recovery and emission reduction; and (4) use CO2 recycling involving renewable sources of energy to conserve carbon resources for sustainable development. The approaches for enhancing CO2 utilization may include one or more of the following: (1) for applications that do not require pure CO2, develop effective processes for using the CO2-Concentrated flue gas from industrial plants or CO2-rich resources without CO2 separation; (2) for applications that need pure CO2, develop more efficient and less-energy intensive processes for separation of CO2 selectively without the negative impacts of co-existing gases such as H2O, 02, and N-2; (3) replace a hazardous or less-effective substance in existing processes with CO2 as an alternate medium or solvent or co-reactant or a combination of them; (4) make use of CO2 based on the unique physical properties as supercritical fluid or as either solvent or anti-solvent; (5) use CO2 based on the unique chemical properties for CO2 to be incorporated with high 'atom efficiency' such as carboxylation and carbonate synthesis; (6) produce useful chemicals and materials using CO2 as a reactant or feedstock; (7) use CO2 for energy recovery while reducing its emissions to the atmosphere by sequestration; (8) recycle CO2 as C-source for chemicals and fuels using renewable sources of energy; and (9) convert CO2 under either bio-chemical or geologic-formation conditions into ""new fossil"" energies. Several cases are discussed in more detail. The first example is tri-reforming of methane versus the well-known CO2 reforming over transition metal catalysts such as supported Ni catalysts. Using CO2 along with H2O and 02 in flue gases of power plants without separation, tri-reforming is a synergetic combination of CO2 reforming, steam reforming and partial oxidation and it can eliminate carbon deposition problem and produces syngas with desired H-2/CO ratios for industrial applications. The second example is a CO2 ""molecular basket"" as CO2-selective high-capacity adsorbent which was developed using mesoporous molecular sieve MCM-41 and polyethylenimine (PEI). The MCM41-PEI adsorbent has higher adsorption capacity than either PEI or MCM-41 alon and can be used as highly CO2-selective adsorbent for gas mixtures without the pre-removal of moisture because it even enhances CO2 adsorption capacity. The third example is synthesis of dimethyl carbonate using CO2 and methanol, which demonstrates the environmental benefit of avoiding toxic phosgene and a processing advantage. The fourth example is the application of supercritical CO2 for extraction and for chemical processing where CO2 is either a solvent or a co-reactant, or both. The CO2 utilization contributes to enhancing sustainability, since various chemicals, materials, and fuels can be synthesized using CO2, which should be a sustainable way in the long term when renewable sources of energy are used as energy input. (c) 2006 Elsevier B.V. All rights reserved.",10.1016/j.cattod.2006.02.029
J,"Van Gerpen, J",Biodiesel processing and production,"Biodiesel is an alternative diesel fuel that is produced from vegetable oils and animal fats. It consists of the monoalkyl esters formed by a catalyzed reaction of the triglycerides in the oil or fat with a simple monohydric alcohol. The reaction conditions generally involve a trade-off between reaction time and temperature as reaction completeness is the most critical fuel quality parameter. Much of the process complexity originates from contaminants in the feedstock, such as water and free fatty acids, or impurities in the final product, such as methanol, free glycerol, and soap. Processes have been developed to produce biodiesel from high free fatty acid feedstocks, such as recycled restaurant grease, animal fats, and soapstock. (c) 2004 Elsevier B.V. All rights reserved.",10.1016/j.fuproc.2004.11.005
J,"Heinz, DC; Chang, CI",Fully constrained least squares linear spectral mixture analysis method for material quantification in hyperspectral imagery,"Linear spectral mixture analysis (LSMA) is a widely used technique in remote sensing to estimate abundance fractions of materials present in an image pixel. In order for an LSMA-based estimator to produce accurate amounts of material abundance, it generally requires two constraints imposed on the linear mixture model used in LSMA, which are the abundance sum-to-one constraint and the abundance nonnegativity constraint. The first constraint requires the sum of the abundance fractions of materials present in an image pixel to be one and the second imposes a constraint that these abundance fractions be nonnegative. While the first constraint is easy to deal with, the second constraint is difficult to implement since it results in a set of inequalities and can only be solved by numerical methods. Consequently, most LSMA-based methods are unconstrained and produce solutions that do not necessarily reflect the true abundance fractions of materials. In this case, they can only be used for the purposes of material detection, discrimination, and classification, but not for material quantification. In this paper, we present a fully constrained least squares (FCLS) linear spectral mixture analysis method for material quantification. Since no closed form can be derived for this method, an efficient algorithm is developed to yield optimal solutions. In order to further apply the designed algorithm to unknown image scenes, an unsupervised least squares error (LSE)-based method is also proposed to extend the FCLS method in an unsupervised manner. A series of computer simulations and real hyperspectral data experiments were conducted to demonstrate the performance of the proposed FCLS LSMA approach in material quantification.",10.1109/36.911111
J,"Reddy, JN",Analysis of functionally graded plates,"Theoretical formulation, Navier's solutions of rectangular plates, and finite element models based on the third-order shear deformation plate theory are presented for the analysis of through-thickness functionally graded plates. The plates are assumed to have isotropic, two-constituent material distribution through the thickness, and the modulus of elasticity of the plate is assumed to vary according to a power-law distribution in terms of the volume fractions of the constituents. The formulation accounts for the thermomechanical coupling, time dependency, and the von Karman-type geometric Iron-linearity. Numerical results of the linear third-order theory and non-linear first-order theory are presented to show the effect of the material distribution on the deflections and stresses. Copyright (C) 2000 John Wiley & Sons, Ltd.",10.1002/(SICI)1097-0207(20000110/30)47:1/3<663::AID-NME787>3.0.CO;2-8
J,"Jeffrey, SJ; Carter, JO; Moodie, KB; Beswick, AR",Using spatial interpolation to construct a comprehensive archive of Australian climate data,"A comprehensive archive of Australian rainfall and climate data has been constructed from ground-based observational data. Continuous, daily time step records have been constructed using spatial interpolation algorithms to estimate missing data. Datasets have been constructed for daily rainfall, maximum and minimum temperatures, evaporation, solar radiation and vapour pressure. Datasets are available for approximately 4600 locations across Australia, commencing in 1890 for rainfall and 1957 for climate variables. The datasets can be accessed on the Internet at http://www.dnr.qld.gov.au/silo. Interpolated surfaces have been computed on a regular 0.05 degrees grid extending from latitude 10 degreesS to 44 degreesS and longitude 112 degreesE to 154 degreesE. A thin plate smoothing spline was used to interpolate daily climate variables, and ordinary kriging was used to interpolate daily and monthly rainfall. Independent cross validation has been used to analyse the temporal and spatial error of the interpolated data. An Internet based facility has been developed which allows database clients to interrogate the gridded surfaces at any desired location. (C) 2001 Elsevier Science Ltd. All rights reserved.",10.1016/S1364-8152(01)00008-1
J,"Blankertz, B; Tomioka, R; Lemm, S; Kawanabe, M; Müller, KR",Optimizing spatial filters for robust EEG single-trial analysis,"Due to the volume conduction multichannel electroencephalogram (EEG) recordings give a rather blurred image of brain activity. Therefore spatial filters are extremely useful in single-trial analysis in order to improve the signal-to-noise ratio. There are powerful methods from machine learning and signal processing that permit the optimization of spatio-temporal filters for each subject in a data dependent fashion beyond the fixed filters based on the sensor geometry, e.g., Laplacians. Here we elucidate the theoretical background of the common spatial pattern (CSP) algorithm, a popular method in brain-computer interface (BCI) research. Apart from reviewing several variants of the basic algorithm, we reveal tricks of the trade for achieving a powerful CSP performance, briefly elaborate on theoretical aspects of CSP, and demonstrate the application of CSP-type preprocessing in our studies of the Berlin BCI (BBCI) project.",10.1109/MSP.2008.4408441
J,"Ball, JM; Lee, MM; Hey, A; Snaith, HJ",Low-temperature processed meso-superstructured to thin-film perovskite solar cells,"We have reduced the processing temperature of the bulk absorber layer in CH3NH3PbI3-xClx perovskite solar cells from 500 to <150 degrees C and achieved power conversion efficiencies up to 12.3%. Remarkably, we find that devices with planar thin-film architecture, where the ambipolar perovskite transports both holes and electrons, convert the absorbed photons into collected charge with close to 100% efficiency.",10.1039/c3ee40810h
J,"Goel, S; Negi, R",Guaranteeing secrecy using artificial noise,"The broadcast nature of the wireless medium makes the communication over this medium vulnerable to eavesdropping. This paper considers the problem of secret communication between two nodes, over a fading wireless medium, in the presence of a passive eavesdropper. The assumption used is that the transmitter and its helpers (amplifying relays) have more antennas than the eavesdropper. The transmitter ensures secrecy of communication by utilizing some of the available power to produce 'artificial noise', such that only the eavesdropper's channel is degraded. Two scenarios are considered, one where the transmitter has multiple transmit antennas, and the other where amplifying relays simulate the effect of multiple antennas. The channel state information (CSI) is assumed to be publicly known, and hence, the secrecy of communication is independent of the secrecy of CSI.",10.1109/TWC.2008.060848
J,"Zhao, GX; Li, JX; Ren, XM; Chen, CL; Wang, XK",Few-Layered Graphene Oxide Nanosheets As Superior Sorbents for Heavy Metal Ion Pollution Management,"Graphene has attracted multidisciplinary study because of its unique physicochemical properties. Herein, few-layered graphene oxide nanosheets were synthesized from graphite using the modified Hummers method, and were used as sorbents for the removal of Cd(II) and Co(II) ions from large volumes of aqueous solutions. The effects of pH, ionic strength, and humic acid on Cd(II) and Co (II) sorption were investigated. The results indicated that Cd(II) and Co(II) sorption on graphene oxide nanosheets was strongly dependent on pH and weakly dependent on ionic strength. The abundant oxygen-containing functional groups on the surfaces of graphene oxide nanosheets played an important role on Cd(II) and Co(II) sorption. The presence of humic acid reduced Cd(II) and Co(II) sorption on graphene oxide nanosheets at pH < 8. The maximum sorption capacities (C-smax) of Cd(II) and Co (II) on graphene oxide nanosheets at pH 6.0 +/- 0.1 and T = 303 K were about 106.3 and 68.2 mg/g, respectively, higher than any currently reported. The thermodynamic parameters calculated from temperature-dependent sorption isotherms suggested that Cd(II) and Co(II) sorptions on graphene oxide nanosheets were endothermic and spontaneous processes. The graphene oxide nanosheets may be suitable materials in heavy metal ion pollution cleanup if they are synthesized in large scale and at low price in near future.",10.1021/es203439v
J,"Doppler, K; Rinne, M; Wijting, C; Ribeiro, CB; Hugl, K",Device-to-Device Communication as an Underlay to LTE-Advanced Networks,"In this article device-to-device (D2D) communication underlaying a 3GPP LTE-Advanced cellular network is studied as an enabler of local services with limited interference impact on the primary cellular network. The approach of the study is a tight integration of D2D communication into an LTE-Advanced network. In particular, we propose mechanisms for D2D communication session setup and management involving procedures in the LTE System Architecture Evolution. Moreover, we present numerical results based on system simulations in an interference limited local area scenario. Our results show that D2D communication can increase the total throughput observed in the cell area.",10.1109/MCOM.2009.5350367
J,"Ruparelia, JP; Chatteriee, AK; Duttagupta, SP; Mukherji, S",Strain specificity in antimicrobial activity of silver and copper nanoparticles,"The antimicrobial properties of silver and copper nanoparticles were investigated using Escherichia coli (four strains), Bacillus subtilis and Staphylococcus aureus (three strains). The average sizes of the silver and copper nanoparticles were 3 nm and 9 nm, respectively, as determined through transmission electron microscopy. Energy-dispersive X-ray spectra of silver and copper nanoparticles revealed that while silver was in its pure form, an oxide layer existed on the copper nanoparticles. The bactericidal effect of silver and copper nanoparticles were compared based on diameter of inhibition zone in disk diffusion tests and minimum inhibitory concentration (MIC) and minimum bactericidal concentration (MBC) of nanoparticles dispersed in batch cultures. Bacterial sensitivity to nanoparticles was found to vary depending on the microbial species. Disk diffusion studies with E. coli and S. aureus revealed greater effectiveness of the silver nanoparticles compared to the copper nanoparticles. B. subtilis depicted the highest sensitivity to nanoparticles compared to the other strains and was more adversely affected by the copper nanoparticles. Good correlation was observed between MIC and MBC (r(2)=0.98) measured in liquid cultures. For copper nanoparticles a good negative correlation was observed between the inhibition zone observed in disk diffusion test and MIC/MBC determined based on liquid cultures with the various strains (r(2)=-0.75). Although strain-specific variation in MIC/MBC was negligible for S. aureus, some strain-specific variation was observed for E. coli. (c) 2007 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved.",10.1016/j.actbio.2007.11.006
J,"Fritzmann, C; Löwenberg, J; Wintgens, T; Melin, T",State-of-the-art of reverse osmosis desalination,"Throughout the world, water scarcity is being recognised as a present or future threat to human activity and as a consequence, a definite trend to develop alternative water resources such as desalination can be observed. The most commonly used desalination technologies are reverse osmosis (RO) and thermal processes such as multi-stage flash (MSF) and multi-effect distillation (MED). In Europe, reverse osmosis, due to its lower energy consumption has gained much wider acceptance than its thermal alternatives. This review summarises the current state-of-the art of reverse osmosis desalination, dealing not only with the reverse osmosis stage, but with the entire process from raw water intake to post treatment of product water. The discussion of process fundamentals, membranes and membrane modules and of current and future developments in membrane technology is accompanied by an analysis of operational issues as fouling and scaling and of measures for their prevention such as adequate cleaning procedures and antiscalant use. Special focus is placed on pre-treatment of raw water and post-treatment of brine as well as of product water to meet drinking and irrigation water standards, including evaluation of current boron removal options. Energy requirements of reverse osmosis plants as well as currently applied energy recovery systems for reduction of energy consumption are described and cost and cost structure of reverse osmosis desalination are outlined. Finally, current practices of waste management and disposal as well as new trends such as the use of hybrid plants, i.e. combining reverse osmosis with thermal processes and/or power generation are addressed.",10.1016/j.desal.2006.12.009
J,"Federici, JF; Schulkin, B; Huang, F; Gary, D; Barat, R; Oliveira, F; Zimdars, D","THz imaging and sensing for security applications - explosives, weapons and drugs","Over the past 5 years, there has been a significant interest in employing terahertz (THz) technology, spectroscopy and imaging for security applications. There are three prime motivations for this interest: (a) THz radiation can detect concealed weapons since many non-metallic, non-polar materials are transparent to THz radiation; (b) target compounds such as explosives and illicit drugs have characteristic THz spectra that can be used to identify these compounds and (c) THz radiation poses no health risk for scanning of people. In this paper, stand-off interferometric imaging and sensing for the detection of explosives, weapons and drugs is emphasized. Future prospects of THz technology are discussed.",10.1088/0268-1242/20/7/018
J,"Yue, S; Pilon, P; Cavadias, G",Power of the Mann-Kendall and Spearman's rho tests for detecting monotonic trends in hydrological series,"In many hydrological studies, two non-parametric rank-based statistical tests, namely the Mann-Kendall test and Spearman's rho test are used for detecting monotonic trends in time series data. However, the power of these tests has not been well documented. This study investigates the power of the tests by Monte Carlo simulation. Simulation results indicate that their power depends on the pre-assigned significance level. magnitude of trend. sample size. and the amount of variation within a time series. That is. the bigger the absolute magnitude of trend. the more powerful are the tests, as the sample size increases, the tests become more powerful: and as the amount of variation increases within a time series. the power of the tests decrease. When a trend is present. the power is also dependent on the distribution type and skewness of the time series. The simulation results also demonstrate that these two tests have similar power in detecting a trend, to the point of being indistinguishable in practice. The two tests are implemented to assess the significance of trends in annual maximum daily streamflow data of 20 pristine basins in Ontario, Canada. Results indicate that the P-values computed by these different tests are almost identical. By the binomial distribution. the field significant downward trend was assessed at the significance level of 0.05, Results indicate that a higher number of sites show evidence of decreasing trends than one might expect due to chance alone. (C) 2002 Elsevier Science B.V. All rights reserved.",10.1016/S0022-1694(01)00594-7
J,"Nedic, A; Ozdaglar, A; Parrilo, PA",Constrained Consensus and Optimization in Multi-Agent Networks,"We present distributed algorithms that can be used by multiple agents to align their estimates with a particular value over a network with time-varying connectivity. Our framework is general in that this value can represent a consensus value among multiple agents or an optimal solution of an optimization problem, where the global objective function is a combination of local agent objective functions. Our main focus is on constrained problems where the estimates of each agent are restricted to lie in different convex sets. To highlight the effects of constraints, we first consider a constrained consensus problem and present a distributed ""projected consensus algorithm"" in which agents combine their local averaging operation with projection on their individual constraint sets. This algorithm can be viewed as a version of an alternating projection method with weights that are varying over time and across agents. We establish convergence and convergence rate results for the projected consensus algorithm. We next study a constrained optimization problem for optimizing the sum of local objective functions of the agents subject to the intersection of their local constraint sets. We present a distributed ""projected subgradient algorithm"" which involves each agent performing a local averaging operation, taking a subgradient step to minimize its own objective function, and projecting on its constraint set. We show that, with an appropriately selected stepsize rule, the agent estimates generated by this algorithm converge to the same optimal solution for the cases when the weights are constant and equal, and when the weights are time-varying but all agents have the same constraint set.",10.1109/TAC.2010.2041686
J,"Ackermann, T; Andersson, G; Söder, L",Distributed generation:: a definition,"Distributed generation (DG) is expected to become more important in the future generation system. The current literature, however, does not use a consistent definition of DG. This paper discusses the relevant issues and aims at providing a general definition for distributed power generation in competitive electricity markets. In general, DG can be defined as electric power generation within distribution networks or on the customer side of the network. In addition, the terms distributed resources, distributed capacity and distributed utility are discussed. Network and connection issues of distributed generation are presented, too. (C) 2001 Elsevier Science S.A. All rights reserved.",10.1016/S0378-7796(01)00101-8
J,"Stoica, I; Morris, R; Liben-Nowell, D; Karger, DR; Kaashoek, MF; Dabek, F; Balakrishnan, H",Chord: A scalable peer-to-peer lookup protocol for Internet applications,"A fundamental problem that confronts peer-to-peer applications is the efficient location of the node that stores a desired data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis and simulations show that Chord is scalable: Communication cost and the state maintained by each node scale logarithmically with the number of Chord nodes.",10.1109/TNET.2002.808407
J,"Pasqualetti, F; Dörfler, F; Bullo, F",Attack Detection and Identification in Cyber-Physical Systems,"Cyber-physical systems are ubiquitous in power systems, transportation networks, industrial control processes, and critical infrastructures. These systems need to operate reliably in the face of unforeseen failures and external malicious attacks. In this paper: i) we propose a mathematical framework for cyber-physical systems, attacks, and monitors; ii) we characterize fundamental monitoring limitations from system-theoretic and graph-theoretic perspectives; and ii) we design centralized and distributed attack detection and identification monitors. Finally, we validate our findings through compelling examples.",10.1109/TAC.2013.2266831
J,"Kerr, YH; Waldteufel, P; Wigneron, JP; Delwart, S; Cabot, F; Boutin, J; Escorihuela, MJ; Font, J; Reul, N; Gruhier, C; Juglea, SE; Drinkwater, MR; Hahne, A; Martín-Neira, M; Mecklenburg, S",The SMOS Mission: New Tool for Monitoring Key Elements of the Global Water Cycle,"It is now well understood that data on soil moisture and sea surface salinity (SSS) are required to improve meteorological and climate predictions. These two quantities are not yet available globally or with adequate temporal or spatial sampling. It is recognized that a spaceborne L-band radiometer with a suitable antenna is the most promising way of fulfilling this gap. With these scientific objectives and technical solution at the heart of a proposed mission concept the European Space Agency (ESA) selected the Soil Moisture and Ocean Salinity (SMOS) mission as its second Earth Explorer Opportunity Mission. The development of the SMOS mission was led by ESA in collaboration with the Centre National d'Etudes Spatiales (CNES) in France and the Centro para el Desarrollo Tecnologico Industrial (CDTI) in Spain. SMOS carries a single payload, an L-Band 2-D interferometric radiometer operating in the 1400-1427-MHz protected band [1]. The instrument receives the radiation emitted from Earth's surface, which can then be related to the moisture content in the first few centimeters of soil over land, and to salinity in the surface waters of the oceans. SMOS will achieve an unprecedented maximum spatial resolution of 50 km at L-band over land (43 km on average over the field of view), providing multiangular dual polarized (or fully polarized) brightness temperatures over the globe. SMOS has a revisit time of less than 3 days so as to retrieve soil moisture and ocean salinity data, meeting the mission's science objectives. The caveat in relation to its sampling requirements is that SMOS will have a somewhat reduced sensitivity when compared to conventional radiometers. The SMOS satellite was launched successfully on November 2, 2009.",10.1109/JPROC.2010.2043032
J,"Beck, A; Teboulle, M",Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems,"This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation (TV) minimization model with constraints. We derive a fast algorithm for the constrained TV-based image deburring problem. To achieve this task, we combine an acceleration of the well known dual approach to the denoising problem with a novel monotone version of a fast iterative shrinkage/thresholding algorithm (FISTA) we have recently introduced. The resulting gradient-based algorithm shares a remarkable simplicity together with a proven global rate of convergence which is significantly better than currently known gradient projections-based methods. Our results are applicable to both the anisotropic and isotropic discretized TV functionals. Initial numerical results demonstrate the viability and efficiency of the proposed algorithms on image deblurring problems with box constraints.",10.1109/TIP.2009.2028250
J,"Annadurai, G; Juang, RS; Lee, DJ",Use of cellulose-based wastes for adsorption of dyes from aqueous solutions,"Low-cost banana and orange peels were prepared as adsorbents for the adsorption of dyes from aqueous solutions. Dye concentration and pH were varied. The adsorption capacities for both peels decreased in the order methyl orange (MO) > methylene blue (MB) > Rhodamine B (RB) > Congo red (CR) > methyl violet (MV) > amido black 10B (AB). The isotherm data could be well described by the Freundlich and Langmuir equations in the concentration range of 10-120 mg/l. An alkaline pH was favorable for the adsorption of dyes. Based on the adsorption capacity, it was shown that banana peel was more effective than orange peel. Kinetic parameters of adsorption such as the Langergren rate constant and the intraparticle diffusion rate constant were determined. For the present adsorption process intraparticle diffusion of dyes within the particle was identified to be rate limiting. Both peel wastes were shown to be promising materials for adsorption removal of dyes from aqueous solutions. (C) 2002 Elsevier Science B.V. All rights reserved.",10.1016/S0304-3894(02)00017-1
J,"Li, X; Orchard, MT",New edge-directed interpolation,This paper proposes an edge-directed interpolation algorithm for natural images. The basic idea is to first estimate local covariance coefficients from a low-resolution image and then use these covariance estimates to adapt the interpolation at a higher resolution based on the geometric duality between the low-resolution covariance and the high-resolution covariance. The edge-directed property of covariance-based adaptation attributes to its capability of tuning the interpolation coefficients to match an arbitrarily oriented step edge. A hybrid approach of switching between bilinear interpolation and covariance-based adaptive interpolation is proposed to reduce the overall computational complexity. Two important applications of the new interpolation algorithm are studied: resolution enhancement of grayscale images and reconstruction of color images from CCD samples. Simulation results demonstrate that our new interpolation algorithm substantially improves the subjective quality of the interpolated images over conventional linear interpolation.,10.1109/83.951537
J,"Chen, B; Wornell, GW",Quantization index modulation: A class of provably good methods for digital watermarking and information embedding,"We consider the problem of embedding one signal (e,g,, a digital watermark), within another ""host"" Signal to form a third, ""composite"" signal. The embedding is designed to achieve efficient tradeoffs among the three conflicting goals of maximizing information-embedding rate, minimizing distortion between the host signal and composite signal, and maximizing the robustness of the embedding. We introduce new classes of embedding methods, termed quantization index modulation (QIM) and distortion-compensated QIM (DC-QIM), and develop convenient realizations in the form of what we refer to as dither modulation. Using deterministic models to evaluate digital watermarking methods, we show that QIM is ""provably good"" against arbitrary bounded and fully informed attacks, which arise in several copyright applications, and in particular, it achieves provably better rate distortion-robustness tradeoffs than currently popular spread-spectrum and low-bit(s) modulation methods. Furthermore, we show that for some important classes of probabilistic models, DC-QIM is optimal (capacity-achieving) and regular QIM is near-optimal. These include both additive white Gaussian noise (AWGN) channels, which mag be good models for hybrid transmission applications such as digital audio broadcasting, and mean-square-error-constrained attack channels that model private-key watermarking applications.",10.1109/18.923725
J,"Lin, J; Yu, W; Zhang, N; Yang, XY; Zhang, HL; Zhao, W","A Survey on Internet of Things: Architecture, Enabling Technologies, Security and Privacy, and Applications","Fog/edge computing has been proposed to be integrated with Internet of Things (IoT) to enable computing services devices deployed at network edge, aiming to improve the user's experience and resilience of the services in case of failures. With the advantage of distributed architecture and close to end-users, fog/edge computing can provide faster response and greater quality of service for IoT applications. Thus, fog/edge computing-based IoT becomes future infrastructure on IoT development. To develop fog/edge computing-based IoT infrastructure, the architecture, enabling techniques, and issues related to IoT should be investigated first, and then the integration of fog/edge computing and IoT should be explored. To this end, this paper conducts a comprehensive overview of IoT with respect to system architecture, enabling technologies, security and privacy issues, and present the integration of fog/edge computing and IoT, and applications. Particularly, this paper first explores the relationship between cyber-physical systems and IoT, both of which play important roles in realizing an intelligent cyber-physical world. Then, existing architectures, enabling technologies, and security and privacy issues in IoT are presented to enhance the understanding of the state of the art IoT development. To investigate the fog/edge computing-based IoT, this paper also investigate the relationship between IoT and fog/edge computing, and discuss issues in fog/edge computing-based IoT. Finally, several applications, including the smart grid, smart transportation, and smart cities, are presented to demonstrate how fog/edge computing-based IoT to be implemented in real-world applications.",10.1109/JIOT.2017.2683200
J,"Kiureghian, AD; Didevsen, O",Aleatory or epistemic? Does it matter?,"The Sources and characters Of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems. (c) 2008 Elsevier Ltd. All rights reserved.",10.1016/j.strusafe.2008.06.020
J,"Bioucas-Dias, JM; Figueiredo, MAT",A new TwIST: Two-step iterative shrinkage/thresholding algorithms for image restoration,"Iterative shrinkage/thresholding (IST) algorithms have been recently proposed to handle a class of convex unconstrained optimization problems arising in image restoration and other linear inverse problems. This class of problems results from combining a linear observation model with a nonquadratic regularizer (e.g., total variation or wavelet-based regularization). It happens that the convergence rate of these IST algorithms depends heavily on the linear observation operator, becoming very slow when this operator is ill-conditioned or ill-posed. In this paper, we introduce two-step IST (TwIST) algorithms, exhibiting much faster convergence rate than IST for ill-conditioned problems. For a vast class of nonquadratic convex regularizers (EP norms, some Besov norms, and total variation), we show that TWIST converges to a minimizer of the objective function, for a given range of values of its parameters. For noninvertible observation operators, we introduce a monotonic version of TWIST (NITWIST); although the convergence proof does not apply to this scenario, we give experimental evidence that MTwIST exhibits similar speed gains over IST. The effectiveness of the new methods are experimentally confirmed on problems of image deconvolution and of restoration with missing samples.",10.1109/TIP.2007.909319
J,"Zeng, Y; Zhang, R",Energy-Efficient UAV Communication With Trajectory Optimization,"Wireless communication with unmanned aerial vehicles (UAVs) is a promising technology for future communication systems. In this paper, assuming that the UAV flies horizontally with a fixed altitude, we study energy-efficient UAV communication with a ground terminal via optimizing the UAV's trajectory, a new design paradigm that jointly considers both the communication throughput and the UAV's energy consumption. To this end, we first derive a theoretical model on the propulsion energy consumption of fixed-wing UAVs as a function of the UAV's flying speed, direction, and acceleration. Based on the derived model and by ignoring the radiation and signal processing energy consumption, the energy efficiency of UAV communication is defined as the total information bits communicated normalized by the UAV propulsion energy consumed for a finite time horizon. For the case of unconstrained trajectory optimization, we show that both the rate-maximization and energy-minimization designs lead to vanishing energy efficiency and thus are energy-inefficient in general. Next, we introduce a simple circular UAV trajectory, under which the UAV's flight radius and speed are jointly optimized to maximize the energy efficiency. Furthermore, an efficient design is proposed for maximizing the UAV's energy efficiency with general constraints on the trajectory, including its initial/final locations and velocities, as well as minimum/maximum speed and acceleration. Numerical results show that the proposed designs achieve significantly higher energy efficiency for UAV communication as compared with other benchmark schemes.",10.1109/TWC.2017.2688328
J,"Ye, W; Heidemann, J; Estrin, D",Medium access control with coordinated adaptive sleeping for wireless sensor networks,"This paper proposes S-MAC, a medium access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with nodes remaining largely inactive for long time, but becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in several ways: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses a few novel techniques to reduce energy consumption and support self-configuration. It enables low-duty-cycle operation in a multihop network. Nodes form virtual clusters based on common sleep schedules to reduce control overhead and enable traffic-adaptive wake-up. S-MAC uses in-channel signaling to avoid overhearing unnecessary traffic. Finally, S-MAC applies message passing to reduce contention latency for applications that require in-network data processing. The paper presents measurement results of S-MAC performance on a sample sensor node, the UC Berkeley Mote, and reveals fundamental tradeoffs on energy, latency and throughput. Results show that S-MAC obtains significant energy savings compared with an 802.11-like MAC without sleeping.",10.1109/TNET.2004.828953
J,"Yoo, T; Goldsmith, A",On the optimality of multiantenna broadcast scheduling using zero-forcing beamforming,"Although the capacity of multiple-input/multiple-output (MIMO) broadcast channels (BCs) can be achieved by dirty paper coding (DPC), it is difficult to implement in practical systems. This paper investigates if., for a large number of users, simpler schemes can achieve the same performance. Specifically, we show that a zero-forcing beamforming (ZFBF) strategy, while generally suboptimal, can achieve the same asymptotic sum capacity as that of DPC, as the number of users goes to infinity. In proving this asymptotic result, we provide an algorithm for determining which users should be active under ZFBF. These users are semiorthogonal to one another and can be grouped for simultaneous transmission to enhance the throughput of scheduling algorithms. Based on the user grouping, we propose and compare two fair scheduling schemes in round-robin ZFBF and proportional-fair ZFBF. We provide numerical results to confirm the optimality of ZFBF and to compare the performance of ZFBF and proposed fair scheduling schemes with that of various MIMO BC strategies.",10.1109/JSAC.2005.862421
J,"Li, ZZ; Hoiem, D",Learning without Forgetting,"When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.",10.1109/TPAMI.2017.2773081
J,"Liang, QL; Mendel, JM",Interval type-2 fuzzy logic systems: Theory and design,"In this paper, we present the theory and design of interval type-2 fuzzy logic systems (FLSs), We propose an efficient and simplified method to compute the input and antecedent operations for interval type-2 FLSs; one that is based on a general inference formula for them. We introduce the concept of upper and lower membership functions (MFs) and illustrate our efficient inference method for the case of Gaussian primary MFs, We also propose a method for designing an interval type-2 FLS in which we tune its parameters, Finally, we design type-2 FLSs to perform time-series forecasting when a nonstationary time-series is corrupted by additive mise where SNR is uncertain and demonstrate improved performance over type-1 FLSs.",10.1109/91.873577
J,"Kim, SJ; Koh, K; Lustig, M; Boyd, S; Gorinevsky, D",An Interior-Point Method for Large-Scale <i>l</i><sub>1</sub>-Regularized Least Squares,"Recently, a lot of attention has been paid to l(1) regularization based methods for sparse signal reconstruction (e.g., basis pursuit denoising and compressed sensing) and feature selection (e.g., the Lasso algorithm) in signal processing, statistics, and related fields. These problems can be cast as l(1)-regularized least-squares programs (LSPs), which can be reformulated as convex quadratic programs, and then solved by several standard methods such as interior-point methods, at least for small and medium size problems. In this paper, we describe a specialized interior-point method for solving large-scale, l(1)-regularized LSPs that uses the preconditioned conjugate gradients algorithm to compute the search direction. The interior-point method can solve large sparse problems, with a million variables and observations, in a few tens of minutes on a PC. It can efficiently solve large dense problems, that arise in sparse signal recovery with orthogonal transforms, by exploiting fast algorithms for these transforms. The method is illustrated on a magnetic resonance imaging data set.",10.1109/JSTSP.2007.910971
J,"Taubman, D",High performance scalable image compression with EBCOT,"A new image compression algorithm is proposed, based on independent Embedded Block Coding with Optimized Truncation of the embedded bit-streams (EBCOT), The algorithm exhibits state-of-the-art compression performance while producing a bit-stream with a rich set of features, including resolution and SNR scalability together with a ""random access"" property, The algorithm has modest complexity and is suitable for applications involving remote browsing of large compressed images. The algorithm lends itself to explicit optimization with respect to MSE as well as more realistic psychovisual metrics, capable of modeling the spatially varying visual masking phenomenon.",10.1109/83.847830
S,"Ewing, R; Cervero, R",Travel and the built environment - A synthesis,"The potential to moderate travel demand through changes in the built environment is the subject of more than 50 recent empirical studies. The majority of recent studies are summarized. Elasticities of travel demand with respect to density, diversity, design, and regional accessibility are then derived from selected studies. These elasticity values may be useful in travel forecasting and sketch planning and have already been incorporated into one sketch planning tool, the Environmental Protection Agency's Smart Growth Index model. In weighing the evidence, what can be said, with a degree of certainty, about the effects of built environments on key transportation ""outcome"" variables: trip frequency, trip length, mode choice, and composite measures of travel demand, vehicle miles traveled (VMT) and vehicle hours traveled (VHT)? Trip frequencies have attracted considerable academic interest of late. They appear to be primarily a function of socioeconomic characteristics of travelers and secondarily a function of the built environment. Trip lengths have received relatively little attention, which may account for the various degrees of importance attributed to the built environment in recent studies. Trip lengths are primarily a function of the built environment and secondarily a function of socioeconomic characteristics. Mode choices have received the most intensive study over the decades. Mode choices depend on both the built environment and socioeconomics (although they probably depend more on the latter). Studies of overall VMT or VHT find the built environment to be much more significant, a product of the differential trip lengths that factor into calculations of VMT and VHT.",10.3141/1780-10
J,"Mayne, DQ",Model predictive control: Recent developments and future promise,"This paper recalls a few past achievements in Model Predictive Control, gives an overview of some current developments and suggests a few avenues for future research. (C) 2014 Elsevier Ltd. All rights reserved.",10.1016/j.automatica.2014.10.128
J,"Niu, SM; Wang, SH; Lin, L; Liu, Y; Zhou, YS; Hu, YF; Wang, ZL",Theoretical study of contact-mode triboelectric nanogenerators as an effective power source,"A theoretical model for contact-mode TENGs was constructed in this paper. Based on the theoretical model, its real-time output characteristics and the relationship between the optimum resistance and TENG parameters were derived. The theory presented here is the first in-depth interpretation of the contact-mode TENG, which can serve as important guidance for rational design of the TENG structure in specific applications.",10.1039/c3ee42571a
J,"Anipsitakis, GP; Dionysiou, DD",Degradation of organic contaminants in water with sulfate radicals generated by the conjunction of peroxymonosulfate with cobalt,"A highly efficient advanced oxidation process for the destruction of organic contaminants in water is reported. The technology is based on the cobalt-mediated decomposition of peroxymonosulfate that leads to the formation of very strong oxidizing species (sulfate radicals) in the aqueous phase. The system is a modification of the Fenton Reagent, since an oxidant is coupled with a transition metal in a similar manner. Sulfate radicals were identified with quenching studies using specific alcohols. The study was primarily focused on comparing the cobalt/peroxymonosulfate (Co/PMS) reagent with the traditional Fenton Reagent [Fe(II)/H2O2] in the dark, at the pH range 2.0-9.0 with and without the presence of buffers such as phosphate and carbonate. Three model contaminants that show diversity in structure were tested: 2,4-dichlorophenol, atrazine, and naphthalene. Cobalt/peroxymonosulf ate was consistently proven to be more efficient than the Fenton Reagent for the degradation of 2,4-dichlorophenol and atrazine, at all the conditions tested. At high pH values, where the efficiency of the Fenton Reagent was diminished, the reactivity of the Co/PMS system was sustained at high values. When naphthalene was treated with the two oxidizing systems in comparison, the Fenton Reagent demonstrated higher degradation efficiencies than cobalt/peroxymonosulfate at acidic pH, but, at higher pH (neutral), the latter was proven much more effective. The extent of mineralization, as total organic carbon removed, was also monitored, and again the Co/PMS reagent demonstrated higher efficiencies than the Fenton Reagent. Cobalt showed true catalytic activity in the overall process, since extremely low concentrations (in the range of mug/L) were sufficient for the decomposition of the oxidant and thus the radical generation. The advantage of Co/PMS compared to the traditional Fenton Reagent is attributed primarily to the oxidizing strength of the radicals formed, since sulfate radicals are stronger oxidants than hydroxyl and the thermodynamics of the transition-metal-oxidant coupling.",10.1021/es0263792
J,"Abbaspour, KC; Yang, J; Maximov, I; Siber, R; Bogner, K; Mieleitner, J; Zobrist, J; Srinivasan, R",Modelling hydrology and water quality in the pre-alpine/alpine Thur watershed using SWAT,"In a national effort, since 1972, the Swiss Government started the ""National Long-term Monitoring of Swiss Rivers"" (NADUF) program aimed at evaluating the chemical and physical states of major rivers leaving Swiss potitical, boundaries. The established monitoring network of 19 sampling stations included locations on all major rivers of Switzerland. This study complements the monitoring program and aims to model one of the program's catchments - Thur River basin (area 1700 km(2)), which is located in the north-east of Switzerland and is a direct tributary to the Rhine. The program SWAT (Soil and Water Assessment Tool was used to simutate all related processes affecting water quantity, sediment, and nutrient toads in the catchment. The main objectives were to test the performance of SWAT and the feasibility of using this model as a simulator of flow and transport processes at a watershed scale. Model calibration and uncertainty analysis were performed with SUFI-2 (Sequential Uncertainty FItting Ver. 2), which was interfaced with SWAT using the generic iSWAT program. Two measures were used to assess the goodness of calibration: (1) the percentage of data bracketed by the 95% prediction uncertainty calculated at the 2.5 and 97.5 percentiles of the cumulative distribution of the simulated variables, and (2) the d-factor, which is the ratio of the average distance between the above percentiles and the standard deviation of the corresponding measured variable. These statistics showed excellent results for discharge and nitrate and quite good results for sediment and total phosphorous. We concluded that: in watersheds similar to Thur with good data quality and availability and relatively small model uncertainty - it is feasible to use SWAT as a flow and transport simulator. This is a precursor for watershed management studies. (c) 2006 Elsevier B.V. All rights reserved.",10.1016/j.jhydrol.2006.09.014
J,"Zhao, ZM; Zhao, JW; Hu, ZL; Li, JD; Li, JJ; Zhang, YJ; Wang, C; Cui, GL",Long-life and deeply rechargeable aqueous Zn anodes enabled by a multifunctional brightener-inspired interphase,"Aqueous Zn anodes have been revisited for their intrinsic safety, low cost, and high volumetric capacity; however, deep-seated issues of dendrite growth and intricate side-reactions hindered their rejuvenation. Herein, a ""brightener-inspired'' polyamide coating layer which elevates the nucleation barrier and restricts Zn2+ 2D diffusion is constructed to effectively regulate the aqueous Zn deposition behavior. Importantly, serving as a buffer layer that isolates active Zn from bulk electrolytes, this interphase also suppresses free water/O2-induced corrosion and passivation. With this synergy effect, the polymermodified Zn anode produces reversible, dendrite-free plating/stripping with a 60-fold enhancement in running lifetime (over 8000 hours) compared to the bare Zn, and even at an ultrahigh areal capacity of 10 mA h cm(-2) (10 mA cm(-2) for 1 h, 85% depth of discharge). This efficient rechargeability for Zn anodes enables a substantially stable full-cell paired with a MnO2 cathode. The strategy presented here is straightforward and scalable, representing a stark, but promising approach to solve the anode issues in advanced Zn batteries.",10.1039/c9ee00596j
J,"Abdel-Hamid, O; Mohamed, AR; Jiang, H; Deng, L; Penn, G; Yu, D",Convolutional Neural Networks for Speech Recognition,"Recently, the hybrid deep neural network (DNN)hidden Markov model (HMM) has been shown to significantly improve speech recognition performance over the conventional Gaussian mixture model (GMM)-HMM. The performance improvement is partially attributed to the ability of the DNN to model complex correlations in speech features. In this paper, we show that further error rate reduction can be obtained by using convolutional neural networks (CNNs). We first present a concise description of the basic CNN and explain how it can be used for speech recognition. We further propose a limited-weight-sharing scheme that can better model speech features. The special structure such as local connectivity, weight sharing, and pooling in CNNs exhibits some degree of invariance to small shifts of speech features along the frequency axis, which is important to deal with speaker and environment variations. Experimental results show that CNNs reduce the error rate by 6%-10% compared with DNNs on the TIMIT phone recognition and the voice search large vocabulary speech recognition tasks.",10.1109/TASLP.2014.2339736
J,"da Cunha, AL; Zhou, JP; Do, MN","The nonsubsampled contourlet transform: Theory, design, and applications","In this paper, we develop the nonsubsampled contourlet transform (NSCT) and study its applications. The construction proposed in this paper is based on a nonsubsampled pyramid structure and nonsubsampled directional filter banks. The result is a flexible multiscale, multidirection, and shift-invariant image decomposition that can be efficiently implemented via the a trous algorithm. At the core of the proposed scheme is the nonseparable two-channel nonsubsampled filter bank (NSFB). We exploit the less stringent design condition of the NSFB to design filters that lead to a NSCT with better frequency selectivity and regularity when compared to the contourlet transform. We propose a design framework based on the mapping approach, that allows for a fast implementation based on a lifting or ladder structure, and only uses one-dimensional filtering in some cases. In addition, our design ensures that the corresponding frame elements are regular, symmetric, and the frame is close to a tight one. We assess the performance of the NSCT in image denoising and enhancement applications. In both applications the NSCT compares favorably to other existing methods in the literature.",10.1109/TIP.2006.877507
J,"Guo, XJ; Li, Y; Ling, HB",LIME: Low-Light Image Enhancement via Illumination Map Estimation,"When one captures images in low-light conditions, the images often suffer from low visibility. Besides degrading the visual aesthetics of images, this poor quality may also significantly degenerate the performance of many computer vision and multimedia algorithms that are primarily designed for high-quality inputs. In this paper, we propose a simple yet effective low-light image enhancement (LIME) method. More concretely, the illumination of each pixel is first estimated individually by finding the maximum value in R, G, and B channels. Furthermore, we refine the initial illumination map by imposing a structure prior on it, as the final illumination map. Having the well-constructed illumination map, the enhancement can be achieved accordingly. Experiments on a number of challenging low-light images are present to reveal the efficacy of our LIME and show its superiority over several state-of-the-arts in terms of enhancement quality and efficiency.",10.1109/TIP.2016.2639450
J,"Zhou, X; Zhang, R; Ho, CK",Wireless Information and Power Transfer: Architecture Design and Rate-Energy Tradeoff,"Simultaneous information and power transfer over the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose a general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving a smaller form factor. The rate-energy tradeoff for the two architectures are characterized by a so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under a realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT).",10.1109/TCOMM.2013.13.120855
J,"Poppe, R",A survey on vision-based human action recognition,"Vision-based human action recognition is the process of labeling image sequences with action labels. Robust solutions to this problem have applications in domains such as visual surveillance, video retrieval and human-computer interaction. The task is challenging due to variations in motion performance, recording settings and inter-personal differences. In this survey, we explicitly address these challenges. We provide a detailed overview of current advances in the field. Image representations and the subsequent classification process are discussed separately to focus on the novelties of recent research. Moreover, we discuss limitations of the state of the art and outline promising directions of research. (C) 2009 Elsevier B.V. All rights reserved.",10.1016/j.imavis.2009.11.014
J,"Shan, CF; Gong, SG; McOwan, PW",Facial expression recognition based on Local Binary Patterns: A comprehensive study,"Automatic facial expression analysis is an interesting and challenging problem, and impacts important applications in many areas such as human-computer interaction and data-driven animation. Deriving an effective facial representation from original face images is a vital step for successful facial expression recognition. In this paper, we empirically evaluate facial representation based on statistical local features, Local Binary Patterns, for person-independent facial expression recognition. Different machine learning methods are systematically examined on several databases. Extensive experiments illustrate that LBP features are effective and efficient for facial expression recognition. We further formulate Boosted-LBP to extract the most discriminant LBP features, and the best recognition performance is obtained by using Support Vector Machine classifiers with Boosted-LBP features. Moreover, we investigate LBP features for low-resolution facial expression recognition, which is a critical problem but seldom addressed in the existing work. We observe in our experiments that LBP features perform stably and robustly over a useful range of low resolutions of face images, and yield promising performance in Compressed low-resolution video sequences captured in real-world environments. (C) 2008 Elsevier B.V. All rights reserved.",10.1016/j.imavis.2008.08.005
J,"Bloch, M; Barros, J; Rodrigues, MRD; McLaughlin, SW",Wireless information-theoretic security,"This paper considers the transmission of confidential data over wireless channels. Based on an information-theoretic formulation of the problem, in which two legitimates partners communicate over a quasi-static fading channel and an eavesdropper observes their transmissions through a second independent quasi-static fading channel, the important role of fading is characterized in terms of average secure communication rates and outage probability. Based on the insights from this analysis, a practical secure communication protocol is developed, which uses a four-step procedure to ensure wireless information-theoretic security: (i) common randomness via opportunistic transmission, (ii) message reconciliation, (iii) common key generation via privacy amplification, and (iv) message protection with a secret key. A reconciliation procedure based on multilevel coding and optimized low-density parity-check (LDPC) codes is introduced, which allows to achieve communication rates close to the fundamental security limits in several relevant instances. Finally, a set of metrics for assessing average secure key generation rates is established, and it is shown that the protocol is effective in secure key renewal-even in the presence of imperfect channel state information.",10.1109/TIT.2008.921908
J,"Wen, DS; Ding, YL",Experimental investigation into convective heat transfer of nanofluids at the entrance region under laminar flow conditions,"This paper reports an experimental work on the convective heat transfer of nanofluids, made of gamma-Al2O3 nanoparticles and de-ionized water, flowing through a copper tube in the laminar flow regime. The results showed considerable enhancement of convective heat transfer using the nanofluids. The enhancement was particularly significant it. the entrance region, and was much higher than that solely due to the enhancement on thermal conduction. It was also shown that the classical Shah equation failed to predict the heat transfer behaviour of nanofluids. Possible reasons for the enhancement were discussed. Migration of nanoparticles, and the resulting disturbance of the boundary layer were proposed to be the main reasons. (C) 2004 Elsevier Ltd. All rights reserved.",10.1016/j.ijheatmasstransfer.2004.07.012
J,"Stern, LA; Feng, LG; Song, F; Hu, XL",Ni<sub>2</sub>P as a Janus catalyst for water splitting: the oxygen evolution activity of Ni<sub>2</sub>P nanoparticles,"Electrochemical water splitting into hydrogen and oxygen is a promising method for solar energy storage. The development of efficient electrocatalysts for water splitting has drawn much attention. However, catalysts that are active for both the hydrogen evolution and oxygen evolution reactions are rare. Herein, we show for the first time that nickel phosphide (Ni2P), an excellent hydrogen evolving catalyst, is also highly active for oxygen evolution. A current density of 10 mA cm(-2) is generated at an overpotential of only 290mV in 1M KOH. The high activity is attributed to the core-shell (Ni2P/NiOx) structure that the material adopts under catalytic conditions. The Ni2P nanoparticles can serve as both cathode and anode catalysts for an alkaline electrolyzer, which generates 10 mA cm(-2) at 1.63 V.",10.1039/c5ee01155h
J,"Osseiran, A; Boccardi, F; Braun, V; Kusume, K; Marsch, P; Maternia, M; Queseth, O; Schellmann, M; Schotten, H; Taoka, H; Tullberg, H; Uusitalo, MA; Timus, B; Fallgren, M",Scenarios for 5G Mobile and Wireless Communications: The Vision of the METIS Project,"METIS is the EU flagship 5G project with the objective of laying the foundation for 5G systems and building consensus prior to standardization. The METIS overall approach toward 5G builds on the evolution of existing technologies complemented by new radio concepts that are designed to meet the new and challenging requirements of use cases today's radio access networks cannot support. The integration of these new radio concepts, such as massive MIMO, ultra dense networks, moving networks, and device-to-device, ultra reliable, and massive machine communications, will allow 5G to support the expected increase in mobile data volume while broadening the range of application domains that mobile communications can support beyond 2020. In this article, we describe the scenarios identified for the purpose of driving the 5G research direction. Furthermore, we give initial directions for the technology components (e.g., link level components, multinode/multi-antenna, multi-RAT, and multi-layer networks and spectrum handling) that will allow the fulfillment of the requirements of the identified 5G scenarios.",10.1109/MCOM.2014.6815890
J,"Hochwald, BM; ten Brink, S",Achieving near-capacity on a multiple-antenna channel,"Recent advancements in iterative processing of channel codes and the development of turbo codes have allowed the communications industry to achieve near-capacity on a single-antenna Gaussian or fading channel with low complexity. We show how these iterative techniques can also be used to achieve near-capacity on a multiple-antenna system where the receiver knows the channel. Combining iterative processing with multiple-antenna channels is particularly challenging because the channel capacities can be a factor of ten or more higher than their single-antenna counterparts. Using a ""list"" version of the sphere decoder, we provide a simple method to iteratively detect and decode any linear space-time mapping combined with any channel code that can be decoded using so-called ""soft"" inputs and outputs. We exemplify our technique by directly transmitting symbols that are coded with a channel code; we show that iterative processing with even this simple scheme can achieve near-capacity. We consider both simple convolutional and powerful turbo channel codes and show that excellent performance at very high data rates can be attained with either. We compare our simulation results with Shannon capacity limits for ergodic multiple-antenna channel.",10.1109/TCOMM.2003.809789
J,"Shi, BG; Bai, X; Yao, C",An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition,"Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for real-world application scenarios. The experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.",10.1109/TPAMI.2016.2646371
J,"Turner, LK; Collins, FG",Carbon dioxide equivalent (CO<sub>2</sub>-e) emissions: A comparison between geopolymer and OPC cement concrete,"Concrete for construction has traditionally been based on an Ordinary Portland Cement (OPC) binder. Geopolymers, an alternative binder based on fly ash (a fine waste collected from the emissions liberated by coal burning power stations) that is activated by an alkaline activator, have potential to lower the significant carbon footprint of OPC concrete. This paper presents the results of comprehensive carbon footprint estimates for both geopolymer and OPC concrete, including energy expending activities associated with mining and transport of raw materials, manufacturing and concrete construction. Previous studies have shown a wide variation of reported emission estimates: the results of this study are benchmarked with data from those studies. (C) 2013 Elsevier Ltd. All rights reserved.",10.1016/j.conbuildmat.2013.01.023
J,"Maeda, N; Atarashi, H; Sawahashi, M",Performance comparison of channel interleaving methods in frequency domain for VSF-OFCDM broadband wireless access in forward link,"This paper presents a performance comparison of the channel-interleaving method in the frequency domain, i.e., bit interleaving after channel encoding, symbol interleaving after data modulation, and chip interleaving after spreading, for Variable Spreading Factor-Orthogonal Frequency and Code Division Multiplexing (VSF-OFCDM) wireless access with frequency domain spreading, in order to reduce the required average received signal energy per symbol-to-background noise power spectrum density ratio (E-s/N-0) and achieve the maximum radio link capacity. Simulation results show that, for QPSK data modulation employing turbo coding with the channel coding rate R = 3/4, the chip-interleaving method decreases the required average received E-s/N-0 the most for various radio parameters and propagation model conditions, where the number of code-multiplexing, C-mux, the spreading factor, SF, the r.m.s. delay spread, sigma, the number of multipaths, L, and the maximum Doppler frequency, f(D), are varied as parameters. For example, when C-mux = 12 of SF = 16, the improvement in the required average received E-s/N-0 from the case without interleaving at the average packet error rate (PER) of 10(-2), is approximately 0.3, 0.3, and 1.4 dB for the bit, symbol, and chip interleaving, respectively, in a L = 12-path exponential decayed Rayleigh fading channel with sigma of 0.043 musec and fD of 20 Hz. This is because the chip interleaving obtains a higher diversity gain by replacing the chip assignment over the entire bandwidth. Meanwhile, in 16QAM data modulation with R = 1/2, the performance of the chip interleaving is deteriorated, when C-mux/SF > 0.25. due to the inter-code interference caused by different fading variations over the spreading duration since the successive chips during the spreading duration are interleaved to the separated sub-carriers. Thus, bit interleaving exhibits the best performance although the difference between bit interleaving and symbol interleaving is slight. Consequently, we conclude that the bit-interleaving method is the best among the three interleaving methods for reducing the required received E-s/N-0 considering the tradeoff between the randomization effect of burst errors and the mitigation of inter-code interference assuming the application of adaptive modulation and channel coding scheme in OFCDM employing frequency domain spreading.",
J,"Askarzadeh, A",A novel metaheuristic method for solving constrained engineering optimization problems: Crow search algorithm,"This paper proposes a novel metaheuristic optimizer, named crow search algorithm (CSA), based on the intelligent behavior of crows. CSA is a population-based technique which works based on this idea that crows store their excess food in hiding places and retrieve it when the food is needed. CSA is applied to optimize six constrained engineering design problems which have different natures of objective functions, constraints and decision variables. The results obtained by CSA are compared with the results of various algorithms. Simulation results reveal that using CSA may lead to finding promising results compared to the other algorithms. (C) 2016 Elsevier Ltd. All rights reserved.",10.1016/j.compstruc.2016.03.001
J,"Mueller, NC; Nowack, B",Exposure modeling of engineered nanoparticles in the environment,"The aim of this study was to use a life-cycle perspective to model the quantities of engineered nanoparticles released into the environment. Three types of nanoparticles were studied: nano silver (nano-Ag), nano TiO2 (nano-TiO2), and carbon nanotubes (CNT). The quantification was based on a substance flow analysis from products to air,soil, and water in Switzerland. The following parameters were used as model inputs: estimated worldwide production volume, allocation of the production volume to product categories, particle release from products, and flow coefficients within the environmental compartments. The predicted environmental concentrations (PEC) were then compared to the predicted no effect concentrations (PNEC) derived from the literature to estimate a possible risk. The expected concentrations of the three nanoparticles in the different environmental compartments vary widely, caused by the different life cycles of the nanoparticle-containing products. The PEC values for nano-TiO2 in water are 0.7-16 mu g/L and close to or higher than the PNEC value for nano-TiO2 (< 1 mu g/L). The risk quotients (PEC/PNEC) for CNT and nano-Ag were much smaller than one, therefore comprising no reason to expect adverse effects from those particles. The results of this study make it possible for the first time to carry out a quantitative risk assessment of nanoparticles in the environment and suggest further detailed studies of nano-TiO2.",10.1021/es7029637
J,"Wolpaw, JR; Birbaumer, N; Heetderks, WJ; McFarland, DJ; Peckham, PH; Schalk, G; Donchin, E; Quatrano, LA; Robinson, CJ; Vaughan, TM",Brain-computer interface technology: A review of the first international meeting,"Over the past decade, many laboratories have begun to explore brain-computer interface (BCI) technology as a radically new communication option for those with neuromuscular impairments that prevent them from using conventional augmentative communication methods. BCI's provide these users with communication channels that do not depend on peripheral nerves and muscles. This article summarizes the first international meeting devoted to BCI research and development. Current BCI's use electroencephalographic (EEG) activity recorded at the scalp or single-unit activity recorded from within cortex to control cursor movement, select letters or icons, or operate a neuroprosthesis. The central element in each BCI is a translation algorithm that converts electrophysiological input from the user into output that controls external devices. BCI operation depends on effective interaction between two adaptive controllers, the user who encodes his or her commands in the electrophysiological input provided to the BCI, and the BCI which recognizes the commands contained in the input and expresses them in device control. Current BCI's have maximum information transfer rates of 5-25 b/min. Achievement of greater speed and accuracy depends on improvements in signal processing, translation algorithms, and user training. These improvements depend on increased interdisciplinary cooperation between neuroscientists, engineers, computer programmers, psychologists, and rehabilitation specialists, and on adoption and widespread application of objective methods for evaluating alternative methods. The practical use of BCI technology depends on the development of appropriate applications, identification of appropriate user groups, and careful attention to the needs and desires of individual users. BCI research and development will also benefit from greater emphasis on peer-reviewed publications, and from adoption of standard venues for presentations and discussion.",10.1109/TRE.2000.847807
J,"Deng, L; Yu, D",Deep Learning: Methods and Applications,"This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.",10.1561/2000000039
J,"Franquelo, LG; Rodríguez, J; Leon, JI; Kouro, S; Portillo, R; Prats, MM",The Age of Multilevel Converters Arrives,,10.1109/MIE.2008.923519
J,"Gilles, J",Empirical Wavelet Transform,"Some recent methods, like the empirical mode decomposition (EMD), propose to decompose a signal accordingly to its contained information. Even though its adaptability seems useful for many applications, the main issue with this approach is its lack of theory. This paper presents a new approach to build adaptive wavelets. The main idea is to extract the different modes of a signal by designing an appropriate wavelet filter bank. This construction leads us to a new wavelet transform, called the empirical wavelet transform. Many experiments are presented showing the usefulness of this method compared to the classic EMD.",10.1109/TSP.2013.2265222
J,"Pruden, A; Pei, RT; Storteboom, H; Carlson, KH",Antibiotic resistance genes as emerging contaminants: Studies in northern Colorado,"This study explores antibiotic resistance genes (ARGs) as emerging environmental contaminants. The purpose of this study was to investigate the occurrence of ARGs in various environmental compartments in northern Colorado, including Cache La Poudre (Poudre) River sediments, irrigation ditches, dairy lagoons, and the effluents of wastewater recycling and drinking water treatment plants. Additionally, ARG concentrations in the Poudre River sediments were analyzed at three time points at five sites with varying levels of urban/agricultural impact and compared with two previously published time points. It was expected that ARG concentrations would be significantly higher in environments directly impacted by urban/ agricultural activity than in pristine and lesser-impacted environments. Polymerase chain reaction (PCR) detection assays were applied to detect the presence/absence of several tetracycline and sulfonamide ARGs. Quantitative real-time PCR was used to further quantify two tetracycline ARGs (tet(W) and tet(O)) and two sulfonamide ARGs (sul- (I) and sul(II)). The following trend was observed with respect to ARG concentrations (normalized to eubacterial 16S rRNA genes): dairy lagoon water > irrigation ditch water > urban/agriculturally impacted river sediments (p < 0.0001), except for sul(II), which was absent in ditch water. It was noted that tet(W) and tet(O) were also present in treated drinking water and recycled wastewater, suggesting that these are potential pathways for the spread of ARGs to and from humans. On the basis of this study, there is a need for environmental scientists and engineers to help address the issue of the spread of ARGs in the environment.",10.1021/es060413l
J,"Sauvola, J; Pietikäinen, M",Adaptive document image binarization,"A new method is presented for adaptive document image binarization, where the page is considered as a collection of subcomponents such as text, background and picture. The problems caused by noise, illumination and many source type-related degradations are addressed. Two new algorithms are applied to determine a local threshold for each pixel. The performance evaluation of the algorithm utilizes test images with ground-truth, evaluation metrics for binarization of textual and synthetic images, and a weight-based ranking procedure for the final result presentation. The proposed algorithms were tested with images including different types of document components and degradations. The results were compared with a number of known techniques in the literature. The benchmarking results show that the method adapts and performs well in each case qualitatively and quantitatively. (C) 1999 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.",10.1016/S0031-3203(99)00055-2
J,"Barnich, O; Van Droogenbroeck, M",ViBe: A Universal Background Subtraction Algorithm for Video Sequences,"This paper presents a technique for motion detection that incorporates several innovative mechanisms. For example, our proposed technique stores, for each pixel, a set of values taken in the past at the same location or in the neighborhood. It then compares this set to the current pixel value in order to determine whether that pixel belongs to the background, and adapts the model by choosing randomly which values to substitute from the background model. This approach differs from those based upon the classical belief that the oldest values should be replaced first. Finally, when the pixel is found to be part of the background, its value is propagated into the background model of a neighboring pixel. We describe our method in full details (including pseudo-code and the parameter values used) and compare it to other background subtraction techniques. Efficiency figures show that our method outperforms recent and proven state-of-the-art methods in terms of both computation speed and detection rate. We also analyze the performance of a downscaled version of our algorithm to the absolute minimum of one comparison and one byte of memory per pixel. It appears that even such a simplified version of our algorithm performs better than mainstream techniques.",10.1109/TIP.2010.2101613
J,"Choi, WY; Park, BG; Lee, JD; Liu, TJK",Tunneling field-effect transistors (TFETs) with subthreshold swing (SS) less than 60 mV/dec,"We have demonstrated a 70-nm n-channel tunneling field-effect transistor (TFET) which has a subthreshold swing (SS) of 52.8 mV/dec at room temperature. It is the first experimental result that shows a sub-60-mV/dec SS in the silicon-based TFETs. Based on simulation results, the gate oxide and silicon-on-insulator layer thicknesses were scaled down to 2 and 70 nm, respectively. However, the ON/OFF current ratio of the TFET was still lower than that of the MOSFET. In order to increase the ON current further, the following approaches can be considered: reduction of effective gate oxide thickness, increase in the steepness of the gradient of the source to channel doping profile, and utilization of a lower bandgap channel material.",10.1109/LED.2007.901273
J,"Levelt, PF; Van den Oord, GHJ; Dobber, MR; Mälkki, A; Visser, H; de Vries, J; Stammes, P; Lundell, JOV; Saari, H",The Ozone Monitoring Instrument,"The Ozone Monitoring Instrument (OMI) flies on the National Aeronautics and Space Adminsitration's Earth Observing System Aura satellite launched in July 2004. OMI is a ultraviolet/visible (UV/VIS) nadir solar backscatter spectrometer, which provides nearly global coverage in one day with a spatial resolution of 13 km x 24 km. Trace gases measured include 03, NO,, SO,, HCHO, BrO, and OClO. In addition, OMI will measure aerosol characteristics, cloud top heights, and UV irradiance at the surface. OMI's unique capabilities for measuring important trace gases with a small footprint and daily global coverage will be a major contribution to our understanding of stratospheric and tropospheric chemistry and climate change. OMI's high spatial resolution is unprecedented and will enable detection of air pollution on urban scale resolution. In this paper, the instrument and its performance will be discussed.",10.1109/TGRS.2006.872333
J,"Platnick, S; King, MD; Ackerman, SA; Menzel, WP; Baum, BA; Riédi, JC; Frey, RA",The MODIS cloud products:: Algorithms and examples from Terra,"The Moderate Resolution Imaging Spectroradiometer (MODIS) is one of five instruments aboard the Terra Earth Observing System (EOS) platform launched in December 1999. After achieving final orbit, MODIS began earth observations in late February 2000 and has been acquiring data since that time. The instrument is also being flown on the Aqua spacecraft, launched in May 2002. A comprehensive set of remote sensing algorithms for cloud detection and the retrieval of cloud physical and optical properties have been developed by members of the MODIS atmosphere science team. The archived products from these algorithms have applications in climate change studies, climate modeling, numerical weather prediction, as well as fundamental atmospheric research. In addition to an extensive cloud mask, products include cloud-top properties (temperature, pressure, effective emissivity), cloud thermodynamic phase, cloud optical and microphysical parameters (optical thickness, effective particle radius, water path), as well as derived statistics. We will describe the various algorithms being used for the remote sensing of cloud properties from MODIS data with an emphasis on the pixel-level retrievals (referred to as Level-2 products), with 1-km or 5-km spatial resolution at nadir. An example of each Level-2 cloud product from a common data granule (5 min of data) off the coast of South America will be discussed. Future efforts will also be mentioned. Relevant points related to the global gridded statistics products (Level-3) are highlighted though additional details are given in an accompanying paper in this issue.",10.1109/TGRS.2002.808301
J,"Wright, J; Ma, Y; Mairal, J; Sapiro, G; Huang, TS; Yan, SC",Sparse Representation for Computer Vision and Pattern Recognition,"Techniques from sparse signal representation are beginning to see significant impact in computer vision, often on nontraditional applications where the goal is not just to obtain a compact high-fidelity representation of the observed signal, but also to extract semantic information. The choice of dictionary plays a key role in bridging this gap: unconventional dictionaries consisting of, or learned from, the training samples themselves provide the key to obtaining state-of-the-art results and to attaching semantic meaning to sparse signal representations. Understanding the good performance of such unconventional dictionaries in turn demands new algorithmic and analytical techniques. This review paper highlights a few representative examples of how the interaction between sparse signal representation and computer vision can enrich both fields, and raises a number of open questions for further study.",10.1109/JPROC.2010.2044470
J,"Ngo, HQ; Ashikhmin, A; Yang, H; Larsson, EG; Marzetta, TL",Cell-Free Massive MIMO Versus Small Cells,"A Cell-Free Massive MIMO (multiple-input multiple-output) system comprises a very large number of distributed access points (APs), which simultaneously serve a much smaller number of users over the same time/frequency resources based on directly measured channel characteristics. The APs and users have only one antenna each. The APs acquire channel state information through time-division duplex operation and the reception of uplink pilot signals transmitted by the users. The APs perform multiplexing/de-multiplexing through conjugate beamforming on the downlink and matched filtering on the uplink. Closed-form expressions for individual user uplink and downlink throughputs lead to max-min power control algorithms. Max-min power control ensures uniformly good service throughout the area of coverage. A pilot assignment algorithm helps to mitigate the effects of pilot contamination, but power control is far more important in that regard. Cell-Free Massive MIMO has considerably improved performance with respect to a conventional small-cell scheme, whereby each user is served by a dedicated AP, in terms of both 95%-likely per-user throughput and immunity to shadow fading spatial correlation. Under uncorrelated shadow fading conditions, the cell-free scheme provides nearly fivefold improvement in 95%-likely per-user throughput over the small-cell scheme, and tenfold improvement when shadow fading is correlated.",10.1109/TWC.2017.2655515
J,"Yan, Y; Mao, YX; Li, B",SECOND: Sparsely Embedded Convolutional Detection,"LiDAR-based or RGB-D-based object detection is used in numerous applications, ranging from autonomous driving to robot vision. Voxel-based 3D convolutional networks have been used for some time to enhance the retention of information when processing point cloud LiDAR data. However, problems remain, including a slow inference speed and low orientation estimation performance. We therefore investigate an improved sparse convolution method for such networks, which significantly increases the speed of both training and inference. We also introduce a new form of angle loss regression to improve the orientation estimation performance and a new data augmentation approach that can enhance the convergence speed and performance. The proposed network produces state-of-the-art results on the KITTI 3D object detection benchmarks while maintaining a fast inference speed.",10.3390/s18103337
J,"Armstrong, J",OFDM for Optical Communications,"Orthogonal frequency division multiplexing (OFDM) is a modulation technique which is now used in most new and emerging broadband wired and wireless communication systems because it is an effective solution to intersymbol interference caused by a dispersive channel. Very recently a number of researchers have shown that OFDM is also a promising technology for optical communications. This paper gives a tutorial overview of OFDM highlighting the aspects that are likely to be important in optical applications. To achieve good performance in optical systems OFDM must be adapted in various ways. The constraints imposed by single mode optical fiber, multimode optical fiber and optical wireless are discussed and the new forms of optical OFDM which have been developed are outlined. The main drawbacks or OFDM are its high peak to average power ratio and its sensitivity to phase noise and frequency offset. The impairments that these cause are described and their implications for optical systems discussed.",10.1109/JLT.2008.2010061
J,"Li, XM; Chen, H; Qi, XJ; Dou, Q; Fu, CW; Heng, PA",H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation From CT Volumes,"Liver cancer is one of the leading causes of cancer death. To assist doctors in hepatocellular carcinoma diagnosis and treatment planning, an accurate and automatic liver and tumor segmentation method is highly demanded in the clinical practice. Recently, fully convolutional neural networks (FCNs), including 2-D and 3-D FCNs, serve as the backbone in many volumetric image segmentation. However, 2-D convolutions cannot fully leverage the spatial information along the third dimension while 3-D convolutions suffer from high computational cost and GPU memory consumption. To address these issues, we propose a novel hybrid densely connected UNet (H-DenseUNet), which consists of a 2-D DenseUNet for efficiently extracting intra-slice features and a 3-D counterpart for hierarchically aggregating volumetric contexts under the spirit of the auto-context algorithm for liver and tumor segmentation. We formulate the learning process of the H-DenseUNet in an end-to-endmanner, where the intra-slice representations and inter-slice features can be jointly optimized through a hybrid feature fusion layer. We extensively evaluated our method on the data set of the MICCAI 2017 Liver Tumor Segmentation Challenge and 3DIRCADb data set. Our method outperformed other state-of-the-arts on the segmentation results of tumors and achieved very competitive performance for liver segmentation even with a single model.",10.1109/TMI.2018.2845918
J,"Jolliet, O; Margni, M; Charles, R; Humbert, S; Payet, J; Rebitzer, G; Rosenbaum, R",IMPACT 2002+: A new life cycle impact assessment methodology,"The new IMPACT 2002+ life cycle impact assessment methodology proposes a feasible implementation of a combined midpoint/ damage approach, linking all types of life cycle inventory results (elementary flows and other interventions) via 14 midpoint categories to four damage categories. For IMPACT 2002+, new concepts and methods have been developed, especially for the comparative assessment of human toxicity and ecotoxicity. Human Damage Factors are calculated for carcinogens and non-carcinogens, employing intake fractions, best estimates of dose-response slope factors, as well as severities. The transfer of contaminants into the human food is no more based on consumption surveys, but accounts for agricultural and livestock production levels. Indoor and outdoor air emissions can be compared and the intermittent character of rainfall is considered. Both human toxicity and ecotoxicity effect factors are based on mean responses rather than on conservative assumptions. Other midpoint categories are adapted from existing characterizing methods (Eco-indicator 99 and CML 2002). All midpoint scores are expressed in units of a reference substance and related to the four damage categories human health, ecosystem quality, climate change, and resources. Normalization can be performed either at midpoint or at damage level. The IMPACT 2002+ method presently provides characterization factors for almost 1500 different LCI-results, which can be downloaded at http://www.epfl.ch/impact",10.1007/BF02978505
J,"Wang, Q; Mao, ZD; Wang, B; Guo, L",Knowledge Graph Embedding: A Survey of Approaches and Applications,"Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.",10.1109/TKDE.2017.2754499
J,"Guerrero, JM; Chandorkar, M; Lee, TL; Loh, PC",Advanced Control Architectures for Intelligent Microgrids-Part I: Decentralized and Hierarchical Control,"This paper presents a review of advanced control techniques for microgrids. This paper covers decentralized, distributed, and hierarchical control of grid-connected and islanded microgrids. At first, decentralized control techniques for microgrids are reviewed. Then, the recent developments in the stability analysis of decentralized controlled microgrids are discussed. Finally, hierarchical control for microgrids that mimic the behavior of the mains grid is reviewed.",10.1109/TIE.2012.2194969
J,"Gao, F; Masek, J; Schwaller, M; Hall, F",On the blending of the Landsat and MODIS surface reflectance: Predicting daily Landsat surface reflectance,"The 16-day revisit cycle of Landsat has long limited its use for studying global biophysical processes, which evolve rapidly during the growing season. In cloudy areas of the Earth, the problem is compounded, and researchers are fortunate to get two to three clear images per year. At the same time, the coarse resolution of sensors such as the Advanced Very High Resolution Radiometer and Moderate Resolution Imaging Spectroradiometer (MODIS) limits the sensors' ability to quantify biophysical processes in heterogeneous landscapes. In this paper, the authors present a new spatial and temporal adaptive reflectance fusion model (STARFM) algorithm to blend Landsat and MODIS surface reflectance. Using this approach, high-frequency temporal information from MODIS and high-resolution spatial information from Landsat can be blended for applications that require high resolution in both time and space. The MODIS daily 500-m surface reflectance and the 16-day repeat cycle Landsat Enhanced Thematic Mapper Plus (ETM+) 30-m surface reflectance are used to produce a synthetic ""daily"" surface reflectance product at ETM+ spatial resolution. The authors present results both with simulated (model) data and actual Landsat/MODIS acquisitions. In general, the STARFM accurately predicts surface reflectance at an effective resolution close to that of the ETM+. However, the performance depends on the characteristic patch size of the landscape and degrades somewhat when used on extremely heterogeneous fine-grained landscapes.",10.1109/TGRS.2006.872081
J,"Pointcheval, D; Stern, J",Security arguments for digital signatures and blind signatures,"Since the appearance of public-key cryptography in the seminal Diffie-Hellman paper, many new schemes have been proposed and many have been broken. Thus, the simple fact that a cryptographic algorithm withstands cryptanalytic attacks for several years is often considered as a kind of validation procedure. A much more convincing line of research has tried to provide ""provable"" security for cryptographic protocols. Unfortunately, in many cases, provable security is at the cost of a considerable loss in terms of efficiency. Another way to achieve some kind of provable security is to identify concrete cryptographic objects, such as hash functions, with ideal random objects and to use arguments from relativized complexity theory. The model underlying this approach is often called the ""random oracle model."" We use the word ""arguments"" for security results proved in this model. As usual, these arguments are relative to well-established hard algorithmic problems such as factorization or the discrete logarithm. In this paper we offer security arguments for a large class of known signature schemes. Moreover, we give for the first time an argument for a very slight variation of the well-known El Gamal signature scheme. In spite of the existential forgery of the original scheme, we prove that our variant resists existential forgeries even against an adaptively chosen-message attack. This is provided that the discrete logarithm problem is hard to solve. Next, we study the security of blind signatures which are the most important ingredient for anonymity in off-line electronic cash systems. We first define an appropriate notion of security related to the setting of electronic cash. We then propose new schemes for which one can provide security arguments.",10.1007/s001450010003
J,"Wong, HSP; Raoux, S; Kim, S; Liang, JL; Reifenberg, JP; Rajendran, B; Asheghi, M; Goodson, KE",Phase Change Memory,"In this paper, recent progress of phase change memory (PCM) is reviewed. The electrical and thermal properties of phase change materials are surveyed with a focus on the scalability of the materials and their impact on device design. Innovations in the device structure, memory cell selector, and strategies for achieving multibit operation and 3-D, multilayer high-density memory arrays are described. The scaling properties of PCM are illustrated with recent experimental results using special device test structures and novel material synthesis. Factors affecting the reliability of PCM are discussed.",10.1109/JPROC.2010.2070050
J,"Zhao, H; Gallo, O; Frosio, I; Kautz, J",Loss Functions for Image Restoration With Neural Networks,"Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is l(2). In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.",10.1109/TCI.2016.2644865
J,"Angelidaki, I; Alves, M; Bolzonella, D; Borzacconi, L; Campos, JL; Guwy, AJ; Kalyuzhnyi, S; Jenicek, P; van Lier, JB",Defining the biomethane potential (BMP) of solid organic wastes and energy crops: a proposed protocol for batch assays,"The application of anaerobic digestion technology is growing worldwide because of its economic and environmental benefits. As a consequence, a number of studies and research activities dealing with the determination of the biogas potential of solid organic substrates have been carrying out in the recent years. Therefore, it is of particular importance to define a protocol for the determination of the ultimate methane potential for a given solid substrates. In fact, this parameter determines, to a certain extent, both design and economic details of a biogas plant. Furthermore, the definition of common units to be used in anaerobic assays is increasingly requested from the scientific and engineering community. This paper presents some guidelines for biomethane potential assays prepared by the Task Group for the Anaerobic Biodegradation, Activity and Inhibition Assays of the Anaerobic Digestion Specialist Group of the International Water Association. This is the first step for the definition of a standard protocol.",10.2166/wst.2009.040
J,"Chen, BL; Zhou, DD; Zhu, LZ",Transitional adsorption and partition of nonpolar and polar aromatic contaminants by biochars of pine needles with different pyrolytic temperatures,"The combined adsorption and partition effects of biochars with varying fractions of noncarbonized organic matter have not been clearly defined. Biochars, produced by pyrolysis of pine needles at different temperatures (100-700 degrees C, referred as P100-P700), were characterized by elemental analysis, BET-N(2) surface areas and FIR. Sorption isotherms of naphthalene, nitrobenzene, and m-dinitrobenzene from water to the biochars were compared. Sorption parameters (N and log K(f)) are linearly related to sorbent aromaticities, which increase with the pyrolytic temperature. Sorption mechanisms of biochars are evolved from partitioning-dominant at low pyrolytic temperatures to adsorption-dominant at higher pyrolytic temperatures. The quantitative contributions of adsorption and partition are determined by the relative carbonized and noncarbonized fractions and their surface and bulk properties. The partition of P100-P300 biochars originates from an amorphous aliphatic fraction, which is enhanced with a reduction of the substrate polarity; for P400-P600, the partition occurs with a condensed aromatic core that diminishes with a further reduction of the polarity. Simultaneously, the adsorption component exhibits a transition from a polarity-selective (P200-P400) to a porosity-selective (P500-P600) process, and displays no selectivity with P700 and AC in which the adsorptive saturation capacities are comparable to predicted values based on the monolayer surface coverage of molecule.",10.1021/es8002684
J,"Rodríguez, JJ; Kuncheva, LI",Rotation forest:: A new classifier ensemble method,"We propose a method for generating classifier ensembles based on feature extraction. To create the training data for a base classifier, the feature set is randomly split into K subsets (K is a parameter of the algorithm) and Principal Component Analysis (PCA) is applied to each subset. All principal components are retained in order to preserve the variability information in the data. Thus, K axis rotations take place to form the new features for a base classifier. The idea of the rotation approach is to encourage simultaneously individual accuracy and diversity within the ensemble. Diversity is promoted through the feature extraction for each base classifier. Decision trees were chosen here because they are sensitive to rotation of the feature axes, hence the name ""forest."" Accuracy is sought by keeping all principal components and also using the whole data set to train each base classifier. Using WEKA, we examined the Rotation Forest ensemble on a random selection of 33 benchmark data sets from the UCI repository and compared it with Bagging, AdaBoost, and Random Forest. The results were favorable to Rotation Forest and prompted an investigation into diversity-accuracy landscape of the ensemble models. Diversity-error diagrams revealed that Rotation Forest ensembles construct individual classifiers which are more accurate than these in AdaBoost and Random Forest, and more diverse than these in Bagging, sometimes more accurate as well.",10.1109/TPAMI.2006.211
J,"Richter, H; Howard, JB",Formation of polycyclic aromatic hydrocarbons and their growth to soot - a review of chemical reaction pathways,"The generation by combustion processes of airborne species of current health concern such as polycyclic aromatic hydrocarbons (PAH) and soot particles necessitates a detailed understanding of chemical reaction pathways responsible for their formation. The present review discusses a general scheme of PAH formation and sequential growth of PAH by reactions with stable and radical species, including single-ring aromatics, other PAH and acetylene, followed by the nucleation or inception of small soot particles, soot growth by coagulation and mass addition from gas phase species, and carbonization of the particulate material. Experimental and theoretical tools which have allowed the achievement of deeper insight into the corresponding chemical processes are presented. The significant roles of propargyl (C3H3) and cyclopentadienyl (C5H5) radicals in the formation of first aromatic rings in combustion of aliphatic fuels are discussed. Detailed kinetic modeling of well-defined combustion systems, such as premixed flames, for which sufficient experimental data for a quantitative understanding are available, is of increasing importance. Reliable thermodynamic and kinetic property data are also required for meaningful conclusions, and computational techniques for their determination are presented. Routes of ongoing and future research leading to mon detailed experimental data as well as computational approaches for the exploration of elementary reaction steps and the description of systems of increasing complexity are discussed. (C) 2000 Elsevier Science Ltd. All rights reserved.",10.1016/S0360-1285(00)00009-5
J,"Islam, SMR; Kwak, D; Kabir, MH; Hossain, M; Kwak, KS",The Internet of Things for Health Care: A Comprehensive Survey,"The Internet of Things (IoT) makes smart objects the ultimate building blocks in the development of cyber-physical smart pervasive frameworks. The IoT has a variety of application domains, including health care. The IoT revolution is redesigning modern health care with promising technological, economic, and social prospects. This paper surveys advances in IoT-based health care technologies and reviews the state-of-the-art network architectures/platforms, applications, and industrial trends in IoT-based health care solutions. In addition, this paper analyzes distinct IoT security and privacy features, including security requirements, threat models, and attack taxonomies from the health care perspective. Further, this paper proposes an intelligent collaborative security model to minimize security risk; discusses how different innovations such as big data, ambient intelligence, and wearables can be leveraged in a health care context; addresses various IoT and eHealth policies and regulations across the world to determine how they can facilitate economies and societies in terms of sustainable development; and provides some avenues for future research on IoT-based health care based on a set of open issues and challenges.",10.1109/ACCESS.2015.2437951
J,"Liu, JY; Hurt, RH",Ion Release Kinetics and Particle Persistence in Aqueous Nano-Silver Colloids,"Many important aspects of nanosilver behavior are influenced by the ionic activity associated with the particle suspension, including antibacterial potency, eukaryotic toxicity, environmental release, and particle persistence. The present study synthesizes pure, ion-free, citrate-stabilized nanosilver (nAg) colloids as model systems, and measures their time-dependent release of dissolved silver using centrifugal ultrafiltration and atomic absorption spectroscopy. Ion release is shown to be a cooperative oxidation process requiring both dissolved dioxygen and protons. It produces peroxide intermediates, and proceeds to complete reactive dissolution under some conditions. Ion release rates increase with temperature in the range 0-37 degrees C, and decrease with increasing pH or addition of humic or fulvic acids. Sea salts have only a minor effect on dissolved silver release. Silver nanoparticle surfaces can adsorb Ag+, so even simple colloids contain three forms of silver: Ag-0 solids, free Ag+ or its complexes, and surface-adsorbed Ag+. Both thermodynamic analysis and kinetic measurements indicate that Ag-0 nanciparticles will not be persistent in realistic environmental compartments containing dissolved oxygen. An empirical kinetic law is proposed that reproduces the observed effects of dissolution time, pH, humic/fulvic acid content, and temperature observed here in the low range of nanosilver concentration most relevant for the environment.",10.1021/es9035557
J,"Koutsopoulos, S",Synthesis and characterization of hydroxyapatite crystals: A review study on the analytical methods,"For the synthesis of hydroxyapatite crystals from aqueous solutions three preparation methods were employed. From the experimental processes and the characterization of the crystals it was concluded that aging and precipitation kinetics are critical for the purity of the product and its crystallographic characteristics. The authentication details are presented along with the results from infrared spectroscopy, X-ray powder diffraction, Raman spectroscopy, transmission and scanning electron photographs, and chemical analysis. Analytical data for several calcium phosphates were collected from the literature, extensively reviewed, and the results were grouped and presented in tables to provide comparison with the data obtained here. (C) 2002 Wiley Periodicals, Inc.",10.1002/jbm.10280
J,"Larson, EC; Chandler, DM",Most apparent distortion: full-reference image quality assessment and the role of strategy,"The mainstream approach to image quality assessment has centered around accurately modeling the single most relevant strategy employed by the human visual system (HVS) when judging image quality (e. g., detecting visible differences, and extracting image structure/information). In this work, we suggest that a single strategy may not be sufficient; rather, we advocate that the HVS uses multiple strategies to determine image quality. For images containing near-threshold distortions, the image is most apparent, and thus the HVS attempts to look past the image and look for the distortions (a detection-based strategy). For images containing clearly visible distortions, the distortions are most apparent, and thus the HVS attempts to look past the distortion and look for the image's subject matter (an appearance-based strategy). Here, we present a quality assessment method [most apparent distortion (MAD)], which attempts to explicitly model these two separate strategies. Local luminance and contrast masking are used to estimate detection-based perceived distortion in high-quality images, whereas changes in the local statistics of spatial-frequency components are used to estimate appearance-based perceived distortion in low-quality images. We show that a combination of these two measures can perform well in predicting subjective ratings of image quality. (C) 2010 SPIE and IS&T. [DOI: 10.1117/1.3267105]",10.1117/1.3267105
J,"MacFarlane, DR; Tachikawa, N; Forsyth, M; Pringle, JM; Howlett, PC; Elliott, GD; Davis, JH; Watanabe, M; Simon, P; Angell, CA",Energy applications of ionic liquids,"Ionic liquids offer a unique suite of properties that make them important candidates for a number of energy related applications. Cation-anion combinations that exhibit low volatility coupled with high electrochemical and thermal stability, as well as ionic conductivity, create the possibility of designing ideal electrolytes for batteries, super-capacitors, actuators, dye sensitised solar cells and thermo-electrochemical cells. In the field of water splitting to produce hydrogen they have been used to synthesize some of the best performing water oxidation catalysts and some members of the protic ionic liquid family co-catalyse an unusual, very high energy efficiency water oxidation process. As fuel cell electrolytes, the high proton conductivity of some of the protic ionic liquid family offers the potential of fuel cells operating in the optimum temperature region above 100 degrees C. Beyond electrochemical applications, the low vapour pressure of these liquids, along with their ability to offer tuneable functionality, also makes them ideal as CO2 absorbents for post-combustion CO2 capture. Similarly, the tuneable phase properties of the many members of this large family of salts are also allowing the creation of phase-change thermal energy storage materials having melting points tuned to the application. This perspective article provides an overview of these developing energy related applications of ionic liquids and offers some thoughts on the emerging challenges and opportunities.",10.1039/c3ee42099j
J,"Wen, L; Li, XY; Gao, L; Zhang, YY",A New Convolutional Neural Network-Based Data-Driven Fault Diagnosis Method,"Fault diagnosis is vital in manufacturing system, since early detections on the emerging problem can save invaluable time and cost. With the development of smart manufacturing, the data-driven fault diagnosis becomes a hot topic. However, the traditional data-driven fault diagnosis methods rely on the features extracted by experts. The feature extraction process is an exhausted work and greatly impac7ts the final result. Deep learning (DL) provides an effective way to extract the features of raw data automatically. Convolutional neural network (CNN) is an effective DL method. In this study, a new CNN based on LeNet-5 is proposed for fault diagnosis. Through a conversion method converting signals into two-dimensional (2-D) images, the proposed method can extract the features of the converted 2-D images and eliminate the effect of handcrafted features. The proposed method which is tested on three famous datasets, including motor bearing dataset, self-priming centrifugal pump dataset, and axial piston hydraulic pump dataset, has achieved prediction accuracy of 99.79%, 99.481%, and 100%, respectively. The results have been compared with other DL and traditional methods, including adaptive deep CNN, sparse filter, deep belief network, and support vector machine. The comparisons show that the proposed CNN-based data-driven fault diagnosis method has achieved significant improvements.",10.1109/TIE.2017.2774777
J,"Witte, F",The history of biodegradable magnesium implants: A review,"Today, more than 200 years after the first production of metallic magnesium by Sir Humphry Davy in 1808, biodegradable magnesium-based metal implants are currently breaking the paradigm in biomaterial science to develop only highly corrosion resistant metals. This groundbreaking approach to temporary metallic implants is one of the latest developments in biomaterials science that is being rediscovered. It is a challenging topic, and several secrets still remain that might revolutionize various biomedical implants currently in clinical use. Magnesium alloys were investigated as implant materials long ago. A very early clinical report was given in 1878 by the physician Edward C. Huse. He used magnesium wires as ligature for bleeding vessels. Magnesium alloys for clinical use were explored during the last two centuries mainly by surgeons with various clinical backgrounds, such as cardiovascular, musculoskeletal and general surgery. Nearly all patients benefited from the treatment with magnesium implants. Although most patients experienced subcutaneous gas cavities caused by rapid implant corrosion, most patients had no pain and almost no infections were observed during the postoperative follow-up. This review critically summarizes the in vitro and in vivo knowledge and experience that has been reported on the use of magnesium and its alloys to advance the field of biodegradable metals. (C) 2010 Acta Materialia Inc. Published by Elsevier Ltd. All rights reserved.",10.1016/j.actbio.2010.02.028
J,"Bridgwater, AV",Renewable fuels and chemicals by thermal processing of biomass,"Bio-energy is now accepted as having the potential to provide the major part of the projected renewable energy provisions of the future. There are three main routes to providing these bio-fuels-biological conversion, physical conversion and thermal conversion-all of which employ a range of chemical reactors configurations and designs. This review concentrates on thermal conversion processes and particularly the reactors that have been developed to provide the necessary conditions to optimise performance. A number of primary and secondary products can be derived as gas, liquid and solid fuels and electricity as well as a considerable number of chemicals. The basic conversion processes are summarised with their products and the main technical and non-technical barriers to implementation are identified. (C) 2002 Elsevier Science B.V. All rights reserved.",10.1016/S1385-8947(02)00142-0
J,"Tao, F; Cheng, JF; Qi, QL; Zhang, M; Zhang, H; Sui, FY","Digital twin-driven product design, manufacturing and service with big data","Nowadays, along with the application of new-generation information technologies in industry and manufacturing, the big data-driven manufacturing era is coming. However, although various big data in the entire product lifecycle, including product design, manufacturing, and service, can be obtained, it can be found that the current research on product lifecycle data mainly focuses on physical products rather than virtual models. Besides, due to the lack of convergence between product physical and virtual space, the data in product lifecycle is isolated, fragmented, and stagnant, which is useless for manufacturing enterprises. These problems lead to low level of efficiency, intelligence, sustainability in product design, manufacturing, and service phases. However, physical product data, virtual product data, and connected data that tie physical and virtual product are needed to support product design, manufacturing, and service. Therefore, how to generate and use converged cyber-physical data to better serve product lifecycle, so as to drive product design, manufacturing, and service to be more efficient, smart, and sustainable, is emphasized and investigated based on our previous study on big data in product lifecycle management. In this paper, a new method for product design, manufacturing, and service driven by digital twin is proposed. The detailed application methods and frameworks of digital twin-driven product design, manufacturing, and service are investigated. Furthermore, three cases are given to illustrate the future applications of digital twin in the three phases of a product respectively.",10.1007/s00170-017-0233-1
J,"Kouro, S; Cortés, P; Vargas, R; Ammann, U; Rodríguez, J",Model Predictive Control-A Simple and Powerful Method to Control Power Converters,"This paper presents a detailed description of Finite Control Set Model Predictive Control (FCS-MPC) applied to power converters. Several key aspects related to this methodology are, in depth, presented and compared with traditional power converter control techniques, such as linear controllers with pulsewidth-modulation-based methods. The basic concepts, operating principles, control diagrams, and results are used to provide a comparison between the different control strategies. The analysis is performed on a traditional three-phase voltage source inverter, used as a simple and comprehensive reference frame. However, additional topologies and power systems are addressed to highlight differences, potentialities, and challenges of FCS-MPC. Among the conclusions are the feasibility and great potential of FCS-MPC due to present-day signal-processing capabilities, particularly for power systems with a reduced number of switching states and more complex operating principles, such as matrix converters. In addition, the possibility to address different or additional control objectives easily in a single cost function enables a simple, flexible, and improved performance controller for power-conversion systems.",10.1109/TIE.2008.2008349
J,"Agnew, SR; Duygulu, Ö",Plastic anisotropy and the role of non-basal slip in magnesium alloy AZ31B,"Mechanistic explanations for the plastic behavior of a wrought magnesium alloy are developed using a combination of experimental and simulation techniques. Parameters affecting the practical sheet formability, such as strain hardening rate, strain rate sensitivity, the degree of anisotropy, and the stresses and strains at fracture, are examined systematically by conducting tensile tests of variously oriented samples at a range of temperatures (room temperature to 250 degreesC and strain rates (10(-5)-0.1s(-1)). Polycrystal plasticity simulations are used to model the observed anisotropy and texture evolution. Strong in-plane anisotropy observed at low temperatures is attributed to the initial texture and the greater than anticipated non-basal cross-slip of dislocations with dropadrop type Burgers vectors. The agreement between the measured and simulated anisotropy and texture is further validated by direct observations of the dislocation microstructures using transmission electron microscopy. The increase in the ductility with temperature is accompanied by a decrease in the flow stress, an increase in the strain rate sensitivity, and a decrease in the normal anisotropy. Polycrystal simulations indicate that an increased activity of non-basal, dropc + adrop, dislocations provides a self-consistent explanation for the observed changes in the anisotropy with increasing temperature. (C) 2004 Elsevier Ltd. All rights reserved.",10.1016/j.ijplas.2004.05.018
J,"Eckert, F; Klamt, A",Fast solvent screening via quantum chemistry: COSMO-RS approach,"COSMO-RS, a general and fast methodology for the a priori prediction of thermophysical data of liquids is presented. It is based on cheap unimolecular quantum chemical calculations, which, combined with exact statistical thermodynamics, provide the information necessary for the evaluation of molecular interactions in liquids. COSMO-RS is an alternative to structure interpolating group contribution methods. The method is independent of experimental data and generally applicable. A methodological comparison with group contribution methods is given. The applicability, of the COSMO-RS method to the goal of solvent screening is demonstrated at various examples of vapor-liquid-, liquid-liquid-, solid-liquid-equilibria and vapor-pressure predictions.",10.1002/aic.690480220
J,"Vo, BN; Ma, WK",The Gaussian mixture probability hypothesis density filter,"A new recursive algorithm is proposed for jointly estimating the time-varying number of targets and their states from a sequence of observation sets in the presence of data association uncertainty, detection uncertainty, noise, and false alarms. The approach involves modelling the respective collections of targets and measurements as random finite sets and applying the probability hypothesis density (PHD) recursion to propagate the posterior intensity, which is a first-order statistic of the random finite set of targets, in time. At present, there is no closed-form solution to the PHD recursion. This paper shows that under linear, Gaussian assumptions on the target dynamics and birth process, the posterior intensity at any time step is a Gaussian mixture. More importantly, closed-form recursions for propagating the means, covariances, and weights of the constituent Gaussian components of the posterior intensity are derived. The proposed algorithm combines these recursions with a strategy for managing the number of Gaussian components to increase efficiency. This algorithm is extended to accommodate mildly nonlinear target dynamics using approximation strategies from the extended and unscented Kalman filters.",10.1109/TSP.2006.881190
