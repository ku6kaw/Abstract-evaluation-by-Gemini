ID,Abstract,rule1,rule2,rule3,rule4,rule5,rule6,rule7,rule8,rule9,rule10,rule11,rule12,rule13,rule14,rule15,rule16,rule17,rule18,rule19,rule20,rule21,rule22,rule23,rule24,rule25,rule26,rule27,rule28,rule29,rule30,rule31,rule32,rule33
0,"Grid-connected inverters are known to become unstable when the grid impedance is high. Existing approaches to analyzing such instability are based on inverter control models that account for the grid impedance and the coupling with other grid-connected inverters. A new method to determine inverter-grid system stability using only the inverter output impedance and the grid impedance is developed in this paper. It will be shown that a grid-connected inverter will remain stable if the ratio between the grid impedance and the inverter output impedance satisfies the Nyquist stability criterion. This new impedance-based stability criterion is a generalization to the existing stability criterion for voltage-source systems, and can be applied to all current-source systems. A single-phase solar inverter is studied to demonstrate the application of the proposed method.",yes,no,no,yes,yes,no,yes,no,no,no,yes,yes,no,no,no,yes,yes,yes,no,yes,yes,yes,no,no,yes,yes,yes,yes,yes,no,no,yes,yes
1,"This article provides an up-to-date perspective on the use of anion-exchange membranes in fuel cells, electrolysers, redox flow batteries, reverse electrodialysis cells, and bioelectrochemical systems (e.g. microbial fuel cells). The aim is to highlight key concepts, misconceptions, the current state-of-the-art, technological and scientific limitations, and the future challenges (research priorities) related to the use of anion-exchange membranes in these energy technologies. All the references that the authors deemed relevant, and were available on the web by the manuscript submission date (30th April 2014), are included.",yes,yes,no,no,yes,no,yes,yes,no,no,no,no,no,no,no,no,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,yes,yes,yes,yes
2,"The transition within business from a linear to a circular economy brings with it a range of practical challenges for companies. The following question is addressed: What are the product design and business model strategies for companies that want to move to a circular economy model? This paper develops a framework of strategies to guide designers and business strategists in the move from a linear to a circular economy. Building on Stahel, the terminology of slowing, closing, and narrowing resource loops is introduced. A list of product design strategies, business model strategies, and examples for key decision-makers in businesses is introduced, to facilitate the move to a circular economy. This framework also opens up a future research agenda for the circular economy.",yes,no,no,yes,yes,no,yes,yes,no,no,yes,yes,no,no,no,no,yes,yes,no,yes,no,no,no,yes,yes,yes,yes,yes,no,yes,no,yes,yes
3,"Wide bandgap semiconductors show superior material properties enabling potential power device operation at higher temperatures, voltages, and switching speeds than current Si technology. As a result, a new generation of power devices is being developed for power converter applications in which traditional Si power devices show limited operation. The use of these new power semiconductor devices will allow both an important improvement in the performance of existing power converters and the development of new power converters, accounting for an increase in the efficiency of the electric energy transformations and a more rational use of the electric energy. At present, SiC and GaN are the more promising semiconductor materials for these new power devices as a consequence of their outstanding properties, commercial availability of starting material, and maturity of their technological processes. This paper presents a review of recent progresses in the development of SiC- and GaN-based power semiconductor devices together with an overall view of the state of the art of this new device generation.",yes,no,no,yes,no,no,no,no,no,no,yes,yes,no,no,no,no,yes,yes,no,yes,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,yes,yes
4,"Wide bandgap, semiconductors are extremely attractive for the gamut of power electronics applications from power conditioning to microwave transmitters for communications and radar Of the various materials and device technologies, the AlGaN/GaN high-electron mobility transistor seems the most promising. This paper attempts to present the status of the technology and the market with a view of highlighting both the progress and the remaining problems.",yes,yes,no,no,no,no,no,happy,happy,no,yes,yes,no,no,no,no,yes,yes,happy,happy,no,no,yes,no,yes,yes,happy,happy,no,no,yes,yes,yes
5,"Biodiesel, defined as the mono-alkyl esters of vegetable oils or animal fats, is an ""alternative"" diesel fuel that is becoming accepted in a steadily growing number of countries around the world. Since the source of biodiesel varies with the location and other sources such as recycled oils are continuously gaining interest, it is important to possess data on how the various fatty acid profiles of the different sources can influence biodiesel fuel properties. The properties of the various individual fatty esters that comprise biodiesel determine the overall fuel properties of the biodiesel fuel. In turn, the properties of the various fatty esters are determined by the structural features of the fatty acid and the alcohol moieties that comprise a fatty ester. Structural features that influence the physical and fuel properties of a fatty ester molecule are chain length, degree of unsaturation, and branching of the chain. Important fuel properties of biodiesel that are influenced by the fatty acid profile and, in turn, by the structural features of the various fatty esters are cetane number and ultimately exhaust emissions, heat of combustion, cold flow, oxidative stability, viscosity, and lubricity. Published by Elsevier B.V.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6,"Experimental data are presented that clearly demonstrate the scope of application of peak signal-to-noise ratio (PSNR) as a video quality metric. It is shown that as long as the video content and the codec type are not changed, PSNR is a valid quality measure. However, when the content is changed, correlation between subjective quality and PSNR is highly reduced. Hence PSNR cannot be a reliable method for assessing the video quality across different video contents.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8,"A new method is presented for robustly estimating multiple view relations from point correspondences. The method comprises two parts. The first is a new robust estimator MLESAC which is a generalization of the RANSAC estimator. It adopts the same sampling strategy as RANSAC to generate putative solutions, but chooses the solution that maximizes the likelihood rather than just the number of inliers. The second part of the algorithm is a general purpose method for automatically parameterizing these relations, using the output of MLESAC. A difficulty with multiview image relations is that there are often nonlinear constraints between the parameters, making optimization a difficult task. The parameterization method overcomes the difficulty of nonlinear constraints and conducts a constrained optimization. The method is general and its use is illustrated for the estimation of fundamental matrices, image-image homographies, and quadratic transformations. Results are given for both synthetic and real images. It is demonstrated that the method gives results equal or superior to those of previous approaches, (C) 2000 Academic Press.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9,"This paper proposes a step-by-step procedure for designing the LCL filter of a front-end three-phase active rectifier. The primary goal is to reduce the switching frequency ripple at a reasonable cost, while at the same time achieving a high-performance front-end rectifier (as characterized by a rapid dynamic response and good stability margin). An example LCL filter design is reported and a filter has been built and tested using the values obtained from this design. The experimental results demonstrate the performance of the design procedure both for the LCL filter and for the rectifier controller. The system is stable and the grid current harmonic content is low both in the low- and high-frequency ranges. Moreover, the good agreement that was obtained between simulation and experimental results validates the proposed approach. Hence, the design procedure and the simulation model provide a powerful tool to design an LCL-filter based active rectifier while avoiding trial-and-error procedures that can result in having to build several filter prototypes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10,"To date, because of the computational complexity of using a general type-2 fuzzy set (T2 FS) in a T2 fuzzy logic system (FLS), most people only use an interval T2 FS, the result being an interval T2 FLS (IT2 FLS). Unfortunately, there is a heavy educational burden even to using an IT2 FLS. This burden has to do with first having to learn general T2 FS mathematics, and then specializing it to an IT2 FSs. In retrospect, we believe that requiring a person to use T2 FS mathematics represents a barrier to the use of an IT2 FLS. In this paper, we demonstrate that it is unnecessary to take the route from general T2 FS to IT2 FS, and that all of the results that are needed to implement an IT2 FLS can be obtained using T1 FS mathematics. As such, this paper is a novel tutorial that makes an IT2 FLS much more accessible to all readers of this journal. We can now develop an IT2 FLS in a much more straightforward way.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11,"Motivated by applications to sensor, peer-to-peer, and ad hoc networks, we study distributed algorithms, also known as gossip algorithms, for exchanging information and for computing in an arbitrarily connected network of nodes. The topology of such networks changes continuously as new nodes join and old nodes leave the network. Algorithms for such networks need to be robust against changes in topology. Additionally, nodes in sensor networks operate under limited computational, communication, and energy resources. These constraints have motivated the design of ""gossip"" algorithms: schemes which distribute the computational burden and in which a node communicates with a randomly chosen neighbor. We analyze the averaging problem under the gossip constraint for an arbitrary network graph, and find that the averaging time of a gossip algorithm depends on the second largest eigenvalue of a doubly stochastic matrix characterizing the algorithm. Designing the fastest gossip algorithm corresponds to minimizing this eigenvalue, which is a semidefinite program (SDP). In general, SDPs cannot be solved in a distributed fashion; however, exploiting problem structure, we propose a distributed subgradient method that solves the optimization problem over the network. The relation of averaging time to the second largest eigenvalue naturally relates it to the mixing time of a random walk with transition probabilities derived from the gossip algorithm. We use this connection to study the performance and scaling of gossip algorithms on two popular networks: Wireless Sensor Networks, which are modeled as Geometric Random Graphs, and the Internet graph under the so-called Preferential Connectivity (PC) model.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12,"In virtually all published literature wherein closure between gravimetric and chemical measurements is tested, the concentration of particulate organics is estimated by multiplying the measured concentration of organic carbon (micrograms carbon/cubic meter air) by a factor of 1.2-1.4. This factor, which is an estimate of the average molecular weight per carbon weight for the organic aerosol, stems from very limited theoretical and laboratory studies conducted during the 1970s. This investigation suggests that 1.4 is the lowest reasonable estimate for the organic molecular weight per carbon weight for an urban aerosol and that 1.4 does not accurately represent the average organic molecular weight per carbon weight for a nonurban aerosol. Based on the current evaluation, ratios of 1.6 +/- 0.2 for urban aerosols and 2.1 +/- 0.2 for nonurban aerosols appear to be more accurate. Measurements are recommended. Literature values also suggest that 1.2 g/cm(3) is a reasonable estimate for the organic aerosol density. This quantity is needed to convert between geometric and aerodynamic size distributions (e.g., to predict aerosol optical properties and understand cloud nucleating properties).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13,"Designing and tuning a proportional-integral-derivative (PID) controller appears to be conceptually intuitive, but can be hard in practice, if multiple (and often conflicting) objectives such as short transient and high stability are to be achieved. Usually, initial designs obtained by all means need to be adjusted repeatedly through computer simulations until the closed-loop system performs or compromises as desired. This stimulates the development of ""intelligent"" tools that can assist engineers to achieve the best overall PID control for the entire operating envelope. This development has further led to the incorporation of some advanced tuning algorithms into PID hardware modules. Corresponding to these developments, this paper presents a modern overview of functionalities and tuning methods in patents, software packages and commercial hardware modules. It is seen that many PID variants have been developed in order to improve transient performance, but standardising and modularising PID control are desired, although challenging. The inclusion of system identification and ""intelligent"" techniques in software based PID systems helps automate the entire design and tuning process to a useful degree. This should also assist future development of ""plug-and-play"" PID controllers that are widely applicable and can be set up easily and operate optimally for enhanced productivity, improved quality and reduced maintenance requirements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14,"A new simulation approach, called 'subset simulation', is proposed to compute small failure probabilities encountered in reliability analysis of engineering systems. The basic idea is to express the failure probability as a product of larger conditional failure probabilities by introducing intermediate failure events. With a proper choice of the conditional events, the conditional failure probabilities can be made sufficiently large so that they can be estimated by means of simulation with a small number of samples. The original problem of calculating a small failure probability, which is computationally demanding, is reduced to calculating a sequence of conditional probabilities, which can be readily and efficiently estimated by means of simulation. The conditional probabilities cannot be estimated efficiently by a standard Monte Carlo procedure, however, and so a Markov chain Monte Carlo simulation (MCS) technique based on the Metropolis algorithm is presented for their estimation. The proposed method is robust to the number of uncertain parameters and efficient in computing small probabilities. The efficiency of the method is demonstrated by calculating the first-excursion probabilities for a linear oscillator subjected to white noise excitation and for a five-story nonlinear hysteretic shear building under uncertain seismic excitation. (C) 2001 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15,"Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth-generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the context of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16,"In large-scale visual recognition and image retrieval tasks, feature vectors, such as Fisher vector (FV) or the vector of locally aggregated descriptors (VLAD), have achieved state-of-the-art results. However, the combination of the large numbers of examples and high-dimensional vectors necessitates dimensionality reduction, in order to reduce its storage and CPU costs to a reasonable range. In spite of the popularity of various feature compression methods, this paper shows that the feature (dimension) selection is a better choice for high-dimensional FV/VLAD than the feature (dimension) compression methods, e.g., product quantization. We show that strong correlation among the feature dimensions in the FV and the VLAD may not exist, which renders feature selection a natural choice. We also show that, many dimensions in FV/VLAD are noise. Throwing them away using feature selection is better than compressing them and useful dimensions altogether using feature compression methods. To choose features, we propose an efficient importance sorting algorithm considering both the supervised and unsupervised cases, for visual recognition and image retrieval, respectively. Combining with the 1-bit quantization, feature selection has achieved both higher accuracy and less computational cost than feature compression methods, such as product quantization, on the FV and the VLAD image representations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17,"Significant improvements of computer facilities and computational fluid dynamics (CFD) software in recent years have enabled prediction and assessment of the pedestrian wind environment around buildings in the design stage. Therefore, guidelines are required that summarize important points in using the CFD technique for this purpose. This paper describes guidelines proposed by the Working Group of the Architectural Institute of Japan (AIJ). The feature of these guidelines is that they are based on cross-comparison between CFD predictions, wind tunnel test results and field measurements for seven test cases used to investigate the influence of many kinds of computational conditions for various flow fields. (c) 2008 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18,"Response surface methodology (RSM) is the most popular optimization method used in recent years. There are so many works based on the application of RSM in chemical and biochemical process. On the other hand, few articles were published about the limitation and usability of it. In this paper, we looked at some of the RSM articles published during the last few years. We tried to identify common mistakes made in the application and the limitations of RSM. We asked ourselves two important questions. These questions are ""Can RSM be used for optimization of all chemical and biochemical processes without any limitation?"" and ""Is RSM usable for other purposes (determination of reaction kinetics, stability or evaluation of kinetic constants etc.) in addition to optimization?"". We were able to answer these questions based on the observations obtained from reviewed articles. We believe that the answers will be helpful for researchers, who will use RSM in their future studies. (c) 2005 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19,"We present a survey of formation control of multi-agent systems. Focusing on the sensing capability and the interaction topology of agents, we categorize the existing results into position-, displacement-, and distance-based control. We then summarize problem formulations, discuss distinctions, and review recent results of the formation control schemes. Further we review some other results that do not fit into the categorization. (C) 2014 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
20,This paper presents a global non-singular terminal sliding mode controller for rigid manipulators. A new terminal sliding mode manifold is first proposed for the second-order system to enable the elimination of the singularity problem associated with conventional terminal sliding mode control. The time taken to reach the equilibrium point from any initial state is guaranteed to be finite time. The proposed terminal sliding mode controller is then applied to the control of n-link rigid manipulators. Simulation results are presented to validate the analysis. (C) 2002 Elsevier Science Ltd. All rights reserved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,"W-4 is a real time visual surveillance system for detecting and tracking multiple people and monitoring their activities in an outdoor environment. It operates on monocular gray-scale video imagery, or on video imagery from an infrared camera. W-4 employs a combination of shape analysis and tracking to locate people and their parts (head, hands, feet, torso) and to create models of people's appearance so that they can be tracked through interactions such as occlusions. It can determine whether a foreground region contains multiple people and can segment the region into its constituent people and track them. W-4 can also determine whether people are carrying objects, and can segment objects from their silhouettes, and construct appearance models for them so they can be identified in subsequent frames. W-4 can recognize events between people and objects, such as depositing an object, exchanging bags, or removing an object. It runs at 25 Hz for 320x240 resolution images on a 400 Mhz dual-Pentium II PC.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,"Rapid Prototyping (RP) technologies provide the ability to fabricate initial prototypes from various model materials. Stratasys Fused Deposition Modeling (FDM) is a typical RP process that can fabricate prototypes out of ABS plastic. To predict the mechanical behavior of FDM parts, it is critical to understand the material properties of the raw FDM process material, and the effect that FDM build parameters have on anisotropic material properties. This paper characterizes the properties of ABS parts fabricated by the FDM 1650. Using a Design of Experiment (DOE) approach, the process parameters of FDM, such as raster orientation, air gap, bead width, color, and model temperature were examined. Tensile strengths and compressive strengths of directionally fabricated specimens were measured and compared with injection molded FDM ABS P400 material, For the FDM parts made with a 0.003 inch overlap between roads, the typical tensile strength ranged between 65 and 72 percent of the strength of injection molded ABS P400. The compressive strength ranged from 80 to 90 percent of the injection molded FDM ABS. Several build rules for designing FDM parts were formulated based on experimental results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23,"Rapid advances in industrialisation and informatisation methods have spurred tremendous progress in developing the next generation of manufacturing technology. Today, we are on the cusp of the Fourth Industrial Revolution. In 2013, amongst one of 10 Future Projects' identified by the German government as part of its High-Tech Strategy 2020 Action Plan, the Industry 4.0 project is considered to be a major endeavour for Germany to establish itself as a leader of integrated industry. In 2014, China's State Council unveiled their ten-year national plan, Made-in-China 2025, which was designed to transform China from the world's workshop into a world manufacturing power. Made-in-China 2025 is an initiative to comprehensively upgrade China's industry including the manufacturing sector. In Industry 4.0 and Made-in-China 2025, many applications require a combination of recently emerging new technologies, which is giving rise to the emergence of Industry 4.0. Such technologies originate from different disciplines including cyber-physical Systems, IoT, cloud computing, Industrial Integration, Enterprise Architecture, SOA, Business Process Management, Industrial Information Integration and others. At this present moment, the lack of powerful tools still poses a major obstacle for exploiting the full potential of Industry 4.0. In particular, formal methods and systems methods are crucial for realising Industry 4.0, which poses unique challenges. In this paper, we briefly survey the state of the art in the area of Industry 4.0 as it relates to industries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24,"Over the last few decades, researchers have developed a number of empirical and theoretical models for the correlation and prediction of the thermophysical properties of pure fluids and mixtures treated as pseudo-pure fluids. In this paper, a survey of all the state-of-the-art formulations of thermophysical properties is presented. The most-accurate thermodynamic properties are obtained from multiparameter Helmholtz-energy-explicit-type formulations. For the transport properties, a wider range of methods has been employed, including the extended corresponding states method. All of the thermophysical property correlations described here have been implemented into CoolProp, an open-source thermophysical property library. This library is written in C++, with wrappers available for the majority of programming languages and platforms of technical interest. As of publication, 110 pure and pseudo-pure fluids are included in the library, as well as properties of 40 incompressible fluids and humid air. The source code for the CoolProp library is included as an electronic annex.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25,"Sustainability, industrial ecology, eco-efficiency, and green chemistry are guiding the development of the next generation of materials, products, and processes. Biodegradable plastics and bio-based polymer products based on annually renewable agricultural and biomass feedstock can form the basis for a portfolio of sustainable, eco-efficient products that can compete and capture markets currently dominated by products based exclusively on petroleum feedstock. Natural/Biofiber composites (Bio-Composites) are emerging as a viable alternative to glass fiber reinforced composites especially in automotive and building product applications. The combination of biofibers such as kenaf, hemp, flax, jute, henequen, pineapple leaf fiber, and sisal with polymer matrices from both nonrenewable and renewable resources to produce composite materials that are competitive with synthetic composites requires special attention, i.e., biofiber-matrix interface and novel processing. Natural fiber-reinforced polypropylene composites have attained commercial attraction in automotive industries. Natural fiber-polypropylene or natural fiber-polyester composites are not sufficiently eco-friendly because of the petroleum-based source and the nonbiodegradable nature of the polymer matrix. Using natural fibers with polymers based on renewable resources will allow many environmental issues to be solved. By embedding biofibers with renewable resource-based biopolymers such as cellulosic plastics; polylactides; starch plastics; polyhydroxyalkanoates (bacterial polyesters); and soy-based plastics, the so-called green bio-composites are continuously being developed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26,"Some ceramics, such as Bioglass((R)), sintered hydroxyapatite, and glass-ceramic A-W, spontaneously bond to living bone. They are called bioactive materials and are already clinically used as important bone substitutes. However, compared with human cortical bone, they have lower fracture toughness and higher elastic moduli. Therefore, it is desirable to develop bioactive materials with improved mechanical properties. All the bioactive materials mentioned above form a bone-like apatite layer on their surfaces in the living body, and bond to bone through this apatite layer. The formation of bone-like apatite on artificial material is induced by functional groups, such as Si-OH, Ti-OH, Zr-OH, Nb-OH, Ta-OH, -COOH, and PO4H2. These groups have specific structures revealing negatively charge, and induce apatite formation via formations of an amorphous calcium compound, e.g., calcium silicate, calcium titanate, and amorphous calcium phosphate. These fundamental findings provide methods for preparing new bioactive materials with different mechanical properties. Tough bioactive materials can be prepared by the chemical treatment of metals and ceramics that have high fracture toughness, e.g., by the NaOH and heat treatments of titanium metal, titanium alloys, and tantalum metal, and by H3PO4 treatment of tetragonal zirconia. Soft bioactive materials can be synthesized by the sol-gel process, in which the bioactive silica or titania is polymerized with a flexible polymer, such as polydimethylsiloxane or polytetramethyloxide, at the molecular level to form an inorganic-organic nano-hybrid. The biomimetic process has been used to deposit nano-sized bone-like apatite on fine polymer fibers, which were textured into a three-dimensional knit framework. This strategy is expected to ultimately lead to bioactive composites that have a bone-like structure and, hence, bone-like mechanical properties. (C) 2003 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27,"Globalisation of supply chains makes their management and control more difficult. Blockchain technology, as a distributed digital ledger technology which ensures transparency, traceability, and security, is showing promise for easing some global supply chain management problems. In this paper, blockchain technology and smart contracts are critically examined with potential application to supply chain management. Local and global government, community, and consumer pressures to meet sustainability goals prompt us to further investigate how blockchain can address and aid supply chain sustainability. Part of this critical examination is how blockchains, a potentially disruptive technology that is early in its evolution, can overcome many potential barriers. Four blockchain technology adoption barriers categories are introduced; inter-organisational, intra-organisational, technical, and external barriers. True blockchain-led transformation of business and supply chain is still in progress and in its early stages; we propose future research propositions and directions that can provide insights into overcoming barriers and adoption of blockchain technology for supply chain management.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,"A Gaussian broadcast channel (GBC) with r single-antenna receivers and t antennas at the transmitter is considered. Both transmitter and receivers have perfect knowledge of the channel. Despite its apparent simplicity, this model is, in general, a nondegraded broadcast channel (BC), for-which the capacity region is not fully known. For the two-user case, we find a special case of Marton's region that achieves optimal sum-rate (throughput). In brief, the transmitter decomposes the channel into two interference channels, where interference is caused by the other user signal. Users are successively encoded, such that encoding of the second user is based on the noncausal knowledge of the interference caused by the first user. The crosstalk parameters are optimized such that the overall throughput is maximum and, surprisingly, this is shown to be optimal over all possible strategies (not only with respect to Marton's achievable region). For the case of r > 2 users, we find a somewhat simpler choice of Marton's region based on ordering and successively encoding the users. For each user i in the given ordering, the interference caused by users j > i is eliminated by zero forcing at the transmitter, while interference caused by users j < i is taken into account by coding for noncausally known interference. Under certain mild conditions, this scheme is found to be throughput-wise. asympmtotically optimal for both high and low signal-to-noise ratio (SNR). We conclude by providing some numerical results for the ergodic throughput of the simplified zero-forcing scheme in independent Rayleigh fading.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29,"The modular multilevel converter (MMC) has been a subject of increasing importance for medium/high-power energy conversion systems. Over the past few years, significant research has been done to address the technical challenges associated with the operation and control of the MMC. In this paper, a general overview of the basics of operation of the MMC along with its control challenges are discussed, and a review of state-of-the-art control strategies and trends is presented. Finally, the applications of the MMC and their challenges are highlighted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30,"In this paper, we address the problem of tracking an object in a video given its location in the first frame and no other information. Recently, a class of tracking techniques called ""tracking by detection"" has been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrade the classifier and can cause drift. In this paper, we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems and can therefore lead to a more robust tracker with fewer parameter tweaks. We propose a novel online MIL algorithm for object tracking that achieves superior results with real-time performance. We present thorough experimental results (both qualitative and quantitative) on a number of challenging video clips.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31,"The goal of this work is to accurately detect and localize boundaries in natural scenes using local image measurements. We formulate features that respond to characteristic changes in brightness, color, and texture associated with natural boundaries. In order to combine the information from these features in an optimal way, we train a classifier using human labeled images as ground truth. The output of this classifier provides the posterior probability of a boundary at each image location and orientation. We present precision-recall curves showing that the resulting detector significantly outperforms existing approaches. Our two main results are 1) that cue combination can be performed adequately with a simple linear model and 2) that a proper, explicit treatment of texture is required to detect boundaries in natural images.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32,"We describe a method for removing noise from digital images, based on a statistical model of the coefficients of an over-complete multiscale oriented basis. Neighborhoods of coefficients at adjacent positions and scales are modeled as the product of two independent random variables: a Gaussian vector and a hidden positive scalar multiplier. The latter modulates the local variance of the coefficients in the neighborhood, and is thus able to account for the empirically observed correlation between the coefficient amplitudes. Under this model, the Bayesian least squares estimate of each coefficient reduces to a weighted average of the local linear estimates over all possible values of the hidden multiplier variable. We demonstrate through simulations with images contaminated by additive white Gaussian noise that the performance of this method substantially surpasses that of previously published methods, both visually and in terms of mean squared error.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33,"Surface roughness evaluation is very important for many fundamental problems such as friction, contact deformation, heat and electric current conduction, tightness of contact joints and positional accuracy. For this reason surface roughness has been the subject of experimental and theoretical investigations for many decades. The real surface geometry is so complicated that a finite number of parameters cannot provide a full description. If the number of parameters used is increased, a more accurate description can be obtained. This is one of the reasons for introducing new parameters for surface evaluation. Surface roughness parameters are normally categorised into three groups according to its functionality. These groups are defined as amplitude parameters, spacing parameters, and hybrid parameters. This paper illustrates the definitions and the mathematical formulae for about 59 of the roughness parameters. This collection of surface roughness parameter was used in a new software computer vision package called Surf Vision developed by the authors. In the package, these definitions were extended to calculate the 3D surface topography of different specimens. (C) 2002 Elsevier Science B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34,"Simultaneous tracking of multiple persons in real-world environments is an active research field and several approaches have been proposed, based on a variety of features and algorithms. Recently, there has been a growing interest in organizing systematic evaluations to compare the various techniques. Unfortunately, the lack of common metrics for measuring the performance of multiple object trackers still makes it hard to compare their results. In this work, we introduce two intuitive and general metrics to allow for objective comparison of tracker characteristics, focusing on their precision in estimating object locations, their accuracy in recognizing object configurations and their ability to consistently label objects over time. These metrics have been extensively used in two large-scale international evaluations, the 2006 and 2007 CLEAR evaluations, to measure and compare the performance of multiple object trackers for a wide variety of tracking tasks. Selected performance results are presented and the advantages and drawbacks of the presented metrics are discussed based on the experience gained during the evaluations. Copyright (C) 2008 K. Bernardin and R. Stiefelhagen.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35,"The simultaneous localization and map building (SLAM) problem asks if it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and then to incrementally build a map of this environment while simultaneously using this map to compute absolute vehicle location. Starting from the estimation-theoretic foundations of this problem developed in [1]-[3], this paper proves that a solution to the SLAM problem is indeed possible. The underlying structure of the SLAM problem is first elucidated. A proof that the estimated map converges monotonically to a relative map with zero uncertainty is then developed. It is then shown that the absolute accuracy of the map and the vehicle location reach a lower bound defined only by the initial vehicle uncertainty. Together, these results show that it is possible for an autonomous vehicle to start in an unknown location in an unknown environment and, using relative observations only, incrementally build a perfect map of the world and to compute simultaneously a bounded estimate of vehicle location. This paper also describes a substantial implementation of the SLAM algorithm on a vehicle operating in an outdoor environment using millimeter-wave (MMW) radar to provide relative map observations. This implementation is used to demonstrate how some key issues such as map management and data association can be handled in a practical environment. The results obtained are cross-compared with absolute locations of the map landmarks obtained by surveying. In conclusion, this paper discusses a number of key issues raised by the solution to the SLAM problem including suboptimal map-building algorithms and map management.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36,"In the literature there are conflicting reports on the optimal scaffold mean pore size required for successful bone tissue engineering. This study set out to investigate the effect of mean pore size, in a series of collagen-glycosaminoglycan (CG) scaffolds with mean pore sizes ranging from 85 mu m to 325 mu m, on osteoblast adhesion and early stage proliferation up to 7 days post-seeding. The results show that cell number was highest in scaffolds with the largest pore size of 325 mu m. However, an early additional peak in cell number was also seen in scaffolds with a mean pore size of 120 mu m at time points up to 48 h post-seeding. This is consistent with previous studies from our laboratory which suggest that scaffold specific surface area plays an important role on initial cell adhesion. This early peak disappears following cell proliferation indicating that while specific surface area may be important for initial cell adhesion, improved cell migration provided by scaffolds with pores above 300 mu m overcomes this effect. An added advantage of the larger pores is a reduction in cell aggregations that develop along the edges of the scaffolds. Ultimately scaffolds with a mean pore size of 325 mu m were deemed optimal for bone tissue engineering. (C) 2009 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37,"This paper describes and evaluates the feasibility of control strategies to be adopted for the operation of a microgrid when it becomes isolated. Normally, the microgrid operates in interconnected mode with the medium voltage network; however, scheduled or forced isolation can take place. In such conditions, the microgrid must have the ability to operate stably and autonomously. An evaluation of the need of storage devices and load shedding strategies is included in this paper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38,"MIMO (multiple-input multiple-output) radar refers to an architecture that employs multiple, spatially distributed transmitters and receivers. While, in a general sense, MIMO radar can be viewed as a type of multistatic radar, the separate nomenclature suggests unique features that set MIMO radar apart from the multistatic radar literature and that have a close relation to MIMO communications. This article reviews some recent work on MIMO radar with widely separated antennas. Widely separated transmit/receive antennas capture the spatial diversity of the target's radar cross section (RCS). Unique features of MIMO radar are explained and illustrated by examples. It is shown that with noncoherent processing, a target's RCS spatial variations can be exploited to obtain a diversity gain for target detection and for estimation of various parameters, such as angle of arrival and Doppler. For target location, it is shown that coherent processing can provide a resolution far exceeding that supported by the radar's waveform.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39,"The pace of the development of silicon photonics has quickened since 2004 due to investment by industry and government. Commercial state-of-the-art CMOS silicon-on-insulator (SOI) foundries are now being utilized in a crucial test of 1.55-mu m monolithic optoelectronic (OE) integration, a test sponsored by the Defense Advanced Research Projects Agency (DARPA). The preliminary results indicate that the silicon photonics are truly CMOS compatible. R&D groups have now developed 10-100-Gb/s electro-optic modulators, ultrafast Ge-on-Si photodetectors, efficient fiber-to-waveguide couplers, and Si Raman lasers. Electrically pumped silicon lasers are under intense investigation, with several approaches being tried; however, lasing has not yet been attained. The new paradigm for the Si-based photonic and optoelectric integrated circuits is that these chip-scale networks, when suitably designed, will operate at a wavelength anywhere within the broad spectral range of 1.2-100 mu m, with cryocooling needed in some cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40,"Time series clustering has been shown effective in providing useful information in various domains. There seems to be an increased interest in time series clustering as part of the effort in temporal data mining research. To provide an overview, this paper surveys and summarizes previous works that investigated the clustering of time series data in various application domains. The basics of time series clustering are presented, including general-purpose clustering algorithms commonly used in time series clustering studies, the criteria for evaluating the performance of the clustering results, and the measures to determine the similarity/dissimilarity between two time series being compared, either in the forms of raw data, extracted features, or some model parameters. The past researchs are organized into three groups depending upon whether they work directly with the raw data either in the time or frequency domain, indirectly with features extracted from the raw data, or indirectly with models built from the raw data. The uniqueness and limitation of previous research are discussed and several possible topics for future research are identified. Moreover, the areas that time series clustering have been applied to are also summarized, including the sources of data used. It is hoped that this review will serve as the steppingstone for those interested in advancing this area of research. (c) 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41,"Direct Sparse Odometry (DSO) is a visual odometry method based on a novel, highly accurate sparse and direct structure and motion formulation. It combines a fully direct probabilistic model (minimizing a photometric error) with consistent, joint optimization of all model parameters, including geometry-represented as inverse depth in a reference frame-and camera motion. This is achieved in real time by omitting the smoothness prior used in other direct methods and instead sampling pixels evenly throughout the images. Since our method does not depend on keypoint detectors or descriptors, it can naturally sample pixels from across all image regions that have intensity gradient, including edges or smooth intensity variations on essentially featureless walls. The proposed model integrates a full photometric calibration, accounting for exposure time, lens vignetting, and non linear response functions. We thoroughly evaluate our method on three different datasets comprising several hours of video. The experiments show that the presented approach significantly outperforms state-of-the-art direct and indirect methods in a variety of real-world settings, both in terms of tracking accuracy and robustness.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,"This review presents a comprehensive description of the current pathways for recycling of polymers, via both mechanical and chemical recycling. The principles of these recycling pathways are framed against current-day industrial reality, by discussing predominant industrial technologies, design strategies and recycling examples of specific waste streams. Starting with an overview on types of solid plastic waste (SPW) and their origins, the manuscript continues with a discussion on the different valorisation options for SPW. The section on mechanical recycling contains an overview of current sorting technologies, specific challenges for mechanical recycling such as thermo-mechanical or lifetime degradation and the immiscibility of polymer blends. It also includes some industrial examples such as polyethylene terephthalate (PET) recycling, and SPW from post-consumer packaging, end-of-life vehicles or electr(on)ic devices. A separate section is dedicated to the relationship between design and recycling, emphasizing the role of concepts such as Design from Recycling. The section on chemical recycling collects a state-of-the-art on techniques such as chemolysis, pyrolysis, fluid catalytic cracking, hydrogen techniques and gasification. Additionally, this review discusses the main challenges (and some potential remedies) to these recycling strategies and ground them in the relevant polymer science, thus providing an academic angle as well as an applied one. (C) 2017 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43,"The particle swarm optimization (PSO), new to the electromagnetics community, is a robust stochastic evolutionary computation technique based on the movement and intelligence of swarms. This paper introduces a conceptual overview and detailed explanation of the PSO algorithm, as well as how it can be used for electromagnetic optimizations. This paper also presents several results illustrating the swarm behavior in a PSO algorithm developed by the authors at UCLA specifically for engineering optimizations (UCLA-PSO). Also discussed is recent progress in the development of the PSO and the special considerations needed for engineering implementation including suggestions for the selection of parameter values. Additionally, a study of boundary conditions is presented indicating the invisible wall technique outperforms absorbing and reflecting wall techniques. These concepts are then integrated into a representative example of optimization of a profiled corrugated horn antenna.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44,"Due to the fast inference and good performance, discriminative learning methods have been widely studied in image denoising. However, these methods mostly learn a specific model for each noise level, and require multiple models for denoising images with different noise levels. They also lack flexibility to deal with spatially variant noise, limiting their applications in practical denoising. To address these issues, we present a fast and flexible denoising convolutional neural network, namely FFDNet, with a tunable noise level map as the input. The proposed FFDNet works on downsampled sub-images, achieving a good trade-off between inference speed and denoising performance. In contrast to the existing discriminative denoisers, FFDNet enjoys several desirable properties, including: 1) the ability to handle a wide range of noise levels (i.e., [0, 75]) effectively with a single network; 2) the ability to remove spatially variant noise by specifying a non-uniform noise level map; and 3) faster speed than benchmark BM3D even on CPU without sacrificing denoising performance. Extensive experiments on synthetic and real noisy images are conducted to evaluate FFDNet in comparison with state-of-the-art denoisers. The results show that FFDNet is effective and efficient, making it highly attractive for practical denoising applications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45,"We describe approximate digital implementations of two new mathematical transforms, namely. the ridgelet transform [2] and the curvelet transform [6], [5]. Our implementations offer exact reconstruction, stability against perturbations, ease of implementation, and low computational complexity. A central tool is Fourier-domain computation of an approximate digital Radon transform. We introduce a very simple interpolation in Fourier space which takes Cartesian samples and yields samples on a rectopolar grid, which is a pseudo-polar sampling set based on a concentric squares geometry. Despite the crudeness of our interpolation, the visual performance is surprisingly good. Our ridgelet transform applies to the Radon transform a special overcomplete wavelet pyramid whose wavelets have compact support in the frequency domain. Our curvelet transform uses our ridgelet transform as a component step, and implements curvelet subbands using a filter bank of a trous wavelet filters. Our philosophy throughout is that transforms should be overcomplete, rather than critically sampled. We apply these digital transforms to the denoising of some standard images embedded in white noise. In the tests reported here, simple thresholding of the curvelet coefficients is very competitive with ""state of the art"" techniques based on wavelets, including thresholding of decimated or undecimated wavelet transforms and also including tree-based Bayesian posterior mean methods. Moreover, the curvelet reconstructions exhibit higher perceptual quality than wavelet-based reconstructions, offering visually sharper images and, in particular, higher quality recovery of edges and of faint linear and curvilinear features. Existing theory for curvelet and ridgelet transforms suggests that these new approaches can outperform wavelet methods in certain image reconstruction problems. The empirical results reported here are in encouraging agreement.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46,"Laboratory experiments on the activated sludge (AS) process were carried out to investigate the influence of microbial extracellular polymeric substances (EPS), including loosely bound EPS (LB-EPS) and tightly bound EPS (TB-EPS), on biomass flocculation, sludge settlement and dewaterability. The heat EPS extraction method was modified to include a mild step and a harsh step for extracting the LB-EPS and TB-EPS, respectively, from the sludge suspension. Six lab-scale AS reactors were used to grow AS with different carbon sources of glucose and sodium acetate, and different sludge retention times (SRTs) of 5, 10 and 20 days. The variation in the bioreactor condition produced sludge with different abundances of EPS and different flocculation and separation characteristics. The sludge that was fed on glucose had more EPS than the sludge that was fed on acetate. For any of the feeding substrates, the sludge had a nearly consistent TB-EPS value regardless of the SRT, and an LB-EPS content that decreased with the SRT. The acetate-fed sludge performed better than the glucose-fed sludge in terms of bioflocculation, sludge sedimentation and compression, and sludge dewaterability. The sludge flocculation and separation improved considerably as the SRT lengthened. The results demonstrate that the LB-EPS had a negative effect on bioflocculation and sludge-water separation. The parameters for the performance of sludge-water separation were much more closely correlated with the amount of LB-EPS than with the amount of TB-EPS. It is argued that although EPS is essential to sludge floc formation, excessive EPS in the form of LB-EPS could weaken cell attachment and the floc structure, resulting in poor bioflocculation, greater cell erosion and retarded sludge-water separation. (C) 2006 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47,"New force fields for carbon dioxide and nitrogen are introduced that quantitatively reproduce the vapor - liquid equilibria (VLE) of the neat systems and their mixtures with alkanes. In addition to the usual VLE calculations for pure CO2 and N-2, calculations of the binary mixtures with propane were used in the force-field development to achieve a good balance between dispersive and electrostatic (qundrupole- quadrupole) interactions. The transfer ability of the force fields was then assessed from calculations of the VLE for the binary mixtures with n-hexane, the binary mixture of CO2/N-2, and the ternary mixture of CO2/N-2/propane. The VLE calculations were carried out using configurational-bias Monte Carlo simulations in either the grand canonical ensemble with histogram-reweighting or in the Gibbs ensemble.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48,"Titanium dioxide is a common additive in many food, personal care, and other consumer products used by people, which after use can enter the sewage system and, subsequently, enter the environment as treated effluent discharged to surface waters or biosolids applied to agricultural land, incinerated wastes, or landfill solids. This study quantifies the amount of titanium in common food products, derives estimates of human exposure to dietary (nano-) TiO2, and discusses the impact of the nanoscale fraction of TiO2 entering the environment. The foods with the highest content of TiO2 included candies, sweets, and chewing gums. Among personal care products, toothpastes and select sunscreens contained 1% to >10% titanium by weight. While some other cremes contained titanium, despite being colored white, most shampoos, deodorants, and shaving creams contained the lowest levels of titanium (<0.01 mu g/mg). For several high-consumption pharmaceuticals, the titanium content ranged from below the instrument detection limit (0.0001 mu g Ti/mg) to a high of 0.014 mu g Ti/mg. Electron microscopy and stability testing of food-grade TiO2 (E171) suggests that approximately 36% of the particles are less than 100 nm in at least one dimension and that it readily disperses in water as fairly stable colloids. However, filtration of water solubilized consumer products and personal care products indicated that less than 5% of the titanium was able to pass through 0.45 or 0.7 mu m pores. Two white paints contained 110 mu g Ti/mg while three sealants (i.e., prime coat paint) contained less titanium (25 to 40 mu g Ti/mg). This research showed that, while many white-colored products contained titanium, it was not a prerequisite. Although several of these product classes contained low amounts of titanium, their widespread use and disposal down the drain and eventually to wastewater treatment plants (WWTPs) deserves attention. A Monte Carlo human exposure analysis to TiO2 through foods identified children as having the highest exposures because TiO2 content of sweets is higher than other food products and that a typical exposure for a US adult may be on the order of 1 mg Ti per kilogram body weight per day. Thus, because of the millions of tons of titanium-based white pigment used annually, testing should focus on food-grade TiO2 (E171) rather than that adopted in many environmental health and safety tests (i.e., P25), which is used in much lower amounts in products less likely to enter the environment (e.g., catalyst supports, photocatalytic coatings).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49,"An alternative theory of solid mechanics, known as the peridynamic theory, formulates problems in terms of integral equations rather than partial differential equations. This theory assumes that particles in a continuum interact with each other across a finite distance, as in molecular dynamics. Damage is incorporated in the theory at the level of these two-particle interactions, so localization and fracture occur as a natural outgrowth of the equation of motion and constitutive models. A numerical method for solving dynamic problems within the peridynamic theory is described. Accuracy and numerical stability are discussed. Examples illustrate the properties of the method for modeling brittle dynamic crack growth. (c) 2005 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
50,"Natural fibers are emerging as low cost, lightweight and apparently environmentally superior alternatives to glass fibers in composites. We review select comparative life cycle assessment studies of natural fiber and glass fiber composites, and identify key drivers of their relative environmental performance. Natural fiber composites are likely to be environmentally superior to glass fiber composites in most cases for the following reasons: (1) natural fiber production has lower environmental impacts compared to glass fiber production; (2) natural fiber composites have higher fiber content for equivalent performance, reducing more polluting base polymer content; (3) the light-weight natural fiber composites improve fuel efficiency and reduce emissions in the use phase of the component, especially in auto applications; and (4) end of life incineration of natural fibers results in recovered energy and carbon credits. (C) 2003 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
51,"Multiple-antenna wireless communication links promise very high data rates with low error probabilities, especially when the wireless channel response is known at the receiver. In practice, knowledge of the channel is often obtained by sending known training symbols to the receiver. We show how training affects the capacity of a fading channel-too little training and the channel is improperly learned, too much training and there is no time left for data transmission before the channel changes. We compute a lower bound on the capacity of a channel that is learned by training, and maximize the, bound as a function of the received signal-to-noise ratio (SNR), fading coherence time, and number of transmitter antennas. When the training and data powers are allowed to vary, we show that the optimal number of training symbols is equal to the number of transmit antennas-this number is also the smallest training interval length that guarantees meaningful estimates of the channel matrix. When the training and data powers are instead required to be equal, the optimal number of symbols may be larger than the number of antennas. We show that training-based schemes can be optimal at high SNR, but suboptimal at low SNR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52,"Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various data sets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning data sets and methods for scene classification is still lacking. In addition, almost all existing data sets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale data set, termed ""NWPU-RESISC45,"" which is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This data set contains 31 500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 1) is large-scale on the scene classes and the total image number; 2) holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion; and 3) has high within-class diversity and between-class similarity. The creation of this data set will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed data set, and the results are reported as a useful baseline for future research.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53,"Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4% on average; outperforming some of the previous reported results by up to 9%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters' influence on performance to provide insights about their optimisation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54,"Various available beam theories, including the Euler-Bernoulli, Timoshenko, Reddy, and Levinson beam theories, are reformulated using the nonlocal differential constitutive relations of Eringen. The equations of motion of the nonlocal theories are derived, and variational statements in terms of the generalized displacements are presented. Analytical solutions of bending, vibration and buckling are presented using the nonlocal theories to bring out the effect of the nonlocal behavior on deflections, buckling loads, and natural frequencies. The theoretical development as well as numerical solutions presented herein should serve as references for nonlocal theories of beams, plates, and shells. (C) 2007 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
55,"Since 2006, deep learning (DL) has become a rapidly growing research direction, redefining state-of-the-art performances in a wide range of areas such as object recognition, image segmentation, speech recognition and machine translation. In modern manufacturing systems, data-driven machine health monitoring is gaining in popularity due to the widespread deployment of low-cost sensors and their connection to the Internet. Meanwhile, deep learning provides useful tools for processing and analyzing these big machinery data. The main purpose of this paper is to review and summarize the emerging research work of deep learning on machine health monitoring. After the brief introduction of deep learning techniques, the applications of deep learning in machine health monitoring systems are reviewed mainly from the following aspects: Auto-encoder (AE) and its variants, Restricted Boltzmann Machines and its variants including Deep Belief Network (DBN) and Deep Boltzmann Machines (DBM), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). In addition, an experimental study on the performances of these approaches has been conducted, in which the data and code have been online. Finally, some new trends of DL-based machine health monitoring methods are discussed. (C) 2018 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56,"This study presents PKSolver, a freely available menu-driven add-in program for Microsoft Excel written in Visual Basic for Applications (VBA), for solving basic problems in pharmacokinetic (PK) and pharmacodynamic (PD) data analysis. The program provides a range of modules for PK and PD analysis including noncompartmental analysis (NCA), compartmental analysis (CA), and pharmacodynamic modeling. Two special built-in modules, multiple absorption sites (MAS) and enterohepatic circulation (EHC), were developed for fitting the double-peak concentration-time profile based on the classical one-compartment model. In addition, twenty frequently used pharmacokinetic functions were encoded as a macro and can be directly accessed in an Excel spreadsheet. To evaluate the program, a detailed comparison of modeling PK data using PKSolver and professional PK/PD software package WinNonlin and Scientist was performed. The results showed that the parameters estimated with PKSolver were satisfactory. In conclusion, the PKSolver simplified the PK and PD data analysis process and its output could be generated in Microsoft Word in the form of an integrated report. The program provides pharmacokinetic researchers with a fast and easy-to-use tool for routine and basic PK and PD data analysis with a more user-friendly interface. (C) 2010 Elsevier Ireland Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
57,"Microbial fuel cells (MFCs) are typically designed as a two-chamber system with the bacteria in the anode chamber separated from the cathode chamber by a polymeric proton exchange membrane (PEM). Most MFCs use aqueous cathodes where water is bubbled with air to provide dissolved oxygen to electrode. To increase energy output and reduce the cost of MFCs, we examined power generation in an air-cathode MFC containing carbon electrodes in the presence and absence of a polymeric proton exchange membrane (PEM). Bacteria present in domestic wastewater were used as the biocatalyst, and glucose and wastewater were tested as substrates. Power density was found to be much greater than typically reported for aqueous-cathode MFCs, reaching a maximum of 262 +/- 10 mW/m(2) (6.6 +/- 0.3 mW/L; liquid volume) using glucose. Removing the PEM increased the maximum power density to 494 +/- 21 mW/m(2) (12.5 +/- 0.5 mW/L). Coulombic efficiency was 40-55% with the PEM and 9-12% with the PEM removed, indicating substantial oxygen diffusion into the anode chamber in the absence of the PEM. Power output increased with glucose concentration according to saturation-type kinetics, with a half saturation constant of 79 mg/L with the PEM-MFC and 103 mg/L in the MFC without a PEM (1000 Omega resistor). Similar results on the effect of the PEM on power density were found using wastewater, where 28 +/- 3 mW/m(2) (0.7 +/- 0.1 mW/L) (28% Coulombic efficiency) was produced with the PEM, and 146 +/- 8 mW/m(2) (3.7 +/- 0.2 mW/L) (20% Coulombic efficiency) was produced when the PEM was removed. The increase in power output when a PEM was removed was attributed to a higher cathode potential as shown by an increase in the open circuit potential. An analysis based on available anode surface area and maximum bacterial growth rates suggests that mediatorless MFCs may have an upper order-of-magnitude limit in power density of 10(3) mW/m(2). A cost-effective approach to achieving power densities in this range will likely require systems that do not contain a polymeric PEM in the MFC and systems based on direct oxygen transfer to a carbon cathode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58,"We present a distributed random linear network coding approach for transmission and compression of information in general multisource multicast networks. Network nodes independently and randomly select linear mappings from inputs onto output links over some field. We show that this achieves capacity with probability exponentially approaching I with the code length. We also demonstrate that random linear coding performs compression when necessary in a network, generalizing error exponents for linear Slepian-Wolf coding in a natural way. Benefits of this approach are decentralized operation and robustness to network changes or link failures. We show that this approach can take advantage of redundant network capacity for improved success probability and robustness. We illustrate some potential advantages of random linear network coding over routing in two examples of practical scenarios: distributed network operation and networks with dynamically varying connections. Our derivation of these results also yields a new bound on required field size for centralized network coding on general multicast networks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59,"It may be endemic to mechanistic modelling of complex environmental systems that there are many different model structures and many different parameter sets within a chosen model structure that may be behavioural or acceptable in reproducing the observed behaviour of that system. This has been called the equifinality concept. The generalised likelihood uncertainty estimation (GLUE) methodology for model identification allowing for equifinality is described. Prediction within this methodology is a process of ensemble forecasting using a sample of parameter sets from the behavioural model space, with each sample weighted according to its likelihood measure to estimate prediction quantiles. This allows that different models may contribute to the ensemble prediction interval at different time steps and that the distributional form of the predictions may change over time. Any effects of model nonlinearity, covariation of parameter values and errors in model structure, input data or observed variables, with which the simulations are compared, are handled implicitly within this procedure. GLUE involves a number of choices that must be made explicit and can be therefore subjected to scrutiny and discussion. These include ways of combining information from different types of model evaluation or from different periods in a data assimilation context. An example application to rainfall-runoff modelling is used to illustrate the methodology, including the updating of likelihood measures. (C) 2001 Elsevier Science B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
60,"This paper presents a formal probabilistic framework for seismic design and assessment of structures and its application to steel moment-resisting frame buildings. This is the probabilistic basis for the 2000 SAC Federal Emergency Management Agency (FEMA) steel moment frame guidelines. The framework is based on realizing a performance objective expressed as the probability of exceeding a specified performance level. Performance levels are quantified as expressions relating generic structural variables ""demand"" and ""capacity"" that are described by nonlinear, dynamic displacements of the structure. Common probabilistic analysis tools are used to convolve both the randomness and uncertainty characteristics of ground motion intensity, structural ""demand,"" and structural system ""capacity"" in order to derive an expression for the probability of achieving the specified performance level. Stemming from this probabilistic framework, a safety-checking for-mat of the conventional ""load and resistance factor"" kind is developed with load and resistance terms being replaced by the more generic terms ""demand"" and ""capacity,"" respectively. This framework also allows for a peformance objective being met. This format has been format based on quantitative confidence statements regarding the likelihood of the performance objective being met. This format has been adopted in the SAC/FEMA guidelines.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
61,"Cascaded multilevel inverters synthesize a medium-voltage output based on a series connection of power cells which use standard low-voltage component configurations. This characteristic allows one to achieve high-quality output voltages and input currents and also outstanding availability due to their intrinsic component redundancy. Due to these features, the cascaded multilevel inverter has been recognized as an important alternative in the medium-voltage inverter market. This paper presents a survey of different topologies, control strategies and modulation techniques used by these inverters. Regenerative and advanced topologies are also discussed. Applications where the mentioned features play a key role are shown. Finally, future developments are addressed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62,"The stress triaxiality is, besides the strain intensity, the most important factor that controls initiation of ductile fracture. In this study, a series of tests including upsetting tests, shear tests and tensile tests on 2024-T351 aluminum alloy providing clues to fracture ductility for a wide range of stress triaxiality was carried out. Numerical simulations of each test was performed using commercial finite element code ABAQUS. Good correlation of experiments and numerical simulations was achieved. Based on the experimental and numerical results, the relation between the equivalent strain to fracture versus the stress triaxiality was quantified and it was shown that there are three distinct branches of this function with possible slope discontinuities in the transition regime. For negative stress triaxialities, fracture is governed by shear mode. For large triaxialities void growth is the dominant failure mode, while at low stress triaxialities between above two regimes, fracture may develop as a combination of shear and void growth modes. (C) 2004 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63,"China faces great challenges in protecting its soil from contamination caused by rapid industrialization and urbanization over the last three decades. Recent nationwide surveys show that 16% of the soil samples, 19% for the agricultural soils, are contaminated based on Chinas soil environmental quality limits, mainly with heavy metals and metalloids. Comparisons with other regions of the world show that the current status of soil contamination, based on the total contaminant concentrations, is not worse in China. However, the concentrations of some heavy metals in Chinese soils appear to be increasing at much greater rates. Exceedance of the contaminant limits in food crops is widespread in some areas, especially southern China, due to elevated inputs of contaminants, acidic nature of the soil and crop species or cultivars prone to heavy metal accumulation. Minimizing the transfer of contaminants from soil to the food chain is a top priority. A number of options are proposed, including identification of the sources of contaminants to agricultural systems, minimization of contaminant inputs, reduction of heavy metal phytoavailability in soil with liming or other immobilizing materials, selection and breeding of low accumulating crop cultivars, adoption of appropriate water and fertilizer management, bioremediation, and change of land use to grow nonfood crops. Implementation of these strategies requires not only technological advances, but also social-economic evaluation and effective enforcement of environmental protection law.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64,"Super-resolution reconstruction produces one or a set of high-resolution images from a set of low-resolution images. In the last two decades, a variety of super-resolution methods have been proposed. These methods are usually very sensitive to their assumed model of data and noise, which limits their utility. This paper reviews some of these methods and addresses their shortcomings. We propose an alternate approach using L-1 norm minimization and robust regularization based on a bilateral prior to deal with different data and noise models. This computationally inexpensive method is robust to errors in motion and blur estimation and results in images with sharp edges. Simulation results confirm the effectiveness of our method and demonstrate its superiority to other super-resolution methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65,"In this paper, we develop an online sequential learning algorithm for single hidden layer feedforward networks (SLFNs) with. additive or radial basis function (RBF) hidden nodes in a unified framework. The algorithm is referred to as online sequential extreme learning machine (OS-ELM) and can learn data one-by-one or chunk-by-chunk (a block of data) with fixed or varying chunk size. The activation functions for additive nodes in OS-ELM can be any bounded nonconstant piecewise continuous functions and the activation functions for RBF nodes can be any integrable piecewise continuous functions. In OS-ELM, the parameters of hidden nodes (the input weights and biases of additive nodes or the centers and impact factors of RBF nodes) are randomly selected and the output weights are analytically determined based on the sequentially arriving data. The algorithm uses the ideas of ELM of Huang et al. developed for batch learning which has been shown to be extremely fast with generalization performance better than other batch training methods. Apart from selecting the number of hidden nodes, no other control parameters have to be manually chosen. Detailed performance comparison of OS-ELM is done with other popular sequential learning algorithms on benchmark problems drawn from the regression, classification and time series prediction areas. The results show that the OS-ELM is faster than the other sequential algorithms and produces better generalization performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
66,"Providing clean and affordable water to meet human needs is a grand challenge of the 21st century. Worldwide, water supply struggles to keep up with the fast growing demand, which is exacerbated by population growth, global climate change, and water quality deterioration. The need for technological innovation to enable integrated water management cannot be overstated. Nanotechnology holds great potential in advancing water and wastewater treatment to improve treatment efficiency as well as to augment water supply through safe use of unconventional water sources. Here we review recent development in nanotechnology for water and wastewater treatment. The discussion covers candidate nanomaterials, properties and mechanisms that enable the applications, advantages and limitations as compared to existing processes, and barriers and research needs for commercialization. By tracing these technological advances to the physicochemical properties of nanomaterials, the present review outlines the opportunities and limitations to further capitalize on these unique properties for sustainable water management. (C) 2013 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
67,"Low power dissipation and maximum battery runtime are crucial in portable electronics. With accurate and efficient circuit and battery models in hand, circuit designers can predict and optimize battery runtime and circuit performance. In this paper, an accurate, intuitive, and comprehensive electrical battery model is proposed and implemented in a Cadence environment. This model accounts for all dynamic characteristics of the battery, from nonlinear open-circuit voltage, current-, temperature-, cycle number-, and storage time-dependent capacity to transient response. A simplified model neglecting the effects of self-discharge, cycle number, and temperature, which are nonconsequential in low-power Li-ion-supplied applications, is validated with experimental data on NiMH and polymer Li-ion batteries. Less than 0.4% runtime error and 30-mV maximum error voltage show that the proposed model predicts both the battery, runtime and I-V performance accurately. The model can also be easily extended to other battery and power sourcing technologies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68,"Internet-delivered e-services are increasingly being made available to consumers; however, little is known about how consumers evaluate them for potential adoption. Past Technology Adoption Research has focused primarily on the positive utility gains attributable to system adoption. This research extends that approach to include measures of negative utility (potential losses) attributable to e-service adoption. Drawing from Perceived Risk Theory, specific risk facets were operationalized, integrated, and empirically tested within the Technology Acceptance Model resulting in a proposed e-services adoption model. Results indicated that e-services adoption is adversely affected primarily by performance-based risk perceptions, and perceived ease of use of the e-service reduced these risk concerns. Implications of integrating perceived risk into the proposed e-services adoption model are discussed. (C) 2003 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69,"Broadband wireless access systems deployed in residential and business environments are likely to face hostile radio propagation environments, with multipath delay spread extending over tens or hundreds of bit intervals. Orthogonal frequency-division multiplex (OFDM) is a recognized multicarrier solution to combat the effects of such multipath conditions. This article surveys frequency domain equalization (FIDE) applied to single-carrier (SC) modulation solutions. SC radio modems with frequency domain equalization have similar performance, efficiency, and low signal processing complexity advantages as OFDM, and in addition are less sensitive than OFDM to RF impairments such as power amplifier nonlinearities. We discuss similarities and differences of SC and OFDM systems and coexistence possibilities, and present examples of SC-FDE performance capabilities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
70,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
71,"Drawing upon a variety of existing maps, data and information, a new Global Lakes and Wetlands Database (GLWD) has been created. The combination of best available sources for lakes and wetlands on a global scale (1: 1 to 1:3 million resolution), and the application of Geographic Information System (GIS) functionality enabled the generation of a database which focuses in three coordinated levels on (1) large lakes and reservoirs, (2) smaller water bodies, and (3) wetlands. Level 1 comprises the shoreline polygons of the 3067 largest lakes (surface area greater than or equal to50 km(2)) and 654 largest reservoirs (storage capacity greater than or equal to0.5 km(3)) worldwide, and offers extensive attribute data. Level 2 contains the shoreline polygons of approx. 250,000 smaller lakes, reservoirs and rivers (surface area; greater than or equal to0.1 km(2)), excluding all water bodies of level 1. Finally, level 3 represents lakes, reservoirs, rivers, and different wetland types in the form of a global raster map at 30-second resolution, including all water bodies of levels 1 and 2. In a validation against documented data, GLWD proved to represent a comprehensive database of global lakes 1 km(2) and to provide a good representation of the maximum global wetland extent. GLWD-1 and GLWD-2 establish two global polygon maps to which existing lake registers, compilations or remote sensing data can be linked in order to allow for further analyses in a GIS environment. GLWD-3 may serve as an estimate of wetland extents for global hydrology and climatology models, or to identify large-scale wetland distributions and important wetland complexes. (C) 2004 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
72,"Digital twin (DT) is one of the most promising enabling technologies for realizing smart manufacturing and Industry 4.0. DTs are characterized by the seamless integration between the cyber and physical spaces. The importance of DTs is increasingly recognized by both academia and industry. It has been almost 15 years since the concept of the DT was initially proposed. To date, many DT applications have been successfully implemented in different industries, including product design, production, prognostics and health management, and some other fields. However, at present, no paper has focused on the review of DT applications in industry. In an effort to understand the development and application of DTs in industry, this paper thoroughly reviews the state-of-the-art of the DT research concerning the key components of DTs, the current development of DTs, and the major DT applications in industry. This paper also outlines the current challenges and some possible directions for future work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
73,"Wireless vehicular communication has the potential to enable a host of new applications, the most important of which are a class of safety applications that can prevent collisions and save thousands of lives. The automotive industry is working to develop the dedicated short-range communication (DSRC) technology, for use in vehicle-to-vehicle and vehicle-to-roadside communication. The effectiveness of this technology is highly dependent on cooperative standards for interoperability. This paper explains the content and status of the DSRC standards being developed for deployment in the United States. Included in the discussion are the IEEE 802.11p amendment for wireless access in vehicular environments (WAVE), the IEEE 1609.2, 1609.3, and 1609.4 standards for Security, Network Services and Multi-Channel Operation, the SAE J2735 Message Set Dictionary, and the emerging SAE J2945.1 Communication Minimum Performance Requirements standard. The paper shows how these standards fit together to provide a comprehensive solution for DSRC. Most of the key standards are either recently published or expected to be completed in the coming year. A reader will gain a thorough understanding of DSRC technology for vehicular communication, including insights into why specific technical solutions are being adopted, and key challenges remaining for successful DSRC deployment. The U.S. Department of Transportation is planning to decide in 2013 whether to require DSRC equipment in new vehicles.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74,"Plastics debris is accumulating in the environment and is fragmenting into smaller pieces; as it does, the potential for ingestion by animals increases. The consequences of macroplastic debris for wildlife are well documented, however the impacts of microplastic (<1 mm) are poorly understood. The mussel, Mytilus edulis, was used to investigate ingestion, translocation, and accumulation of this debris. Initial experiments showed that upon ingestion, microplastic accumulated in the gut. Mussels were subsequently exposed to treatments containing seawater and microplastic (3.0 or 9.6 mu m). After transfer to clean microplastic was tracked in the hemolymph. Particles conditions, translocated from the gut to the circulatory system within 3 days and persisted for over 48 days. Abundance of microplastic was greatest after 12 days and declined thereafter. Smaller particles were more abundant than larger particles and our data indicate as plastic fragments into smaller particles, the potential for accumulation in the tissues of an organism increases. The short-term pulse exposure used here did not result in significant biological effects. However, plastics are exceedingly durable and so further work using a wider range of organisms, polymers, and periods of exposure will be required to establish the biological consequences of this debris.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75,"The fourth generation wireless communication systems have been deployed or are soon to be deployed in many countries. However, with an explosion of wireless mobile devices and services, there are still some challenges that cannot be accommodated even by 4G, such as the spectrum crisis and high energy consumption. Wireless system designers have been facing the continuously increasing demand for high data rates and mobility required by new wireless applications and therefore have started research on fifth generation wireless systems that are expected to be deployed beyond 2020. In this article, we propose a potential cellular architecture that separates indoor and outdoor scenarios, and discuss various promising technologies for 5G wireless communication systems, such as massive MIMO, energy-efficient communications, cognitive radio networks, and visible light communications. Future challenges facing these potential technologies are also discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76,"Objective. Brain-computer interfaces (BCI) enable direct communication with a computer, using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the distinct characteristics of its expected EEG control signal, limiting its application to that specific signal. Convolutional neural networks (CNNs), which have been used in computer vision and speech recognition to perform automatic feature extraction and classification, have successfully been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms, while simultaneously being as compact as possible. Approach. In this work we introduce EEGNet, a compact convolutional neural network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet, both for within-subject and cross-subject classification, to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). Main results. We show that EEGNet generalizes across paradigms better than, and achieves comparably high performance to, the reference algorithms when only limited training data is available across all tested paradigms. In addition, we demonstrate three different approaches to visualize the contents of a trained EEGNet model to enable interpretation of the learned features. Significance. Our results suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range of BCI tasks. Our models can be found at: https://github.com/vlawhern/arl-eegmodels.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77,"The Young's modulus (E) of a material is a key parameter for mechanical engineering design. Silicon, the most common single material used in microelectromechanical systems (MEMS), is an anisotropic crystalline material whose material properties depend on orientation relative to the crystal lattice. This fact means that the correct value of E for analyzing two different designs in silicon may differ by up to 45%. However, perhaps, because of the perceived complexity of the subject, many researchers oversimplify silicon elastic behavior and use inaccurate values for design and analysis. This paper presents the best known elasticity data for silicon, both in depth and in a summary form, so that it may be readily accessible to MEMS designers. [2009-0054]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78,"A generalization of the original peridynamic framework for solid mechanics is proposed. This generalization permits the response of a material at a point to depend collectively on the deformation of all bonds connected to the point. This extends the types of material response that can be reproduced by peridynamic theory to include an explicit dependence on such collectively determined quantities as volume change or shear angle. To accomplish this generalization, a mathematical object called a deformation state is defined, a function that maps any bond onto its image under the deformation. A similar object called a force state is defined, which contains the forces within bonds of all lengths and orientation. The relation between the deformation state and force state is the constitutive model for the material. In addition to providing a more general capability for reproducing material response, the new framework provides a means to incorporate a constitutive model from the conventional theory of solid mechanics directly into a peridynamic model. It also allows the condition of plastic incompressibility to be enforced in a peridynamic material model for permanent deformation analogous to conventional plasticity theory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79,"Deformation twinning, martensitic phase transformation and mechanical properties of austenitic Fe-(15-30) wt.%Mn steels with additions of aluminium and silicon have been investigated. It is known that additions of aluminium increase the stacking fault energy gamma(fcc) and therefore strongly suppress the gamma --> epsilon transformation while silicon decrease gamma(fcc) and sustains the gamma --> epsilon transformation. The gamma --> epsilon phase transformation takes place in steels with gamma(fcc) less than or equal to 20 mJ/m(2). For steels with higher stacking fault energy twinning is the main deformation mechanism. Tensile tests were carried out at different strain rates and temperatures. The formation of twins, alpha- and epsilon- martensite during plastic deformation was analysed by optical microscopy, X-ray diffraction, scanning electron microscopy (SEM) and transmission electron microscopy (TEM). The developed light weight high manganese TRIP (""transformation induced plasticity"") and TWIP (""twinning induced plasticity"") steels exhibit high flow stress (600-1100 MPa) and extremely large elongation (60-95%) even at extremely high strain rates of about 10(3) s(-1). Recent trends in the automotive industry towards improved safely standards and a reduced weight as well as a more rational and cost effective manufacturing have led to great interest in these high strength and ""super tough"" steels. (C) 2000 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
80,"Injectable self-healing hydrogel dressing with multifunctional properties including anti-infection, anti oxidative and conductivity promoting wound healing process will be highly desired in wound healing application and its design is still a challenge. We developed a series of injectable conductive self-healed hydrogels based on quaternized chitosan-g-polyaniline (QCSP) and benzaldehyde group functionalized poly(ethylene glycol)-co-poly(glycerol sebacate) (PEGS-FA) as antibacterial, anti-oxidant and electroactive dressing for cutaneous wound healing. These hydrogels presented good self-healing, electroactivity, free radical scavenging capacity, antibacterial activity, adhesiveness, conductivity, swelling ratio, and biocompatibility. Interestingly, the hydrogel with an optimal crosslinker concentration of 1.5 wt% PEGS-FA showed excellent in vivo blood clotting capacity, and it significantly enhanced in vivo wound healing process in a full-thickness skin defect model than quaternized chitosan/PEGS-FA hydrogel and commercial dressing (Tegaderm (TM) film) by upregulating the gene expression of growth factors including VEGF, EGF and TGF-beta and then promoting granulation tissue thickness and collagen deposition. Taken together, the antibacterial electroactive injectable hydrogel dressing prolonged the lifespan of dressing relying on self-healing ability and significantly promoted the in vivo wound healing process attributed to its multifunctional properties, meaning that they are excellent candidates for full-thickness skin wound healing. (C) 2017 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,"Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 x 3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,"Efficacy of aligned poly(L-lactic acid) (PLLA) nano/micro fibrous scaffolds for neural tissue engineering is described and their performance with random PLLA scaffolds is compared as well in this study. Perfectly aligned PLLA fibrous scaffolds were fabricated by an electrospinning technique under optimum condition and the diameter of the electrospun fibers can easily be tailored by adjusting the concentration of polymer solution. As the structure of PLLA scaffold was intended for neural tissue engineering, its suitability was evaluated in vitro using neural stem cells (NSCs) as a model cell line. Cell morphology, differentiation and neurite outgrowth were studied by various microscopic techniques. The results show that the direction of NSC elongation and its neurite outgrowth is parallel to the direction of PLLA fibers for aligned scaffolds. No significant changes were observed on the cell orientation with respect to the fiber diameters. However, the rate of NSC differentiation was higher for PLLA nanofibers than that of micro fibers and it was independent of the fiber alignment. Based on the experimental results, the aligned nanofibrous PLLA scaffold could be used as a potential cell carrier in neural tissue engineering. (C) 2004 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,"The concept of a miss-distance, or error, between a reference quantity and its estimated/controlled value, plays a fundamental role in any filtering/control problem. Yet there is no satisfactory notion of a miss-distance in the well-established field of multi-object filtering. In this paper, we outline the inconsistencies of existing metrics in the context of multi-object miss-distances for performance evaluation. We then propose a new mathematically and intuitively consistent metric that addresses the drawbacks of current multi-object performance evaluation metrics.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84,"We introduce a short signature scheme based on the Computational Diffie-Hellman assumption on certain elliptic and hyperelliptic curves. For standard security parameters, the signature length is about half that of a DSA signature with a similar level of security. Our short signature scheme is designed for systems where signatures are typed in by a human or are sent over a low-bandwidth channel. We survey a number of properties of our signature scheme such as signature aggregation and batch verification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,"Graphitic carbon nitrides (g-C3N4) are becoming increasingly significant due to the theoretical prediction of their unusual properties and promising applications ranging from photocatalysis, heterogeneous catalysis, to fuel cells. Recently, a variety of nanostructured and nanoporous g-C3N4 materials have been developed for a wide range of new applications. This feature article gives, at first, an overview on the synthesis of g-C3N4 nanomaterials with controllable structure and morphology, and secondly, presents and categorizes applications of g-C3N4 as multifunctional metal-free catalysts for environmental protection, energy conversion and storage. A special emphasis is placed on the potential applications of nanostructured g-C3N4 in the areas of artificial photocatalysis for hydrogen production, oxygen reduction reaction (ORR) for fuel cells, and metal-free heterogeneous catalysis. Finally, this perspective highlights crucial issues that should be addressed in the future in the aforementioned exciting research areas.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86,"We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only ""virtually"" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VATachieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87,"An emerging solution for prolonging the lifetime of energy constrained relay nodes in wireless networks is to avail the ambient radio-frequency (RF) signal and to simultaneously harvest energy and process information. In this paper, an amplify-and-forward (AF) relaying network is considered, where an energy constrained relay node harvests energy from the received RF signal and uses that harvested energy to forward the source information to the destination. Based on the time switching and power splitting receiver architectures, two relaying protocols, namely, i) time switching-based relaying (TSR) protocol and ii) power splitting-based relaying (PSR) protocol are proposed to enable energy harvesting and information processing at the relay. In order to determine the throughput, analytical expressions for the outage probability and the ergodic capacity are derived for delay-limited and delay-tolerant transmission modes, respectively. The numerical analysis provides practical insights into the effect of various system parameters, such as energy harvesting time, power splitting ratio, source transmission rate, source to relay distance, noise power, and energy harvesting efficiency, on the performance of wireless energy harvesting and information processing using AF relay nodes. In particular, the TSR protocol outperforms the PSR protocol in terms of throughput at relatively low signal-to-noise-ratios and high transmission rates.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
88,"High leakage current in deep-submicrometer regimes is becoming a significant contributor to power dissipation of CMOS circuits as threshold voltage, channel length, and gate oxide thickness are reduced. Consequently, the identification and modeling of different leakage components is very important for estimation and reduction of leakage power especially for low-power applications. This paper reviews various transistor intrinsic leakage mechanisms, including weak inversion, drain-induced barrier lowering, gate-induced drain leakage, and gate oxide tunneling. Channel engineering techniques including retrograde well and halo doping are explained as means to manage short-channel effects for continuous scaling of CMOS devices. Finally, the paper explores different circuit techniques to reduce the leakage power consumption.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
89,"This paper is a review on the tensile properties of natural fiber reinforced polymer composites. Natural fibers have recently become attractive to researchers, engineers and scientists as an alternative reinforcement for fiber reinforced polymer (FRP) composites. Due to their low cost, fairly good mechanical properties, high specific strength, non-abrasive, eco-friendly and bio-degradability characteristics, they are exploited as a replacement for the conventional fiber, such as glass, aramid and carbon. The tensile properties of natural fiber reinforce polymers (both thermoplastics and thermosets) are mainly influenced by the interfacial adhesion between the matrix and the fibers. Several chemical modifications are employed to improve the interfacial matrix-fiber bonding resulting in the enhancement of tensile properties of the composites. In general, the tensile strengths of the natural fiber reinforced polymer composites increase with fiber content, up to a maximum or optimum value, the value will then drop. However, the Young's modulus of the natural fiber reinforced polymer composites increase with increasing fiber loading. Khoathane et al. [1] found that the tensile strength and Young's modulus of composites reinforced with bleached hemp fibers increased incredibly with increasing fiber loading. Mathematical modelling was also mentioned. It was discovered that the rule of mixture (ROM) predicted and experimental tensile strength of different natural fibers reinforced HDPE composites were very close to each other. Halpin-Tsai equation was found to be the most effective equation in predicting the Young's modulus of composites containing different types of natural fibers. (C) 2011 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90,"In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyperparameter selection. The starting point of this paper is the observation that unrolled iterative methods have the form of a CNN (filtering followed by pointwise non-linearity) when the normal operator (H* H, where H* is the adjoint of the forward imaging operator, H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 x 512 image on the GPU.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,"In this correspondence, a completed modeling of the local binary pattern (LBP) operator is proposed and an associated completed LBP (CLBP) scheme is developed for texture classification. A local region is represented by its center pixel and a local difference sign-magnitude transform (LDSMT). The center pixels represent the image gray level and they are converted into a binary code, namely CLBP-Center (CLBP_C), by global thresholding. LDSMT decomposes the image local differences into two complementary components: the signs and the magnitudes, and two operators, namely CLBP-Sign (CLBP_S) and CLBP-Magnitude (CLBP_M), are proposed to code them. The traditional LBP is equivalent to the CLBP_S part of CLBP, and we show that CLBP_S preserves more information of the local structure than CLBP_M, which explains why the simple LBP operator can extract the texture features reasonably well. By combining CLBP_S, CLBP_M, and CLBP_C features into joint or hybrid distributions, significant improvement can be made for rotation invariant texture classification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92,"In the near future, i.e., beyond 4G, some of the prime objectives or demands that need to be addressed are increased capacity, improved data rate, decreased latency, and better quality of service. To meet these demands, drastic improvements need to be made in cellular network architecture. This paper presents the results of a detailed survey on the fifth generation (5G) cellular network architecture and some of the key emerging technologies that are helpful in improving the architecture and meeting the demands of users. In this detailed survey, the prime focus is on the 5G cellular network architecture, massive multiple input multiple output technology, and device-to-device communication (D2D). Along with this, some of the emerging technologies that are addressed in this paper include interference management, spectrum sharing with cognitive radio, ultra-dense networks, multi-radio access technology association, full duplex radios, millimeter wave solutions for 5G cellular networks, and cloud technologies for 5G radio access networks and software defined networks. In this paper, a general probable 5G cellular network architecture is proposed, which shows that D2D, small cell access points, network cloud, and the Internet of Things can be a part of 5G cellular network architecture. A detailed survey is included regarding current research projects being conducted in different countries by research groups and institutions that are working on 5G technologies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,"An iterative algorithm, based on recent work in compressive sensing, is developed for volume image reconstruction from a circular cone-beam scan. The algorithm minimizes the total variation (TV) of the image subject to the constraint that the estimated projection data is within a specified tolerance of the available data and that the values of the volume image are non-negative. The constraints are enforced by the use of projection onto convex sets (POCS) and the TV objective is minimized by steepest descent with an adaptive step-size. The algorithm is referred to as adaptive-steepest-descent-POCS (ASD-POCS). It appears to be robust against cone-beam artifacts, and may be particularly useful when the angular range is limited or when the angular sampling rate is low. The ASD-POCS algorithm is tested with the Defrise disk and jaw computerized phantoms. Some comparisons are performed with the POCS and expectation-maximization (EM) algorithms. Although the algorithm is presented in the context of circular cone-beam image reconstruction, it can also be applied to scanning geometries involving other x-ray source trajectories.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94,"Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations, and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this letter, we propose a deep cascaded multitask framework that exploits the inherent correlation between detection and alignment to boost up their performance. In particular, our framework leverages a cascaded architecture with three stages of carefully designed deep convolutional networks to predict face and landmark location in a coarse-to-fine manner. In addition, we propose a new online hard sample mining strategy that further improves the performance in practice. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging face detection dataset and benchmark and WIDER FACE benchmarks for face detection, and annotated facial landmarks in the wild benchmark for face alignment, while keeps real-time performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
95,"We provide an overview of the extensive recent results on the Shannon capacity of single-user and multiuser multiple-input multiple-output (MIMO) channels. Although enormous capacity gains have been predicted for such channels, these predictions are based on somewhat unrealistic assumptions about the underlying time-varying channel model and how well it can be tracked at the receiver, as well as at the transmitter. More realistic assumptions can dramatically impact the potential capacity gains of MIMO techniques. For time-varying MIMO channels there are multiple Shannon theoretic capacity definitions and, for each definition, different correlation models and channel information assumptions that we consider. We first provide a comprehensive summary of ergodic and capacity versus outage results for single-user MIMO channels. These results indicate that the capacity gain obtained from multiple antennas heavily depends on the available channel information at either the receiver or transmitter, the channel signal-to-noise ratio, and the correlation between the channel gains on each antenna element. We then focus attention on the capacity region of the multiple-access channels (MACs) and the largest known achievable rate region for the broadcast channel. In contrast to single-user MIMO channels, capacity results for these multiuser MIMO channels are quite difficult to obtain, even for constant channels. We. summarize results for the MIMO broadcast and MAC for channels that are either constant or fading with perfect instantaneous knowledge of the antenna gains at,both transmitter(s) and receiver(s). We show that the capacity region of the MIMO multiple access and the largest known achievable rate region (called the dirty-paper region) for the MIMO broadcast channel are intimately related via a duality transformation. This transformation facilitates finding the transmission strategies that achieve a point on the boundary of the MIMO MAC capacity region in terms of the transmission strategies of the MIMO broadcast dirty-paper region and vice-versa. Finally, we discuss capacity results for multicell MIMO channels with base station cooperation. The base stations then act as a spatially diverse antenna array and transmission strategies that exploit this structure exhibit significant capacity gains. This section also provides a brief discussion of system level issues associated with MIMO cellular. Open problems in this field abound and are discussed throughout the paper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96,"This paper presents a fiducial marker system specially appropriated for camera pose estimation in applications such as augmented reality and robot localization. Three main contributions are presented. First, we propose an algorithm for generating configurable marker dictionaries (in size and number of bits) following a criterion to maximize the inter-marker distance and the number of bit transitions. In the process, we derive the maximum theoretical inter-marker distance that dictionaries of square binary markers can have. Second, a method for automatically detecting the markers and correcting possible errors is proposed. Third, a solution to the occlusion problem in augmented reality applications is shown. To that aim, multiple markers are combined with an occlusion mask calculated by color segmentation. The experiments conducted show that our proposal obtains dictionaries with higher inter-marker distances and lower false negative rates than state-of-the-art systems, and provides an effective solution to the occlusion problem. (C) 2014 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97,"This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data. The adjective ""unsupervised"" is justified by two properties of the algorithm: 1) it is capable of selecting the number of components and 2) unlike the standard expectation-maximization (EM) algorithm, it does not require careful initialization. The proposed method also avoids another drawback of EM for mixture fitting: the possibility of convergence toward a singular estimate at the boundary of the parameter space. The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models; instead, we seamlessly integrate estimation and model selection in a single algorithm. Our technique can be applied to any type of parametric mixture model for which it is possible to write an EM algorithm; in this paper, we illustrate it with experiments involving Gaussian mixtures. These experiments testify for the good performance of our approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98,"Capture and sequestration of CO2 from fossil fuel power plants is gaining widespread interest as a potential method of controlling greenhouse gas emissions. Performance and cost models of an amine (MEA)-based CO2 absorption system for postcombustion flue gas applications have been developed and integrated with an existing power plant modeling framework that-includes multipollutant control technologies for other regulated emissions. The integrated model has been applied to study the feasibility and cost of carbon capture and sequestration at both new and existing coal-burning power plants. The cost of carbon avoidance was shown to depend strongly on assumptions about the reference plant design, details of the CO2 capture system design, interactions with other pollution control systems, and method of CO2 storage. The CO2 avoidance cost for retrofit systems was found to be generally higher than for new plants, mainly because of the higher energy penalty resulting from less efficient heat integration;as well as site-specific difficulties typically encountered in retrofit applications. For all cases, a small, reduction in CO2 capture cost was afforded by the SO2 emission trading credits generated by amine-based capture systems. Efforts are underway to model a broader suite of carbon capture and sequestration technologies for more comprehensive assessments in the context of multipollutant environmental management.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
99,"In this letter, the performance of non-orthogonal multiple access (NOMA) is investigated in a cellular downlink scenario with randomly deployed users. The developed analytical results show that NOMA can achieve superior performance in terms of ergodic sumrates; however, the outage performance of NOMA depends critically on the choices of the users' targeted data rates and allocated power. In particular, a wrong choice of the targeted data rates and allocated power can lead to a situation in which the user's outage probability is always one, i.e. the user's targeted quality of service will never be met.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
