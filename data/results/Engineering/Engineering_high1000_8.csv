ID,Abstract,rule1,rule2,rule3,rule4,rule5,rule6,rule7,rule8,rule9,rule10,rule11,rule12,rule13,rule14,rule15,rule16,rule17,rule18,rule19,rule20,rule21,rule22,rule23,rule24,rule25,rule26,rule27,rule28,rule29,rule30,rule31,rule32,rule33
0,"A deformation measurement system based on particle image velocimetry (PIV) and close-range photogrammetry has been developed for use in geotechnical testing. In this paper, the theory underlying this system is described, and the performance is validated. Digital photography is used to capture images of planar soil deformation. Using PIV, the movement of a fine mesh of soil patches is measured to a high precision. Since PIV operates on the image texture, intrusive target markers need not be installed in the observed soil. The resulting displacement vectors are converted from image space to object space using a photogrammetric transformation. A series of validation experiments are reported. These demonstrate that the precision, accuracy and resolution of the system are an order of magnitude higher than previous image-based deformation methods, and are comparable to local instrumentation used in element testing. This performance is achieved concurrent with an order of magnitude increase in the number of measurement points that can be fitted in an image. The performance of the system is illustrated with two example applications.",no,happy,happy,no,no,happy,happy,happy,happy,no,yes,yes,no,yes,yes,yes,yes,yes,yes,yes,yes,no,yes,yes,yes,yes,yes,yes,yes,no,yes,yes,yes
1,"Self-driving vehicles are a maturing technology with the potential to reshape mobility by enhancing the safety, accessibility, efficiency, and convenience of automotive transportation. Safety-critical tasks that must be executed by a self-driving vehicle include planning of motions through a dynamic environment shared with other vehicles and pedestrians, and their robust executions via feedback control. The objective of this paper is to survey the current state of the art on planning and control algorithms with particular regard to the urban setting. A selection of proposed techniques is reviewed along with a discussion of their effectiveness. The surveyed approaches differ in the vehicle mobility model used, in assumptions on the structure of the environment, and in computational requirements. The side by side comparison presented in this survey helps to gain insight into the strengths and limitations of the reviewed approaches and assists with system level design choices.",yes,no,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,yes,no,yes,yes,yes,yes,yes,yes,yes,no,no,yes,no
2,"In 1991 Morris proposed an effective screening sensitivity measure to identify the few important factors in models with many factors. The method is based on computing for each input a number of incremental ratios, namely elementary effects, which are then averaged to assess the overall importance of the input. Despite its value, the method is still rarely used and instead local analyses varying one factor at a time around a baseline point are usually employed. In this piece of work we propose a revised version of the elementary effects method, improved in terms of both the definition of the measure and the sampling strategy. In the present form the method shares many of the positive qualities of the variance-based techniques, having the advantage of a lower computational cost, as demonstrated by the analytical examples. The method is employed to assess the sensitivity of a chemical reaction model for dimethylsulphide (DMS), a gas involved in climate change. Results of the sensitivity analysis open up the ground for model reconsideration: some model components may need a more thorough modelling effort while some others may need to be simplified. (c) 2006 Elsevier Ltd. All rights reserved.",yes,no,no,yes,yes,no,yes,no,yes,no,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,yes,yes,yes,yes,yes,yes,yes,no,no,yes,yes
3,"An unstructured grid, finite-volume, three-dimensional (3D) primitive equation ocean model has been developed for the study of coastal oceanic and estuarine circulation. The model consists of momentum, continuity, temperature, salinity, and density equations and is closed physically and mathematically using the Mellor and Yamada level-2.5 turbulent closure submodel. The irregular bottom slope is represented using a sigma-coordinate transformation, and the horizontal grids comprise unstructured triangular cells. The finite-volume method (FVM) used in this model combines the advantages of a finite-element method (FEM) for geometric flexibility and a finite-difference method (FDM) for simple discrete computation. Currents, temperature, and salinity in the model are computed in the integral form of the equations, which provides a better representation of the conservative laws for mass, momentum, and heat in the coastal region with complex geometry. The model was applied to the Bohai Sea, a semienclosed coastal ocean, and the Satilla River, a Georgia estuary characterized by numerous tidal creeks and inlets. Compared with the results obtained from the finite-difference model (ECOM-si), the new model produces a better simulation of tidal elevations and residual currents, especially around islands and tidal creeks. Given the same initial distribution of temperature in the Bohai Sea, the FVCOM and ECOM-si models show similar distributions of temperature and stratified tidal rectified flow in the interior region away from the coast and islands, but FVCOM appears to provide a better simulation of temperature and currents around the islands, barriers, and inlets with complex topography.",yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,yes,yes,yes,yes,yes,no,yes,yes,yes
4,"We propose a linear-time line segment detector that gives accurate results, a controlled number of false detections, and requires no parameter tuning. This algorithm is tested and compared to state-of-the-art algorithms on a wide set of natural images.",no,no,no,no,yes,yes,yes,yes,no,no,yes,no,yes,no,no,yes,yes,yes,yes,no,no,no,yes,yes,no,yes,yes,yes,no,no,no,yes,yes
5,"We provide a model (for both continuous and discrete time) describing the evolution of a flock. Our model is parameterized by a constant beta capturing the rate of decay-which in our model is polynomial-of the influence between birds in the flock as they separate in space. Our main result shows that when beta < 1/2 convergence of the flock to a common velocity is guaranteed, while for beta < 1/2 convergence-is guaranteed under some condition on the initial positions and velocities of the birds only.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6,"The Landsat Ecosystem Disturbance Adaptive Processing System (LEDAPS) at the National Aeronautics and Space Administration (NASA) Goddard Space Flight Center has processed and released 2100 Landsat Thematic Mapper and Enhanced Thematic Mapper Plus surface reflectance scenes, providing 30-m resolution wall-to-wall reflectance coverage for North America for epochs centered on 1990 and 2000. This dataset can support decadal assessments of environmental and land-cover change, production of reflectance-based biophysical products, and applications that merge reflectance data from multiple sensors [e.g., the Advanced Spaceborne Thermal Emission and Reflection Radiometer, Multiangle Imaging Spectroradiometer, Moderate Resolution Imaging Spectroradiometer (MODIS)]. The raw imagery was obtained from the orthorectified Landsat GeoCover dataset, purchased by NASA from the Earth Satellite Corporation. Through the LEDAPS project, these data were calibrated, converted to top-of-atmosphere reflectance, and then atmospherically corrected using the MODIS/6S methodology. Initial comparisons with ground-based optical thickness measurements and simultaneously acquired MODIS imagery indicate comparable uncertainty in Landsat surface reflectance compared to the standard MODIS reflectance product (the greater of 0.5% absolute reflectance or 5% of the recorded reflectance value). The rapid automated nature of the processing stream also paves the way for routine high-level products from future Landsat sensors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7,"High data rate wireless communications, nearing 1-Gb/s transmission rates, is of interest in emerging wireless local area networks and home audio/visual networks. Designing very high speed wireless links that offer good quality-of-service and range capability in non-line-of-sight (NLOS) environments constitutes a significant research and engineering challenge. Ignoring fading in NLOS environments, we can, in principle, meet the 1-Gb/s data rate requirement with a single-transmit single-receive antenna wireless system if the product of bandwidth (measured in hertz) and spectral efficiency (measured in bits per second per hertz) is equal to 10(9). As we shall outline in this paper a variety of cost, technology and regulatory constraints make such a brute force solution unattractive if not impossible. The use of multiple antennas at transmitter and receiver popularly known as multiple-input multiple-output (MIMO) wireless is an emerging cost-effective technology that offers substantial leverages in making 1-Gb/s wireless links a reality. This paper provides an overview of MIMO wireless technology covering channel models, performance limits, coding, and transceiver design.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8,"Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9,"Thermal energy storage with phase change materials (PCMs) offers a high thermal storage density with a moderate temperature variation, and has attracted growing attention due to its important role in achieving energy conservation in buildings with thermal comfort. Various methods have been investigated by previous researchers to incorporate PCMs into the building structures, and it has been found that with the help of PCMs the indoor temperature fluctuations can be reduced significantly whilst maintaining desirable thermal comfort. This paper summarises previous works on latent thermal energy storage in building applications, covering PCMs, the impregnation methods, current building applications and their thermal performance analyses, as well as numerical simulation of buildings with PCMs. Over 100 references are included in this paper. (C) 2011 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10,"The flow of pollution through international trade flows has the ability to undermine environmental policies, particularly for global pollutants. In this article we determine the CO2 emissions embodied in international trade among 87 countries for the year 2001. We find that globally there are over 5.3 Gt of CO2 embodied in trade and that Annex B countries are net importers Of CO2 emissions. Depending on country characteristics-such as size variables and geographic location-there are considerable variations in the embodied emissions. We argue that emissions embodied in trade may have a significant impact on participation in and effectiveness of global climate policies such as the Kyoto Protocol. We discuss several policy options to reduce the impact of trade in global climate policy. If countries take binding commitments as a part of a coalition, instead of as individual countries, then the impacts of trade can be substantially reduced. Adjusting emission inventories for trade gives a more consistent description of a country's environmental pressures and circumvents many trade related issues. It also gives opportunities to exploit trade as a means of mitigating emissions. Not least, a better understanding of the role that trade plays in a country's economic and environmental development will help design more effective and participatory climate policy post-Kyoto.,",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11,"China's rapidly growing economy and energy consumption are creating serious environmental problems on both local and global scales. Understanding the key drivers behind China's growing energy consumption and the associated CO2 emissions is critical for the development of global climate policies and provides insight into how other emerging economies may develop a low emissions future. Using recently released Chinese economic input-output data and structural decomposition analysis we analyze how changes in China's technology, economic structure, urbanization, and lifestyles affect CO2 emissions. We find that infrastructure construction and urban household consumption, both in turn driven by urbanization and lifestyle changes, have outpaced efficiency improvements in the growth Of CO2 emissions. Net trade had a small effect on total emissions due to equal, but significant, growth in emissions from the production of exports and emissions avoided by imports. Technology and efficiency improvements have only partially offset consumption growth, but there remains considerable untapped potential to reduce emissions by improving both production and consumption systems. As China continues to rapidly develop there is an opportunity to further implement and extend policies, such as the Circular Economy, that will help China avoid the high emissions path taken by today's developed countries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12,"Biodiesel is synthesized via the transesterification of lipid feedstocks with low molecular weight alcohols. Currently, alkaline bases are used to catalyze the reaction. These catalysts require anhydrous conditions and feedstocks with low levels of free fatty acids (FFAs). Inexpensive feedstocks containing high levels of FFAs cannot be directly used with the base catalysts currently employed. Strong liquid acid catalysts are less sensitive to FFAs and can simultaneously conduct esterification and transesterification. However, they are slower and necessitate higher reaction temperatures. Nonetheless, acid-catalyzed processes could produce biodiesel from low-cost feedstocks, lowering production costs. Better yet, if solid acid catalysts could replace liquid acids, the corrosion and environmental problems associated with them could be avoided and product purification protocols reduced, significantly simplifying biodiesel production and reducing cost. This article reviews some of the research related to biodiesel production using acid catalysts, including solid acids.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14,"We study two-hop communication protocols where one or several relay terminals assist in the communication between two or more terminals. All terminals operate in half-duplex mode, hence the transmission of one information symbol from the source terminal to the destination terminal occupies two channel uses. This leads to a loss in spectral efficiency due to the pre-log factor one-half in corresponding capacity expressions. We propose two new half-duplex relaying protocols that avoid the pre-log factor one-half. Firstly, we consider a relaying protocol where a bidirectional connection between two terminals is established via one amplify-and-forward (AF) or decode-and-forward (DF) relay (two-way relaying). We also extend this protocol to a multi-user scenario, where multiple terminals communicate with multiple partner terminals via several orthogonalize-and-forward (OF) relay terminals, i.e., the relays orthogonalize the different two-way transmissions by a distributed zero-forcing algorithm. Secondly, we propose a relaying protocol where two relays, either AF or DF, alternately forward messages from a source terminal to a destination terminal (two-path relaying). It is shown that both protocols recover a significant portion of the half-duplex loss.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15,"The adsorption of Congo Red by coir pith carbon was carried out by varying the parameters such as agitation time, dye concentration, adsorbent dose, pH and temperature. Equilibrium adsorption data followed both Langmuir and Freundlich isotherms. Adsorption followed second-order rate kinetics. The adsorption capacity was found to be 6.7 mg dye per g of the adsorbent. Acidic pH was favourable for the adsorption of Congo Red. Desorption studies suggest that chemisorption might be the major mode of adsorption. (C) 2002 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16,"Usually transitions from microstrip line to rectangular waveguide are made with three-dimensional (3-D) complex mounting structures. In this paper, a new planar platform is developed in which the microstrip line and rectangular waveguide are fully integrated on the same substrate, and they are interconnected via a simple taper, Our experiments at 28 GHz show that an effective bandwidth of 12% at 20 dB return loss is obtained with an in-band insertion loss better than 0.3 dB, The new transition allows a complete integration of waveguide components on substrate with MIC's and MMIC's.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17,"Permanent Scatterer SAR Interferometry (PSInSAR) aims to identify coherent radar targets exhibiting high phase stability over the entire observation time period. These targets often correspond to point-wise, man-made objects widely available over a city, but less present in non-urban areas. To overcome the limits of PSInSAR, analysis of interferometric data-stacks should aim at extracting geophysical parameters not only from point-wise deterministic objects (i.e., PS), but also from distributed scatterers (DS). Rather than developing hybrid processing chains where two or more algorithms are applied to the same data-stack, and results are then combined, in this paper we introduce a new approach, SqueeSAR, to jointly process PS and DS, taking into account their different statistical behavior. As it will be shown, PS and DS can be jointly processed without the need for significant changes to the traditional PSInSAR processing chain and without the need to unwrap hundreds of interferograms, provided that the coherence matrix associated with each DS is properly ""squeezed"" to provide a vector of optimum (wrapped) phase values. Results on real SAR data, acquired over an Alpine area, challenging for any InSAR analysis, confirm the effectiveness of this new approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18,"In this paper, we propose the definition of Mittag-Leffler stability and introduce the fractional Lyapunov direct method. Fractional comparison principle is introduced and the application of Riemann-Liouville fractional order systems is extended by using Caputo fractional order systems. Two illustrative examples are provided to illustrate the proposed stability notion. Published by Elsevier Ltd",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19,"This paper presents a method for face recognition across variations in pose, ranging from frontal to profile views, and across a wide range of illuminations, including cast shadows and specular reflections. To account for these variations, the algorithm simulates the process of image formation in 3D space, using computer graphics, and it estimates 3D shape and texture of faces from single images. The estimate is achieved by fitting a statistical, morphable model of 3D faces to images. The model is learned from a set of textured 3D scans of heads. We describe the construction of the morphable model, an algorithm to fit the model to images, and a framework for face identification. In this framework, faces are represented by model parameters for 3D shape and texture. We present results obtained with 4,488 images from the publicly available CMU-PIE database and 1,940 images from the FERET database.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
20,"Geopolymers are similar to zeolites in chemical composition, but they reveal an amorphous microstructure. They form by the co-polymerisation of individual alumino and silicate species, which originate from the dissolution of silicon and aluminium containing source materials at a high pH in the presence of soluble alkali metal silicates. It has been shown before that geopolymerisation can transform a wide range of waste alumino-silicate materials into building and mining materials with excellent chemical and physical properties, such as fire and acid resistance. The geopolymerisation of 15 natural AI-Si minerals has been investigated in this paper with the aim to determine the effect of mineral properties on the compressive strength of the synthesised geopolymer. All these AI-Si minerals are to some degree soluble in concentrated alkaline solution, with in general a higher extent of dissolution in NaOH than in KOH medium. Statistical analysis revealed that framework silicates show a higher extent of dissolution in alkaline solution than the chain, sheet and ring structures. In general, minerals with a higher extent of dissolution demonstrate better compressive strength after geopolymerisation. The use of KOH instead of NaOH favours the geopolymerisation in the case of all 15 minerals. Stilbite, when conditioned in KOH solution, gives the geopolymer with the highest compressive strength(i.e., 18 MPa). It is proposed that the mechanism of mineral dissolution as well as the mechanism of geopolymerisation can be explained by ion-pair theory. This study shows that a wide range of natural Al-Si minerals could serve as potential source materials for the synthesis of geopolymers. (C) 2000 Elsevier Science B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,"Finding sparse approximate solutions to large underdetermined linear systems of equations is a common problem in signal/image processing and statistics. Basis pursuit, the least absolute shrinkage and selection operator (LASSO), wavelet-based deconvolution and reconstruction, and compressed sensing (CS) are a few well-known areas in which problems of this type appear. One standard approach is to minimize an objective function that includes a quadratic (l(2)) error term added to a sparsity-inducing (usually l(1)) regularizater. We present an algorithmic framework for the more general problem of minimizing the sum of a smooth convex function and a nonsmooth, possibly nonconvex regularizer. We propose iterative methods in which each step is obtained by solving an optimization subproblem involving a quadratic term with diagonal Hessian (i.e., separable in the unknowns) plus the original sparsity-inducing regularizer; our approach is suitable for cases in which this subproblem can be solved much more rapidly than the original problem. Under mild conditions (namely convexity of the regularizer), we prove convergence of the proposed iterative algorithm to a minimum of the objective function. In addition to solving the standard l(2) - l(1) case, our framework yields efficient solution techniques for other regularizers, such as an l(infinity) norm and group-separable regularizers. It also generalizes immediately to the case in which the data is complex rather than real. Experiments with CS problems show that our approach is competitive with the fastest known methods for the standard l(2) - l(1) problem, as well as being efficient on problems with other separable regularization terms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23,"Model predictive control (MPC) is a very attractive solution for controlling power electronic converters. The aim of this paper is to present and discuss the latest developments in MPC for power converters and drives, describing the current state of this control strategy and analyzing the new trends and challenges it presents when applied to power electronic systems. The paper revisits the operating principle of MPC and identifies three key elements in the MPC strategies, namely the prediction model, the cost function, and the optimization algorithm. This paper summarizes the most recent research concerning these elements, providing details about the different solutions proposed by the academic and industrial communities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24,"Neutral-point-clamped (NPC) inverters are the most widely used topology of multilevel inverters in high-power applications (several megawatts). This paper presents in a very simple way the basic operation and the most used modulation and control techniques developed to date. Special attention is paid to the loss distribution in semiconductors, and an active NPC inverter is presented to overcome this problem. This paper discusses the main fields of application and presents some technological problems such as capacitor balance and losses.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25,"A close relationship exists between the advancement of face recognition algorithms and the availability of face databases varying factors that affect facial appearance in a controlled manner. The CMU PIE database has been very influential in advancing research in face recognition across pose and illumination. Despite its success the PIE database has several shortcomings: a limited number of subjects, a single recording session and only few expressions captured. To address these issues we collected the CMU Multi-PIE database. It contains 337 subjects, imaged under 15 view points and 19 illumination conditions in up to four recording sessions. In this paper we introduce the database and describe the recording procedure. We furthermore present results from baseline experiments using PCA and LDA classifiers to highlight similarities and differences between PIE and Multi-PIE. (C) 2009 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26,"Excitation-emission matrixes (EEMs) of 379 dissolved organic matter (DOM) samples from diverse aquatic environments were modeled by parallel factor analysis (PARAFAC). Thirteen components likely representing groups of similarly fluorescing moieties were found to explain the variation in this data set. Seven of the thirteen components were identified as quinone-like based on comparison of their excitation and emission spectra to spectra of model quinones. These quinone-like fluorophores were found to vary in redox state and degree of conjugation. Two components were identified as amino acid-like based on comparison to tyrosine and tryptophan fluorescence spectra. The other four components are not yet associated with any class of molecules. The quinone-like fluorophores account for about 50% of the fluorescence for every sample analyzed, showing that quinone-like fluorophores are an important and ubiquitous fluorescing moiety and in natural waters. Further, the distribution of the quinone-like fluorophores was evaluated as a function of environmental and laboratory redox gradients. Under reducing conditions, the contribution of the reduced quinone-like fluorophores increased concurrent with a decrease in the oxidized quinone-like fluorophores, indicating that DOM fluorescence is a function of redox state of quinone-like moieties. Lastly, a ratio of two quinone-like fluorophores was found to explain the variation in the fluorescence index. These results provide new insight into the redox reactivity of DOM and have implications for the application of fluorescence spectroscopy as a tool to characterize DOM.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27,"The aim of this paper is to advance research on sustainable innovation by adopting a business model perspective. Through a confrontation of the literature on both topics we find that research on sustainable innovation has tended to neglect the way in which firms need to combine a value proposition, the organization of the upstream and downstream value chain and a financial model in order to bring sustainable innovations to the market. Therefore, we review the current literature on business models in the contexts of technological, organizational and social innovation. As the current literature does not offer a general conceptual definition of sustainable business models, we propose examples of normative requirements that business models should meet in order to support sustainable innovations. Finally, we sketch the outline of a research agenda by formulating a number of guiding questions. (C) 2012 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,"As the spectral efficiency of a point-to-point link in cellular networks approaches its theoretical limits, with the forecasted explosion of data traffic, there is a need for an increase in the node density to further improve network capacity. However, in already dense deployments in today's networks, cell splitting gains can be severely limited by high inter-cell interference. Moreover, high capital expenditure cost associated with high power macro nodes further limits viability of such an approach. This article discusses the need for an alternative strategy, where low power nodes are overlaid within a macro network, creating what is referred to as a heterogeneous network. We survey current state of the art in heterogeneous deployments and focus on 3GPP LTE air interface to describe future trends. A high-level overview of the 3GPP LTE air interface, network nodes, and spectrum allocation options is provided, along with the enabling mechanisms for heterogeneous deployments. Interference management techniques that are critical for LTE heterogeneous deployments are discussed in greater detail. Cell range expansion, enabled through cell biasing and adaptive resource partitioning, is seen as an effective method to balance the load among the nodes in the network and improve overall trunking efficiency. An interference cancellation receiver plays a crucial role in ensuring acquisition of weak cells and reliability of control and data reception in the presence of legacy signals.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29,"Microplastics are ubiquitous across ecosystems, yet the exposure risk to humans is unresolved. Focusing on the American diet, we evaluated the number of microplastic particles in commonly consumed foods in relation to their recommended daily intake. The potential for microplastic inhalation and how the source of drinking water may affect microplastic consumption were also explored. Our analysis used 402 data points from 26 studies, which represents over 3600 processed samples. Evaluating approximately 15% of Americans' caloric intake, we estimate that annual microplastics consumption ranges from 39000 to 52000 particles depending on age and sex. These estimates increase to 74000 and 121000 when inhalation is considered. Additionally, individuals who meet their recommended water intake through only bottled sources may be ingesting an additional 90000 microplastics annually, compared to 4000 microplastics for those who consume only tap water. These estimates are subject to large amounts of variation; however, given methodological and data limitations, these values are likely underestimates.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30,"A new plane stress yield function that well describes the anisotropic behavior of sheet metals, in particular, aluminum alloy sheets, was proposed. The anisotropy of the function was introduced in the formulation using two linear transformations on the Cauchy stress tensor. It was shown that the accuracy of this new function was similar to that of other recently proposed non-quadratic yield functions. Moreover, it was proved that the function is convex in stress space. A new experiment was proposed to obtain one of the anisotropy coefficients. This new formulation is expected to be particularly suitable for finite element (FE) modeling simulations of sheet forming processes for aluminum alloy sheets. (C) 2002 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31,"The state-of-the-art on generalized (or any order) derivatives in physics and engineering sciences, is outlined for justifying the interest of the noninteger differentiation. The problems subsequent to its use in real-time operations are then set out so as to motivate the idea of synthesizing it by a recursive distribution of zeros and poles. An analysis of the existing work is also proposed to support this idea. A comprehensive study is given of the synthesis of differentiators with integer, noninteger, real or complex orders, and whose action is limited to any given frequency bandwidth. First, a definition, in the operational and frequency domains, of a frequency-band complex noninteger order differentiator, is given in a mathematical space with four dimensions which is a Banach algebra. Then, the determination of its synthesized form, by a recursive distribution of complex zeros and poles characterized by complex recursive factors, is presented, The complex noninteger differentiation order is expressed as a function of these recursive factors. The number of zeros and poles is calculated to he as low as possible while still ensuring the stability of the synthesized differentiator and providing a suitable approximation to the initial differentiator to be synthesized. A time validation is presented. Finally, guidelines are proposed for the conception of the synthesized differentiator.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32,"As a result of human activity, approximately 7 Gt of carbon are emitted to the earth's atmosphere each year. A large portion of this carbon is in the form of gaseous CO2, and approximately 30% of this CO2 Comes from fossil fuel power plants. In addition to rising levels of atmospheric CO2, the earth's temperature is increasing. Since CO2 can act as a trap for heat (similar to the glass in a greenhouse), reduction of CO2 emissions is an important area of research. Separation and sequestration of CO2 are near-term goals for emissions reduction. Better fuel efficiency (in power production, transportation, and other areas) can be considered a mid-term goal. An acceptable long-term goal for reducing emissions is using alternate power sources such as nuclear, solar, and wind power. Because separation and sequestration are short-term goals, they are critical and challenging steps for researchers. Methods that are reviewed in this paper include absorption using solvents or solid sorbents, pressure- and temperature-swing adsorption using various solid sorbents, cryogenic distillation, membranes, and several novel and emerging technologies. Upon completion of this review, it was concluded that the most promising current method for CO2 separation is liquid absorption using monoethanolamine (MEA). While this method is currently most promising, the development of ceramic and metallic membranes for membrane diffusion should produce membranes significantly more efficient at separation than liquid absorption. The other methods investigated in this report are either too new for comparison or appear unlikely to experience significant changes to make them desirable for implementation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33,"In this technical note, we present a new class of event triggering mechanisms for event-triggered control systems. This class is characterized by the introduction of an internal dynamic variable, which motivates the proposed name of dynamic event triggering mechanism. The stability of the resulting closed-loop system is proved and the influence of design parameters on the decay rate of the Lyapunov function is discussed. For linear systems, we establish a lower bound on the inter-execution time as a function of the parameters. The influence of these parameters on a quadratic integral performance index is also studied. Some simulation results are provided for illustration of the theoretical claims.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34,"Due to the high maneuverability, flexible deployment, and low cost, unmanned aerial vehicles (UAVs) have attracted significant interest recently in assisting wireless communication. This paper considers a multi-UAV enabled wireless communication system, where multiple UAV-mounted aerial base stations are employed to serve a group of users on the ground. To achieve fair performance among users, we maximize the minimum throughput over all ground users in the downlink communication by optimizing the multiuser communication scheduling and association jointly with the UAV's trajectory and power control. The formulated problem is a mixed integer nonconvex optimization problem that is challenging to solve. As such, we propose an efficient iterative algorithm for solving it by applying the block coordinate descent and successive convex optimization techniques. Specifically, the user scheduling and association, UAV trajectory, and transmit power are alternately optimized in each iteration. In particular, for the nonconvex UAV trajectory and transmit power optimization problems, two approximate convex optimization problems are solved, respectively. We further show that the proposed algorithm is guaranteed to converge. To speed up the algorithm convergence and achieve good throughput, a low-complexity and systematic initialization scheme is also proposed for the UAV trajectory design based on the simple circular trajectory and the circle packing scheme. Extensive simulation results are provided to demonstrate the significant throughput gains of the proposed design as compared to other benchmark schemes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35,"This paper contains ground-motion prediction equations (GMPEs) for average horizontal-component ground motions as a function of earthquake magnitude, distance from source to site, local average shear-wave velocity, and fault type. Our equations are for peak ground acceleration (PGA), peak ground velocity (PGV), and 5%-damped pseudo-absolute-acceleration spectra (PSA) at periods between 0.01 s and 10 s. They were derived by empirical regression of an extensive strong-motion database compiled by the ""PEER NGA"" (Pacific Earthquake Engineering Research Center's Next Generation Attenuation) project. For periods less than 1 s, the analysis used 1,574 records from 58 mainshocks in the distance range from 0 km to 400 km (the number of available data decreased as period increased). The primary predictor variables are moment magnitude (M), closest horizontal distance to the surface projection of the fault plane (R-JB), and the time-averaged shear-wave velocity from the surface to 30 m (V-S30). The equations are applicable for M=5-8, R-JB < 200 km, and V-S30=180-1300 m/s.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36,"Municipal effluent discharged from wastewater treatment works (WwTW) is suspected to be a significant contributor of microplastics (MP) to the environment as many personal care products contain plastic microbeads. A secondary WwTW (population equivalent 650 000) was sampled for microplastics at different stages of the treatment process to ascertain at what stage in the treatment process the MP are being removed. The influent contained on average 15.70 (+/-5.23) MP.L-1. This was reduced to 0.25 (+/-0.04) MP.L-1 in the final effluent, a decrease of 98.41%. Despite this large reduction we calculate that this WwTW is releasing 65 million microplastics into the receiving water every day. A significant proportion of the microplastic accumulated in and was removed during the grease removal stage (19.67 (+/-4.51) MP/2.5 g), it was only in the grease that the much publicised microbeads were found. This study shows that despite the efficient removal rates of MP achieved by this modern treatment plant when dealing with such a large volume of effluent even a modest amount of microplastics being released per liter of effluent could result in significant amounts of microplastics entering the environment. This is the first study to describe in detail the fate of microplastics during the wastewater treatment process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37,"Plastic resin pellets (small granules 0.1-0.5 centimeters in diameter) are widely distributed in the ocean all over the world. They are an industrial raw material for the plastic industry and are unintentionally released to the environment both during manufacturing and transport. They are sometimes ingested by seabirds and other marine organisms, and their adverse effects on organisms are a concern. In the present study, PCBs, DDE, and nonylphenols (NP) were detected in polypropylene (PP) resin pellets collected from four Japanese coasts. Concentrations of PCBs (4-117 ng/g), DDE (0.16-3.1 ng/g), and NP (0.13-16 mug/g) varied among the sampling sites. These concentrations were comparable to those for suspended particles and bottom sediments collected from the same area as the pellets. Field adsorption experiments using PP virgin pellets demonstrated significant and steady increase in PCBs and DDE concentrations throughout the six-day experiment, indicating that the source of PCBs and DDE is ambient seawater and that adsorption to pellet surfaces is the mechanism of enrichment. The major source of NP in the marine PP resin pellets was thought to be plastic additives and/or their degradation products. Comparison of PCBs and DDE concentrations in marine PP resin pellets with those in seawater suggests their high degree of accumulation (apparent adsorption coefficient: 10(5)-10(6)). The high accumulation potential suggests that plastic resin pellets serve as both a transport medium and a potential source of toxic chemicals in the marine environment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38,"This article presents the theoretical basis for modeling univariate traffic condition data streams as seasonal autoregressive integrated moving average processes. This foundation rests on the Wold decomposition theorem and on the assertion that a one-week lagged first seasonal difference applied to discrete interval traffic condition data will yield a weakly stationary transformation. Moreover, empirical results using actual intelligent transportation system data are presented and found to be consistent with the theoretical hypothesis. Conclusions are given on the implications of these assertions and findings relative to ongoing intelligent transportation systems research, deployment, and operations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39,"MOSFETs with gate length down to 17 nm are reported. To suppress the short channel effect, a novel self-aligned double-gate MOSFET, FinFET, is proposed, By using boron-doped Si0.4Ge0.6 as a gate material, the desired threshold voltage was achieved for the ultrathin body device. The quasiplanar nature of this new variant of the vertical double-gate MOSFETs can be fabricated relatively easily using the conventional planar MOSFET process technologies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40,"Frequencies from 100 GHz to 3 THz are promising bands for the next generation of wireless communication systems because of the wide swaths of unused and unexplored spectrum. These frequencies also offer the potential for revolutionary applications that will be made possible by new thinking, and advances in devices, circuits, software, signal processing, and systems. This paper describes many of the technical challenges and opportunities for wireless communication and sensing applications above 100 GHz, and presents a number of promising discoveries, novel approaches, and recent results that will aid in the development and implementation of the sixth generation (6G) of wireless networks, and beyond. This paper shows recent regulatory and standard body rulings that are anticipating wireless products and services above 100 GHz and illustrates the viability of wireless cognition, hyper-accurate position location, sensing, and imaging. This paper also presents approaches and results that show how long distance mobile communications will be supported to above 800 GHz since the antenna gains are able to overcome air-induced attenuation, and present methods that reduce the computational complexity and simplify the signal processing used in adaptive antenna arrays, by exploiting the Special Theory of Relativity to create a cone of silence in over-sampled antenna arrays that improve performance for digital phased array antennas. Also, new results that give insights into power efficient beam steering algorithms, and new propagation and partition loss models above 100 GHz are given, and promising imaging, array processing, and position location results are presented. The implementation of spatial consistency at THz frequencies, an important component of channel modeling that considers minute changes and correlations over space, is also discussed. This paper offers the first in-depth look at the vast applications of THz wireless products and applications and provides approaches for how to reduce power and increase performance across several problem domains, giving early evidence that THz techniques are compelling and available for future wireless communications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41,"A new method for detecting spikes in acoustic Doppler velocimeter data sequences is suggested. The method combines three concepts: (1) that differentiation enhances the high frequency portion of a signal, (2) that the expected maximum of a random series is given by the Universal threshold, and (3) that good data cluster in a dense cloud in phase space or Poincare maps. These concepts are used to construct an ellipsoid in three-dimensional phase space, then points lying outside the ellipsoid are designated as spikes. The new method is shown to have superior performance to various other methods and it has the added advantage that it requires no parameters. Several methods for replacing sequences of spurious data are presented. A polynomial fitted to good data on either side of the spike event, then interpolated across the event, is preferred by the authors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,"The natural convective boundary-layer flow of a nanofluid past a vertical plate is studied analytically. The model used for the nanofluid incorporates the effects of Brownian motion and thermophoresis. A similarity solution is presented. This solution depends on a Lewis number Le, a buoyancy-ratio number Nr, a Brownian motion number Nb, and a thermophoresis number Nt. For various values of Pr and Le, the variation of the reduced Nusselt number with Nr, Nb and Nt is expressed by correlation formulas. It was found that the reduced Nusselt number is a decreasing function of each of Nr, Nb and Nt. (C) 2009 Elsevier Masson SAS. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43,"This paper presents methods for collecting and analyzing physiological data during real-world driving tasks to determine a driver's relative stress level. Electrocardiogram, electromyogram, skin conductance, and respiration were recorded continuously while drivers followed a set route through open roads in the greater Boston area. Data from 24 drives of at least 50-min duration were collected for analysis. The data were analyzed in two ways. Analysis I used features from 5-min intervals of data during the rest, highway, and city driving conditions to distinguish three levels of driver stress with an accuracy of over 97% across multiple drivers and driving days. Analysis II compared continuous features, calculated at 1-s intervals throughout the entire drive, with a metric of observable stressors created by independent coders from videotapes. The results show that for most drivers studied, skin conductivity and heart rate metrics are most closely correlated with driver stress level. These findings indicate that physiological signals can provide a metric of driver stress in future cars capable of physiological monitoring. Such a metric could be used to help manage noncritical in-vehicle information systems and could also provide a continuous measure of how different road and traflic conditions affect drivers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44,"The importance of atmospheric aerosols in regulating the Earth's climate and their potential detrimental impact on air quality and human health has stimulated the need for instrumentation which can provide real-time analysis of size resolved aerosol, mass, and chemical composition, We describe here an aerosol mass spectrometer (AMS) which has been developed in response to these aerosol sampling needs and present results which demonstrate quantitative measurement capability for a laboratory-generated pure component NH4NO3 aerosol, The instrument combines standard vacuum and mass spectrometric technologies with recently developed aerosol sampling techniques, A unique aerodynamic aerosol inlet (developed at the University of Minnesota) focuses particles into a narrow beam and efficiently transports them into vacuum where aerodynamic particle size is determined via a particle time-of-flight (TOF) measurement, Time-resolved particle mass detection is performed mass spectrometrically following particle flash vaporization on a resistively heated surface, Calibration data are presented for aerodynamic particle velocity and particle collection efficiency measurements, The capability to measure aerosol size and mass distributions is compared to simultaneous measurements using a differential mobility analyzer (DMA) and condensation particle counter (CPC), Quantitative size classification is demonstrated for pure component NH4NO3 aerosols having mass concentrations > similar to 0.25 mu g m(-3), Results of fluid dynamics calculations illustrating the performance of the aerodynamic lens are also presented and compared to the measured performance, The utility of this AMS as both a laboratory and held portable instrument is discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45,"A new trend of product-service systems (PSSs) that has the potential to minimise environmental impacts of both production and consumption is emerging. This article attempts to build a theoretical framework for PSS and serves as a background for identifying possible investment needs in studying them. There are three main uncertainties regarding the applicability and feasibility of PSSs: the readiness of companies to adopt them, the readiness of consumers to accept them, and their environmental implications.. The main finding is that successful PSSs will require different societal infrastructure, human structures and organisational layouts in order to function in a sustainable manner. (C) 2002 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46,"In this paper, we show that the coarsest, or least dense, quantizer that quadratically stabilizes a single input linear discrete time invariant system is logarithmic, and can be computed by solving a special linear quadratic regulator (LQR) problem. We provide a closed form for the optimal logarithmic base exclusively in terms of the unstable eigenvalues of the system. We show how to design quantized state-feedback controllers, and quantized state estimators. This leads to the design of hybrid output feedback controllers. The theory is then extended to sampling and quantization of continuous time linear systems sampled at constant time intervals. We generalize the definition of density of quantization to the density of sampling and quantization in a natural way, and search for the coarsest sampling and quantization scheme that ensures stability. We show that the resulting optimal sampling time is only function of the sum of the unstable eigenvalues of the continuous time system, and that the associated optimal quantizer is logarithmic with the logarithmic base being a universal constant independent of the system. The coarsest sampling and quantization scheme so obtained is related to the concept of minimal attention control recently introduced by Brockett. Finally, by relaxing the definition of quadratic stability, we show how to construct logarithmic quantizers with only finite number of quantization levels and still achieve practical stability of the closed-loop system. This final result provides a way to practically implement the theory developed in this paper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47,"The need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. We present here SIMPLIcity (Semantics-sensitive Integrated Matching for Picture Libraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. As in other region-based retrieval systems, an image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories, such as textured-nontextured, graph-photograph. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. Compared with retrieval based on individual regions, the overall similarity approach 1) reduces the adverse effect of inaccurate segmentation, 2) helps to clarify the semantics of a particular region, and 3) enables a simple querying interface for region-based image retrieval systems. The application of SIMPLIcity to several databases, including a database of about 200,000 general-purpose images, has demonstrated that our system performs significantly better and faster than existing ones. The system is fairly robust to image alterations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48,"Motor imagery can modify the neuronal activity in the primary sensorimotor areas in a very similar way as observable with a real executed movement. One part of EEG-based brain-computer interfaces (BCI) is based on the recording and classification of circumscribed and transient EEG changes during different types of motor imagery such as, e.g., imagination of left-hand, right-hand, or foot movement. Features such as, e.g., band power or adaptive autoregressive parameters are either extracted in bipolar EEG recordings overlaying sensorimotor areas or from an array of electrodes located over central and neighboring areas. For the classification of the features, linear discrimination analysis and neural networks is used. Characteristic for the Graz BCI is that a classifier is set up in a learning session and updated after one or more sessions with online feedback using the procedure of ""rapid prototyping. "" As a result, a discrimination of two brain states (e.g., left- versus right-hand movement imagination) can be reached within only a few days of training. At this time, a tetraplegic patient is able to operate an EEG-based control of a hand orthosis with nearly 100% classification accuracy, by mental imagination of specific motor commands.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49,"A kinetic study was performed to determine the influence of particle size on the in vivo tissue distribution of spherical-shaped gold nanoparticles in the rat. Gold nanoparticles were chosen as model substances as they are used in several medical applications. In addition, the detection of the presence of gold is feasible with no background levels in the body in the normal situation. Rats were intravenously injected in the tail vein with gold nanoparticles with a diameter of 10, 50, 100 and 250 nm, respectively. After 24 h, the rats were sacrificed and blood and various organs were collected for gold determination. The presence of gold was measured quantitatively with inductively coupled plasma mass spectrometry (ICP-MS). For all gold nanoparticle sizes the majority of the gold was demonstrated to be present in liver and spleen. A clear difference was observed between the distribution of the 10 rum particles and the larger particles. The 10 rum particles were present in various organ systems including blood, liver, spleen, kidney, testis, thymus, heart, lung and brain, whereas the larger particles were only detected in blood, liver and spleen. The results demonstrate that tissue distribution of gold nartoparticles is size-dependent with the smallest 10 run nanoparticles showing the most widespread organ distribution. (C) 2008 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
50,"The substrate integrated waveguide (SIW) technique makes it possible that a complete circuit including planar circuitry, transitions, and rectangular waveguides are fabricated in planar form using a standard printed circuit board or other planar processing techniques. In this paper, guided wave and modes characteristics of such an SIW periodic structure are studied in detail for the first time. A numerical multimode calibration procedure is proposed and developed with a commercial software package on the basis of a full-wave finite-element method for the accurate extraction of complex propagation constants of the SIW structure. Two different lengths of the SIW are numerically simulated under multimode excitation. By means of our proposed technique, the complex propagation constant of each SIW mode can accurately be extracted and the electromagnetic bandstop phenomena of periodic structures are also investigated. Experiments are made to validate our proposed technique. Simple design rules are provided and discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
51,"The effect of pH on the hydrogen oxidation and evolution reaction (HOR/HER) rates is addressed for the first time for the three most active monometallic surfaces: Pt, Ir, and Pd carbon-supported catalysts. Kinetic data were obtained for a proton exchange membrane fuel cell (PEMFC; pH approximate to 0) using the H-2-pump mode and with a rotating disk electrode (RDE) in 0.1 M NaOH. Our findings point toward: (i) a similar approximate to 100-fold activity decrease on all these surfaces when going from low to high pH; (ii) a reaction rate controlled by the Volmer step on Pt/C; and (iii) the H-binding energy being the unique and sole descriptor for the HOR/HER in alkaline electrolytes. Based on a detailed discussion of our data, we propose a new mechanism for the HOR/HER on Pt-metals in alkaline electrolytes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52,"Purpose - Nature-inspired algorithms are among the most powerful algorithms for optimization. The purpose of this paper is to introduce a new nature-inspired metaheuristic optimization algorithm, called bat algorithm (BA), for solving engineering optimization tasks. Design/methodology/approach - The proposed BA is based on the echolocation behavior of bats. After a detailed formulation and explanation of its implementation, BA is verified using eight nonlinear engineering optimization problems reported in the specialized literature. Findings - BA has been carefully implemented and carried out optimization for eight well-known optimization tasks; then a comparison has been made between the proposed algorithm and other existing algorithms. Originality/value - The optimal solutions obtained by the proposed algorithm are better than the best solutions obtained by the existing methods. The unique search features used in BA are analyzed, and their implications for future research are also discussed in detail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53,"Gain scheduling for nonlinear controller design is described in terms of general features of the approach and in terms of early examples of applications in flight control and automotive engine control. Then recent research is discussed, emphasizing work on linearization-based scheduling and work on linear parameter-varying approaches. (C) 2000 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54,"Suppose a discrete-time signal S(t), 0 less than or equal to t < N, is a superposition of atoms taken from a combined time-frequency dictionary made of spike sequences 1({t=r}) and sinusoids exp{2 pi iwt/N)/rootN. Can one recover, from knowledge of S alone, the precise collection of atoms going to make up S? Because every discrete-time signal can be represented as a superposition of spikes alone, or as a superposition of sinusoids alone, there is no unique way of writing S as a sum of spikes and sinusoids in general. We prove that if S is representable as a highly sparse superposition of atoms from this time-frequency dictionary, then there is only one such highly sparse representation of S, and it can be obtained by solving the convex optimization problem of minimizing the l(1) norm of the coefficients among all decompositions. Here ""highly sparse"" means that N-t + N-w < rootN/2 where N-t is the number of time atoms, N, is the number of frequency atoms, and N is the length of the discrete-time signal. Underlying this result is a general l(1) uncertainty principle which says that if two bases are mutually incoherent, no nonzero signal can have a sparse representation in both bases simultaneously. For the above setting, the bases are sinuosids and spikes, and mutual incoherence is measured in terms of the largest inner product between different basis elements. The uncertainty principle holds for a variety of interesting basis pairs, not just sinusoids and spikes. The results have idealized applications to band-limited approximation with gross errors, to error-correcting encryption, and to separation of uncoordinated sources. Related phenomena hold for functions of a real variable, with basis pairs such as sinusoids and wavelets, and for functions of two variables, with basis pairs such as wavelets and ridgelets. In these settings, if a function f is representable by a sufficiently sparse superposition of terms taken from both bases, then there is only one such sparse representation; it may be obtained by minimum l(1) norm atomic decomposition. The condition ""sufficiently sparse"" becomes a multiscale condition; for example, that the number of wavelets at level j plus the number of sinusoids in the jth dyadic frequency band are together less than a constant times 2(j/2).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
55,"Self-assembled monolayers (SAMs) of alkanethiols, which can provide flat and chemically well-defined surfaces, were employed as model surfaces to understand cellular interaction with artificial materials. SAMs presenting a wide range of wettabilities were prepared by mixing two kinds of alkanethiols carrying terminal methyl (CH3) hydroxyl (OH), carboxylic acid (COOH), or amino (NH2) groups. Adhesion behavior of human umbilical vein endothelial cells (HUVECs) and HeLa cells on these mixed SAMs were examined. The number of adhered HUVECs reached a maximum on CH3/OH mixed SAMs with a water contact angle of 40 degrees, while cell adhesion increased with decreasing water contact angle up to 60-70 degrees and then leveled off on CH3/COOH and CH3/NH2 mixed SAMs. Numbers of adhered HeLa cells showed a maximum on CH3/OH and CH3/COOH mixed SAMs with a water contact angle of 50 degrees. These facts suggest that cell adhesion is mainly determined by surface wettability, but is also affected by the surface functional group, its surface density, and the kinds of cells. The effect of exchange of adsorbed proteins on cell adhesion was also examined. HUVECs were cultured on the mixed SAMs preadsorbed with albumin. Cell adhesion was effectively prohibited on hydrophobic SAMs pretreated with albumin. Albumin strongly adsorbed and resisted replacement by cell adhesive proteins on hydrophobic SAMs. On the other hand, cells adhered to albumin-adsorbed hydrophilic SAW Displacement of preadsorbed albumin with cell adhesive proteins effectively occurs on these hydrophilic SAMs. This effect contributes to induce SAMs with moderate wettability to give suitable surfaces for cell adhesion. (c) 2007 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56,"High K-u, uniaxial magnetocrystalline anisotropy, materials are generally attractive for ultrahigh density magnetic recording applications as they allow smaller, thermally stable media grains. Prominent candidates are rare-earth transition metals (Co5Sm,...) and tetragonal intermetallic compounds (L1(0) phases FePt, CoPtY,...), which have 20-40 times higher K-u than today's hexagonal Co-alloy based media. This allows for about 3 times smaller grain diameters, D, and a potential 10-fold areal density increase (proportional to 1 / D-2), well beyond the currently projected 40-100 Gbits/in(2) mark. Realization of such densities will depend on a large number of factors, not all related to solving media microstructure problems, In particular, it is at present not known how to record into such media, which may require write fields in the order of 10-100 kOe. Despite this unsolved problem, there is considerable interest in high Ku alternative media, both for longitudinal and perpendicular recording. Activities in this area will be reviewed and data on sputtered and evaporated thin FePt films, with coercivities exceeding 10000 Oe will be presented.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
57,"We describe a novel general strategy for building steganography detectors for digital images. The process starts with assembling a rich model of the noise component as a union of many diverse submodels formed by joint distributions of neighboring samples from quantized image noise residuals obtained using linear and nonlinear high-pass filters. In contrast to previous approaches, we make the model assembly a part of the training process driven by samples drawn from the corresponding cover-and stego-sources. Ensemble classifiers are used to assemble the model as well as the final steganalyzer due to their low computational complexity and ability to efficiently work with high-dimensional feature spaces and large training sets. We demonstrate the proposed framework on three steganographic algorithms designed to hide messages in images represented in the spatial domain: HUGO, edge-adaptive algorithm by Luo et al. [32], and optimally coded ternary 1 embedding. For each algorithm, we apply a simple submodel-selection technique to increase the detection accuracy per model dimensionality and show how the detection saturates with increasing complexity of the rich model. By observing the differences between how different submodels engage in detection, an interesting interplay between the embedding and detection is revealed. Steganalysis built around rich image models combined with ensemble classifiers is a promising direction towards automatizing steganalysis for a wide spectrum of steganographic schemes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58,"Contemporary workflow management systems are driven by explicit process models, i.e., a completely specified workflow design is required in order to enact a given workflow process. Creating a workflow design is a complicated time-consuming process and, typically, there are discrepancies between the actual workflow processes and the processes as perceived by the management. Therefore, we have developed techniques for discovering workflow models. The starting point for such techniques is a so-called ""workflow log"" containing information about the workflow process as it is actually being executed. We present a new algorithm to extract a process model from such a log and represent it in terms of a Petri net. However, we will also demonstrate that it is not possible to discover arbitrary workflow processes. In this paper, we explore a class of workflow processes that can be discovered. We show that the alpha-algorithm can successfully mine any workflow represented by a so-called SWF-net.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59,"External confinement by the wrapping of FRP sheets (or FRP jacketing) provides a very effective method for the retrofit of reinforced concrete (RC) columns subject to either static or seismic loads. For the reliable and cost-effective design of FRP jackets, an accurate stress-strain model is required for FRP-confined concrete. In this paper, a new design-oriented stress-strain model is proposed for concrete confined by FRP wraps with fibres only or predominantly in the hoop direction based on a careful interpretation of existing test data and observations. This model is simple, so it is suitable for direct use in design, but in the meantime, it captures all the main characteristics of the stress-strain behavior of concrete confined by different types of FRP. In addition, for unconfined concrete, this model reduces directly to idealized stress-strain curves in existing design codes. In the development of this model, a number of important issues including the actual hoop strains in FRP jackets at rupture, the sufficiency of FRP confinement for a significant strength enhancement, and the effect of jacket stiffness on the ultimate axial strain, were all carefully examined and appropriately resolved. The predictions of the model are shown to agree well with test data. (C) 2003 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
60,"Aiming to promptly process the massive fault data and automatically provide accurate diagnosis results, numerous studies have been conducted on intelligent fault diagnosis of rotating machinery. Among these studies, the methods based on artificial neural networks (ANNs) are commonly used, which employ signal processing techniques for extracting features and further input the features to ANNs for classifying faults. Though these methods did work in intelligent fault diagnosis of rotating machinery, they still have two deficiencies. (1) The features are manually extracted depending on much prior knowledge about signal processing techniques and diagnostic expertise. In addition, these manual features are extracted according to a specific diagnosis issue and probably unsuitable for other issues. (2) The ANNs adopted in these methods have shallow architectures, which limits the capacity of ANNs to learn the complex non-linear relationships in fault diagnosis issues. As a breakthrough in artificial intelligence, deep learning holds the potential to overcome the aforementioned deficiencies. Through deep learning, deep neural networks (DNNs) with deep architectures, instead of shallow ones, could be established to mine the useful information from raw data and approximate complex non-linear functions. Based on DNNs, a novel intelligent method is proposed in this paper to overcome the deficiencies of the aforementioned intelligent diagnosis methods. The effectiveness of the proposed method is validated using datasets from rolling element bearings and planetary gearboxes. These datasets contain massive measured signals involving different health conditions under various operating conditions. The diagnosis results show that the proposed method is able to not only adaptively mine available fault characteristics from the measured signals, but also obtain superior diagnosis accuracy compared with the existing methods. (C) 2015 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
61,"In contrast to discrete descriptions of fracture, phase-field descriptions do not require numerical tracking of discontinuities in the displacement field. This greatly reduces implementation complexity. In this work, we extend a phase-field model for quasi-static brittle fracture to the dynamic case. We introduce a phase-field approximation to the Lagrangian for discrete fracture problems and derive the coupled system of equations that govern the motion of the body and evolution of the phase-field. We study the behavior of the model in one dimension and show how it influences material properties. For the temporal discretization of the equations of motion, we present both a monolithic and staggered time integration scheme. We study the behavior of the dynamic model by performing a number of two and three dimensional numerical experiments. We also introduce a local adaptive refinement strategy and study its performance in the context of locally refined T-splines. We show that the combination of the phase-field model and local adaptive refinement provides an effective method for simulating fracture in three dimensions. (C) 2012 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62,"Sparse representations of signals have drawn considerable interest in recent years. The assumption that natural signals, such as images, admit a sparse decomposition over a redundant dictionary leads to efficient algorithms for handling such sources of data. In particular, the design of well adapted dictionaries for images has been a major challenge. The K-SVD has been recently proposed for this task [1] and shown to perform very well for various grayscale image processing tasks. In this paper, we address the problem of learning dictionaries for color images and extend the K-SVD-based grayscale image denoising algorithm that appears in [2]. This work puts forward ways for handling nonhomogeneous noise and missing information, paving the way to state-of-the-art results in applications such as color image denoising, demosaicing, and inpainting, as demonstrated in this paper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63,A version of nonlocal elasticity theory is employed to develop a nonlocal Benoulli/Euler beam model. Some representative problems are solved to illustrate the magnitude of predicted nonlocal effects. Particular attention is paid to cantilever beams which are often used as actuators in small scale systems. (C) 2002 Elsevier Science Ltd. All rights reserved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64,"To evaluate the potential of Na-ion batteries, we contrast in this work the difference between Na-ion and Li-ion based intercalation chemistries in terms of three key battery properties-voltage, phase stability and diffusion barriers. The compounds investigated comprise the layered AMO(2) and AMS(2) structures, the olivine and maricite AMPO(4) structures, and the NASICON A(3)V(2)(PO4)(3) structures. The calculated Na voltages for the compounds investigated are 0.18-0.57 V lower than that of the corresponding Li voltages, in agreement with previous experimental data. We believe the observed lower voltages for Na compounds are predominantly a cathodic effect related to the much smaller energy gain from inserting Na into the host structure compared to inserting Li. We also found a relatively strong dependence of battery properties on structural features. In general, the difference between the Na and Li voltage of the same structure, Delta VNa-Li, is less negative for the maricite structures preferred by Na, and more negative for the olivine structures preferred by Li. The layered compounds have the most negative Delta VNa-Li. In terms of phase stability, we found that open structures, such as the layered and NASICON structures, that are better able to accommodate the larger Na+ ion generally have both Na and Li versions of the same compound. For the close-packed AMPO(4) structures, our results show that Na generally prefers the maricite structure, while Li prefers the olivine structure, in agreement with previous experimental work. We also found surprising evidence that the barriers for Na+ migration can potentially be lower than that for Li+ migration in the layered structures. Overall, our findings indicate that Na-ion systems can be competitive with Li-ion systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65,"This paper is mainly concerned about the heat transfer behaviour of aqueous suspensions of multi-walled carbon nanotubes (CNT nanofluids) flowing through a horizontal tube. Significant enhancement of the convective heat transfer is observed and the enhancement depends on the flow conditions (Reynolds number, Re), CNT concentration and the pH, with the effect of pH smallest. Given other conditions, the enhancement is a function of axial distance from the inlet, increasing first, reaching a maximum, and then decreasing with increasing axial distance. The axial position of the maximum enhancement increases with CNT concentration and Re. Given CNT concentration and the pH level, there appears to be a Re above which a big increase in the convective heat transfer coefficient occurs. Such a big increase seems to correspond to the shear thinning behaviour. For nanofluids containing 0.5 wt.% CNTs, the maximum enhancement reaches over 350% at Re = 800, which could not be attributed purely to the enhanced thermal conduction. Particle re-arrangement, shear induced thermal conduction enhancement, reduction of thermal boundary due to the presence of nanoparticles, as well as the very high aspect ratio of CNTs are proposed to be possible mechanisms. (c) 2005 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
66,"Open innovation has so far been studied mainly in high-tech, Multinational enterprises. This exploratory paper investigates if open innovation practices are also applied by small- and medium-sized enterprises (SMEs). Drawing oil I database collected from 605 innovative SMEs in the Netherlands, we explore the incidence of and apparent trend towards open innovation. The survey furthermore focuses oil the motives and perceived challenges when SMEs adopt open innovation practices. Within the survey, open innovation is measured with eight innovation practices reflecting technology exploration and exploitation in SMEs. We find that the responding SMEs engage in many open innovation practices and have increasingly adopted such practices during the past 7 years. In addition. we find no major differences between manufacturing and services industries, but medium-sized firms are oil average more heavily involved in open innovation than their smaller counterparts. We furthermore find that SMEs Pursue open innovation primarily for market-related motives such as meeting customer demands, or keeping LIP with competitors. Their most important challenges relate to organizational and Cultural issues as a consequence of dealing with increased external contacts. (C) 2008 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
67,"This study evaluated cellular uptake of polymeric nanoparticles by using Caco-2 cells, a human colon adenocarcinoma cell line, as an in vitro model with the aim to apply nanoparticles of biodegradable polymers for oral chemotherapy. The feasibility was demonstrated by showing the localization and quantification of the cell uptake of fluorescent polystyrene nanoparticles of standard size and poly(lactic-co-glycolic acid) (PLGA) nanoparticles coated with polyvinyl alcohol (PVA) or vitamin E TPGS. Coumarin-6 loaded PLGA nanoparticles were prepared by a modified solvent extraction/evaporation method and characterized by laser light scattering for size and size distribution, scanning electron microscopy (SEM) for surface morphology, zeta-potential for surface charge, and spectrofluorometry for fluorescent molecule release from the nanoparticles. The effects of particle size and particle surface coating on the cellular uptake of the nanoparticles were quantified by spectrofluorometric measurement. Cellular uptake of vitamin E TPGS-coated PLGA nanoparticles showed 1.4 folds higher than that of PVA-coated PLGA nanoparticles and 4-6 folds higher than that of nude polystyrene nanoparticles. Images of confocal laser scanning microscopy, cryo-SEM and transmission electron microscopy clearly evidenced the internalization of nanoparticles by the Caco-2 cells, showing that surface modification of PLGA nanoparticles with vitamin E TPGS notably improved the cellular uptake. It is highly feasible for nanoparticles of biodegradable polymers to be applied to promote oral chemotherapy. (C) 2004 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68,"A number of current face recognition algorithms use face representations found by unsupervised statistical methods. Typically these methods find a set of basis images and represent faces as a linear combination of those images. Principal component analysis (PCA) is a popular example of such methods. The basis images found by PCA depend only on pairwise relationships between pixels in the image database. In a task such as face recognition, in which important information may be contained in the high-order relationships among pixels, it seems reasonable to expect that better basis images may be found by methods sensitive to. these high-order statistics. Independent component analysis (ICA), a generalization of PCA, is one such method. We used a version of ICA derived from the principle of optimal information transfer through sigmoidal neurons. ICA was performed on face images in the FERET database under two different architectures, one which treated the images as random variables and the pixels as outcomes, and a second which treated the pixels as random variables and the images as outcomes. The first architecture found spatially local basis images for the faces. The second architecture produced a factorial face code. Both ICA representations were superior to representations based on PCA for recognizing faces across days and changes in expression. A classifier that combined the two ICA representations gave the best performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69,"This paper represents an ongoing investigation of dexterous and natural control of upper extremity prostheses using the myoelectric signal (MES). The scheme described within uses pattern recognition to process four channels of MES, with the task of discriminating multiple classes of limb movement. The method does not require segmentation of the MES data, allowing a continuous stream of class decisions to be delivered to a prosthetic device. It is shown in this paper that, by exploiting the processing power inherent in current computing systems, substantial gains in classifier accuracy and, response time are possible. Other important characteristics for prosthetic control systems are met as well. Due to the fact that the classifier learns the muscle activation patterns for each desired class for each individual, a natural control actuation results. The continuous decision stream allows complex sequences of manipulation involving multiple joints to be performed without interruption. Finally, minimal storage capacity is required, which is an important factor in embedded control systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
70,"A modular multilevel converter (MMC) is one of the next-generation multilevel converters intended for high- or medium-voltage power conversion without transformers. The MMC is based on cascade connection of multiple bidirectional chopper-cells per leg, thus requiring voltage-balancing control of the multiple floating dc capacitors. However, no paper has made an explicit discussion on voltage-balancing control with theoretical and experimental verifications. This paper deals with two types of pulsewidth-modulated modular multilevel converters (PWM-MMCs) with focus on their circuit configurations and voltage-balancing control. Combination of averaging and balancing controls enables the PWM-MMCs to achieve voltage balancing without any external circuit. The viability of the PWM-MMCs, as well as the effectiveness of the voltage-balancing control, is confirmed by simulation and experiment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
71,"In this study, a new metaheuristic optimization algorithm, called cuckoo search (CS), is introduced for solving structural optimization tasks. The new CS algorithm in combination with L,vy flights is first verified using a benchmark nonlinear constrained optimization problem. For the validation against structural engineering optimization problems, CS is subsequently applied to 13 design problems reported in the specialized literature. The performance of the CS algorithm is further compared with various algorithms representative of the state of the art in the area. The optimal solutions obtained by CS are mostly far better than the best solutions obtained by the existing methods. The unique search features used in CS and the implications for future research are finally discussed in detail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
72,"This paper studies properties of homogeneous systems in a geometric, coordinate-free setting. A key contribution of this paper is a result relating regularity properties of a homogeneous function to its degree of homogeneity and the local behavior of the dilation near the origin. This result makes it possible to extend previous results on homogeneous systems to the geometric framework. As an application of our results, we consider finite-time stability of homogeneous systems. The main result that links homogeneity and finite-time stability is that a homogeneous system is finite-time stable if and only if it is asymptotically stable and has a negative degree of homogeneity. We also show that the assumption of homogeneity leads to stronger properties for finite-time stable systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
73,"Together with an explosive growth of the mobile applications and emerging of cloud computing concept, mobile cloud computing (MCC) has been introduced to be a potential technology for mobile services. MCC integrates the cloud computing into the mobile environment and overcomes obstacles related to the performance (e.g., battery life, storage, and bandwidth), environment (e.g., heterogeneity, scalability, and availability), and security (e.g., reliability and privacy) discussed in mobile computing. This paper gives a survey of MCC, which helps general readers have an overview of the MCC including the definition, architecture, and applications. The issues, existing solutions, and approaches are presented. In addition, the future research directions of MCC are discussed. Copyright (c) 2011 John Wiley & Sons, Ltd.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74,"We propose a new type of saliency-context-aware saliency-which aims at detecting the image regions that represent the scene. This definition differs from previous definitions whose goal is to either identify fixation points or detect the dominant object. In accordance with our saliency definition, we present a detection algorithm which is based on four principles observed in the psychological literature. The benefits of the proposed approach are evaluated in two applications where the context of the dominant objects is just as essential as the objects themselves. In image retargeting, we demonstrate that using our saliency prevents distortions in the important regions. In summarization, we show that our saliency helps to produce compact, appealing, and informative summaries.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75,"In this paper, we propose and validate a novel design for a double-gate tunnel field-effect transistor (DG Tunnel FET), for which the simulations show significant improvements compared with single-gate devices using an SiO2 gate dielectric. For the first time, DG Tunnel FET devices, which are using a high-kappa gate dielectric, are explored using realistic design parameters, showing an ON-current as high as 0.23 mA for a gate voltage of 1.8 V, an OFF-current of less than 1 fA (neglecting gate leakage), an improved average subthreshold swing of 57 mV/dec, and a minimum point slope of 11 mV/dec. The 2-D nature of Tunnel FET current flow is studied, demonstrating that the current is not confined to a channel at the gate-dielectric surface. When varying temperature, Tunnel FETs with a high-kappa gate dielectric have a smaller threshold voltage shift than those Using SiO2, while the subthreshold slope for fixed values of V-g remains nearly unchanged, in contrast with the traditional MOSFET. Moreover, an I-on/I-off ratio of more than 2 x 10(11) is shown for simulated devices with a gate length (over the intrinsic region) of 50 nm, which indicates that the Tunnel FET is a promising candidate to achieve better-than-ITRS low-standby-power switch performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76,"Millimeter-wave imaging techniques and systems have been developed at the Pacific Northwest National Laboratory (PNNL), Richland, WA, for the detection of concealed weapons and contraband at airports and other secure locations. These techniques were derived from microwave holography techniques that utilize phase and amplitude information recorded over a two-dimensional aperture to reconstruct a focused image of the target. Millimeter-wave imaging is well suited for the detection of concealed weapons or other contraband carried on personnel since millimeter-waves are nonionizing, readily penetrate common clothing material, and are reflected from the human body and any concealed items. In this paper, a wide-bandwidth three-dimensional holographic microwave imaging technique is described. Practical weapon detection systems for airport or other high-throughput applications require high-speed scanning on the order of 3 to 10 s. To achieve this goal, a prototype imaging system utilizing a 27-33 GHz linear sequentially switched array and a high-speed linear scanner has been developed and tested. This system is described in detail along with numerous imaging results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77,"Carbon dioxide capture from power plant flue gas and subsequent sequestration is expected to play a key role in mitigating global climate change. Conventional amine technologies being considered for separating CO2 from flue gas are costly, energy intensive, and if implemented, would result in large increases in the cost of producing electricity. Membranes offer potential as an energy-efficient, low-cost CO2 capture option. Recently, working with the U.S. Department of Energy (DOE), we have developed membranes with CO2 permeances of greater than 1000 gpu and a CO2/N-2 selectivity of 50 at 30 degrees C. This permeance is ten times higher than commercial CO2 membranes and the selectivity is among the highest reported for non-facilitated transport materials. These membranes, in combination with a novel process design that uses incoming combustion air as a sweep gas to generate driving force, could meet DOE CO2 capture cost targets. Under these conditions, improving membrane permeance is more important than increasing selectivity to further reduce the cost of CO2 capture from flue gas. Membrane cost and reliability issues will be key to the eventual competitiveness of this technology for flue gas treatment. (C) 2009 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78,"Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work addresses these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques, and application scenarios.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79,"To ensure manufacturability and mesh independence in density-based topology optimization schemes, it is imperative to use restriction methods. This paper introduces a new class of morphology-based restriction schemes that work as density filters; that is, the physical stiffness of an element is based on a function of the design variables of the neighboring elements. The new filters have the advantage that they eliminate grey scale transitions between solid and void regions. Using different test examples, it is shown that the schemes, in general, provide black and white designs with minimum length-scale constraints on either or both minimum hole sizes and minimum structural feature sizes. The new schemes are compared with methods and modified methods found in the literature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
80,"Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,"Self-configuration in wireless sensor networks is a general class of estimation problems that we study via the Cramer-Rao bound (CRB). Specifically, we consider sensor location estimation when sensors measure received signal strength (RSS) or time-of-arrival (TOA) between themselves and neighboring sensors. A small fraction of sensors in the network have a known location, whereas the remaining locations must be estimated. We derive CRBs and maximum-likelihood estimators (MLEs) under Gaussian and log-normal models for the TOA and RSS measurements, respectively. An extensive TOA and RSS measurement campaign in an indoor office area illustrates MLE performance. Finally, relative location estimation algorithms are implemented in a wireless sensor network testbed and deployed in indoor and outdoor environments. The measurements and testbed experiments demonstrate 1-m RMS location errors using TOA, and 1- to 2-m RMS location errors using RSS.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,"The capacity of ad hoe wireless networks is constrained by the mutual interference of concurrent transmissions between nodes. We study A model of an ad hoe network where n nodes communicate in random, source-destination pairs. These nodes are assumed to be mobile. We examine the per-session throughput for applications with loose delay constraints,. such that the topology changes over the time-scale of packet delivery. Under this assumption, the per-user throughput can increase dramatically when nodes. are mobile rather than fixed. This improvement can be achieved by exploiting a form of multiuser diversity via packet relaying.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,"Cloud computing is changing the way industries and enterprises do their businesses in that dynamically scalable and virtualized resources are provided as a service over the Internet. This model creates a brand new opportunity for enterprises. In this paper, some of the essential features of cloud computing are briefly discussed with regard to the end-users, enterprises that use the cloud as a platform, and cloud providers themselves. Cloud computing is emerging as one of the major enablers for the manufacturing industry; it can transform the traditional manufacturing business model, help it to align product innovation with business strategy, and create intelligent factory networks that encourage effective collaboration. Two types of cloud computing adoptions in the manufacturing sector have been suggested, manufacturing with direct adoption of cloud computing technologies and cloud manufacturing the manufacturing version of cloud computing. Cloud computing has been in some of key areas of manufacturing such as IT, pay-as-you-go business models, production scaling up and down per demand, and flexibility in deploying and customizing solutions. In cloud manufacturing, distributed resources are encapsulated into cloud services and managed in a centralized way. Clients can use cloud services according to their requirements. Cloud users can request services ranging from product design, manufacturing, testing, management, and all other stages of a product life cycle. (C) 2011 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84,"The size and shape of soil particles reflect the formation history of the grains. In turn, the macroscale behavior of the soil mass results from particle level interactions which are affected by particle shape. Sphericity, roundness, and smoothness characterize different scales associated with particle shape. New experimental data and results from published studies are gathered into two databases to explore the effects of particle shape on packing density and on the small-to-large strain mechanical properties of sandy soils. In agreement with previous studies, these data confirm that increased angularity or eccentricity produces an increase in e(max) and e(min). Furthermore, the data show that increasing particle irregularity causes a decrease in stiffness yet heightened sensitivity to the state of stress, an increase in compressibility under zero-lateral strain loading; an increase in the critical state friction angle 4,,; and an increase in the interceptor of the critical state line (there is a weak effect on the slope X). Therefore, particle shape emerges as a significant soil index property that needs to be properly characterized and documented, particularly in clean sands and gravels. The systematic assessment of particle shape will lead to a better understanding of sand behavior.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,"The global challenge of meeting increased food demand and protecting environmental quality will be won or lost in cropping systems that produce maize, rice, and wheat . Achieving synchrony between N supply and crop demand without excess or deficiency is the key to optimizing trade-offs amongst yield, profit, and environmental protection in both large-scale systems in developed countries and small-scale systems in developing countries. Setting the research agenda and developing effective policies to meet this challenge requires quantitative understanding of current levels of N-use efficiency and losses in these systems, the biophysical controls on these factors, and the economic returns from adoption of improved management practices. Although advances in basic biology, ecology, and biogeochemistry can provide answers, the magnitude of the scientific challenge should not be underestimated because it becomes increasingly difficult to control the fate of N in cropping systems that must sustain yield increases on the world's limited supply of productive farm land.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86,"The 2018 edition of the United Nations World Water Development Report stated that nearly 6 billion peoples will suffer from clean water scarcity by 2050. This is the result of increasing demand for water, reduction of water resources, and increasing pollution of water, driven by dramatic population and economic growth. It is suggested that this number may be an underestimation, and scarcity of clean water by 2050 may be worse as the effects of the three drivers of water scarcity, as well as of unequal growth, accessibility and needs, are underrated. While the report promotes the spontaneous adoption of nature-based-solutions within an unconstrained population and economic expansion, there is an urgent need to regulate demography and economy, while enforcing clear rules to limit pollution, preserve aquifers and save water, equally applying everywhere. The aim of this paper is to highlight the inter-linkage in between population and economic growth and water demand, resources and pollution, that ultimately drive water scarcity, and the relevance of these aspects in local, rather than global, perspective, with a view to stimulating debate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87,"Recently, increasing attention has been directed to the study of the emotional content of speech signals, and hence, many systems have been proposed to identify the emotional content of a spoken utterance. This paper is a survey of speech emotion classification addressing three important aspects of the design of a speech emotion recognition system. The first one is the choice of suitable features for speech representation. The second issue is the design of an appropriate classification scheme and the third issue is the proper preparation of an emotional speech database for evaluating system performance. Conclusions about the performance and limitations of current speech emotion recognition systems are discussed in the last section of this survey. This section also suggests possible ways of improving speech emotion recognition systems. (C) 2010 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
88,"Acoustic propagation is characterized by three major factors: attenuation that increases with signal frequency, time-varying multipath propagation, and low speed of sound (1500 m/s). The background noise, although often characterized as Gaussian, is not white, but has a decaying power spectral density. The channel capacity depends on the distance, and may be extremely limited. Because acoustic propagation is best supported at low frequencies, although the total available bandwidth may be low, an acoustic communication system is inherently wideband in the sense that the bandwidth is not negligible with respect to its center frequency. The channel can have a sparse impulse response, where each physical path acts as a time-varying low-pass filter, and motion introduces additional Doppler spreading and shifting. Surface waves, internal turbulence, fluctuations in the sound speed, and other small-scale phenomena contribute to random signal variations. At this time, there are no standardized models for the acoustic channel fading, and experimental measurements are often made to assess the statistical properties of the channel in particular deployment sites.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
89,"This letter addresses the problem of energy detection of an unknown signal over a multipath channel. It starts with the no-diversity case, and presents some alternative closed-form expressions for the probability of detection to those recently reported in the literature. Detection capability is boosted by implementing both square-law combining and square-law selection diversity schemes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90,"In this paper, we study the salient object detection problem for images. We formulate this problem as a binary labeling task where we separate the salient object from the background. We propose a set of novel features, including multiscale contrast, center-surround histogram, and color spatial distribution, to describe a salient object locally, regionally, and globally. A conditional random field is learned to effectively combine these features for salient object detection. Further, we extend the proposed approach to detect a salient object from sequential images by introducing the dynamic salient features. We collected a large image database containing tens of thousands of carefully labeled images by multiple users and a video segment database, and conducted a set of experiments over them to demonstrate the effectiveness of the proposed approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,"The penetration of renewable sources (particularly wind power) in to the power system network has been increasing in the recent years. As a result of this, there have been serious concerns over reliable and satisfactory operation of the power systems. One of the solutions being proposed to improve the reliability and performance of these systems is to integrate energy storage devices into the power system network. Further, in the present deregulated markets these storage devices could also be used to increase the profit margins of wind farm owners and even provide arbitrage. This paper discusses the present status of battery energy storage technology and methods of assessing their economic viability and impact on power system operation. Further, a discussion on the role of battery storage systems of electric hybrid vehicles in power system storage technologies had been made. Finally, the paper suggests a likely future outlook for the battery technologies and the electric hybrid vehicles in the context of power system applications. (C) 2008 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92,"Multivariate statistical techniques, such as cluster analysis (CA), principal component analysis (PCA), factor analysis (FA) and discriminant analysis (DA), were applied for the evaluation of temporal/spatial variations and the interpretation of a large complex water quality data set of the Fuji river basin, generated during 8 years (1995-2002) monitoring of 12 parameters at 13 different sites (14 976 observations). Hierarchical cluster analysis grouped 13 sampling sites into three clusters, i.e., relatively less polluted (LP), medium polluted (MP) and highly polluted (HP) sites, based on the similarity of water quality characteristics. Factor analysis/principal component analysis, applied to the data sets of the three different groups obtained from cluster analysis, resulted in five, five and three latent factors explaining 73.18, 77.61 and 65.39% of the total variance in water quality data sets of LP, MP and HP areas, respectively. The varifactors obtained from factor analysis indicate that the param eters responsible for water quality variations are mainly related to discharge and temperature (natural), organic pollution (point source: domestic wastewater) in relatively less polluted areas; organic pollution (point source: domestic wastewater) and nutrients (non-point sources: agriculture and orchard plantations) in medium polluted areas; and organic pollution and nutrients (point sources: domestic wastewater, wastewater treatment plants and industries) in highly polluted areas in the basin. Discriminant analysis gave the best results for both spatial and temporal analysis. It provided an important data reduction as it uses only six parameters (discharge, temperature, dissolved oxygen, biochemical oxygen demand, electrical conductivity and nitrate nitrogen), affording more than 85% correct assignations in temporal analysis, and seven parameters (discharge, temperature, biochemical oxygen demand, pH, electrical conductivity, nitrate nitrogen and ammonical nitrogen), affording more than 81% correct assignations in spatial analysis, of three different sampling sites of the basin. Therefore, DA allowed a reduction in the dimensionality of the large data set, delineating a few indicator parameters responsible for large variations in water quality. Thus, this study illustrates the usefulness of multivariate statistical techniques for analysis and interpretation of complex data sets, and in water quality assessment, identification of pollution sources/factors and understanding temporal/spatial variations in water quality for effective river water quality management. (c) 2006 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,"Calculations of the impact of friction and wear on energy consumption, economic expenditure, and CO2 emissions are presented on a global scale. This impact study covers the four main energy consuming sectors: transportation, manufacturing, power generation, and residential. Previously published four case studies on passenger cars, trucks and buses, paper machines and the mining industry were included in our detailed calculations as reference data in our current analyses. The following can be concluded: In total, similar to 23% (119 EJ) of the world's total energy consumption originates from tribological contacts. Of that 20% (103 EJ) is used to overcome friction and 3% (16 EJ) is used to remanufacture worn parts and spare equipment due to wear and wear-related failures. By taking advantage of the new surface, materials, and lubrication technologies for friction reduction and wear protection in vehicles, machinery and other equipment worldwide, energy losses due to friction and wear could potentially be reduced by 40% in the long term (15 years) and by 18% in the short term (8 years). On global scale, these savings would amount to 1.4% of the GDP annually and 8.7% of the total energy consumption in the long term. The largest short term energy savings are envisioned in transportation (25%) and in the power generation (20%) while the potential savings in the manufacturing and residential sectors are estimated to be similar to 10%. In the longer terms, the savings would be 55%, 40%, 25%, and 20%, respectively. Implementing advanced tribological technologies can also reduce the CO2 emissions globally by as much as 1,460 MtCO(2) and result in 450,000 million Euros cost savings in the short term. In the longer term, the reduction can be 3,140 MtCO(2) and the cost savings 970,000 million Euros. Fifty years ago, wear and wear-related failures were a major concern for UK industry and their mitigation was considered to be the major contributor to potential economic savings by as much as 95% in ten years by the development and deployment of new tribological solutions. The corresponding estimated savings are today still of the same orders but the calculated contribution to cost reduction is about 74% by friction reduction and to 26% from better wear protection. Overall, wear appears to be more critical than friction as it may result in catastrophic failures and operational breakdowns that can adversely impact productivity and hence cost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94,"TanDEMA (TerraSAR-X add-on for Digital Elevation Measurements) is an innovative spaceborne radar interferometer that is based on two TerraSAR-X radar satellites flying in close formation. The primary objective of the TanDEMA mission is the generation of a consistent global digital elevation model (DEM) with an unprecedented accuracy, which is equaling or surpassing the HRTI-3 specification. Beyond that, TanDEM-X provides a highly reconfigurable platform for the demonstration of new radar imaging techniques and applications. This paper gives a detailed overview of the TanDEM-X mission concept which is based on the systematic combination of several innovative technologies. The key elements are the bistatic data acquisition employing an innovative phase synchronization link, a novel satellite formation flying concept allowing for the collection of bistatic data with short along-track baselines, as well as the use of new interferometric modes for system verification and DEM calibration. The interferometric performance is analyzed in detail, taking into account the peculiarities of the bistatic operation. Based on this analysis, an optimized DEM data acquisition plan is derived which employs the combination of multiple data takes with different baselines. Finally, a collection of instructive examples illustrates the capabilities of TanDEM-X for the development and demonstration of new remote sensing applications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
95,"As one of the most important chemicals and carbon-free energy carriers, ammonia (NH3) has a worldwide annual production of similar to 150 million tons, and is mainly produced by the traditional high-temperature and high-pressure Haber-Bosch process which consumes massive amounts of energy. Very recently, electrocatalytic and photo(electro) catalytic reduction of N-2 to NH3, which can be performed at ambient conditions using renewable energy, have received tremendous attention. The overall performance of these electrocatalytic and photo(electro) catalytic systems is largely dictated by their core components, catalysts. This perspective for the first time highlights the rational design of electrocatalysts and photo(electro) catalysts for N-2 reduction to NH3 under ambient conditions. Fundamental theory of catalytic reaction pathways for the N-2 reduction reaction and the corresponding material design principles are introduced first. Then, recently developed electrocatalysts and photo(electro) catalysts are summarized, with a special emphasis on the relationship between their physicochemical properties and NH3 production performance. Finally, the opportunities in this emerging research field, in particular, the strategy of combining experimental and theoretical techniques to design efficient and stable catalysts for NH3 production, are outlined.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96,"Cellular networks are in a major transition from a carefully planned set of large tower-mounted base-stations (BSs) to an irregular deployment of heterogeneous infrastructure elements that often additionally includes micro, pico, and femtocells, as well as distributed antennas. In this paper, we develop a tractable, flexible, and accurate model for a downlink heterogeneous cellular network (HCN) consisting of K tiers of randomly located BSs, where each tier may differ in terms of average transmit power, supported data rate and BS density. Assuming a mobile user connects to the strongest candidate BS, the resulting Signal-to-Interference-plus-Noise-Ratio (SINR) is greater than 1 when in coverage, Rayleigh fading, we derive an expression for the probability of coverage (equivalently outage) over the entire network under both open and closed access, which assumes a strikingly simple closed-form in the high SINR regime and is accurate down to -4 dB even under weaker assumptions. For external validation, we compare against an actual LTE network (for tier 1) with the other K - 1 tiers being modeled as independent Poisson Point Processes. In this case as well, our model is accurate to within 1-2 dB. We also derive the average rate achieved by a randomly located mobile and the average load on each tier of BSs. One interesting observation for interference-limited open access networks is that at a given SINR, adding more tiers and/or BSs neither increases nor decreases the probability of coverage or outage when all the tiers have the same target-SINR.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97,"Nano zerovalent iron (nZVI) is an effective adsorbent for removing various organic and inorganic contaminants. In this study. nZVI particles were used to investigate the removal of Cd2+ in the concentration ;range of 25-450 mgL(-1). The effect of temperature on kinetics and equilibrium of cadmium sorption on nZVI particles was thoroughly examined. Consistent with an endothermic reaction, an increase in the temperature resulted in increasing cadmium adsorption rate. The adsorption kinetics well fitted using a pseudo second-order kinetic model. The calculated activation energy for adsorption was 54.8 kJ mol(-1), indicating the adsorption process to be chemisorption. The intraparticle diffusion model described that the intraparticle diffusion was not the only rate-limiting step. The adsorption isotherm data could be well described by the Langmuir as well as Temkin equations. The maximum adsorption capacity of nZVI for Cd2+ was found to be 769.2 mg g(-1) at 297K. Thermodynamic parameters (i.e., change in the free energy (Delta G degrees), the enthalpy (Delta H degrees), and the entropy (Delta S degrees)) were also evaluated. The overall adsorption process was endothermic and spontaneous in nature. EDX analysis indicated the presence of cadmium ions on the nZVI surface. These results suggest that nZVI could be employed as an efficient adsorbent for the removal of cadmium from contaminated water sources. (c) 2010 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98,"A CFD strategy is proposed that combines delayed detached-eddy simulation (DDES) with an improved RANS-LES hybrid model aimed at wall modelling in LES (WMLES). The system ensures a different response depending on whether the simulation does or does not have inflow turbulent content. In the first case, it reduces to WMLES: most of the turbulence is resolved except near the wall. Empirical improvements to this model relative to the pure DES equations provide a great increase of the resolved turbulence activity near the wall and adjust the resolved logarithmic layer to the modelled one, thus resolving the issue of ""log layer mismatch"" which is common in DES and other WMLES methods. An essential new element here is a definition of the subgrid length-scale which depends not only oil the grid spacings, but also on the wall distance. In the case without inflow turbulent content, the proposed model performs as DDES, i.e., it gives a pure RANS solution for attached flows and a DES-like solution for massively separated flows. The coordination of the two branches is carried out by a blending function. The promise of the model is supported by its satisfactory performance in all the three modes it was designed for, namely, in pure WMLES applications (channel flow in a wide Reynolds-number range and flow over a hydrofoil with trailing-edge separation), in a natural DDES application (an airfoil in deep stall), and in a flow where both branches of the model are active in different flow regions (a backward-facing-step, flow). (C) 2008 Elsevier Inc. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
99,"There are two broad categories of risk affecting supply chain design and management: (1) risks arising from the problems of coordinating supply and demand, and (2) risks arising from disruptions to normal activities. This paper is concerned with the second category of risks, which may arise from natural disasters, from strikes and economic disruptions, and from acts of purposeful agents, including terrorists. The paper provides a conceptual framework that reflects the joint activities of risk assessment and risk mitigation that are fundamental to disruption risk management in supply chains. We then consider empirical results from a rich data set covering the period 1995-2000 on accidents in the U.S. Chemical Industry. Based on these results and other literature, we discuss the implications for the design of management systems intended to cope with supply chain disruption risks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
