ID,Abstract,rule1,rule2,rule3,rule4,rule5,rule6,rule7,rule8,rule9,rule10,rule11,rule12,rule13,rule14,rule15,rule16,rule17,rule18,rule19,rule20,rule21,rule22,rule23,rule24,rule25,rule26,rule27,rule28,rule29,rule30,rule31,rule32,rule33
0,"In this paper, the idea of operating an inverter to mimic a synchronous generator (SG) is motivated and developed. We call the inverters that are operated in this way synchronverters. Using synchronverters, the well-established theory/algorithms used to control SGs can still be used in power systems where a significant proportion of the generating capacity is inverter-based. We describe the dynamics, implementation, and operation of synchronverters. The real and reactive power delivered by synchronverters connected in parallel and operated as generators can be automatically shared using the well-known frequency- and voltage-drooping mechanisms. Synchronverters can be easily operated also in island mode, and hence, they provide an ideal solution for microgrids or smart grids. Both simulation and experimental results are given to verify the idea.",yes,no,no,no,yes,no,yes,yes,no,no,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,yes,no,yes,yes,yes,yes,yes,yes,yes,yes,yes
1,"The first part of this paper proposes an adaptive, data-driven threshold for image denoising via wavelet soft-thresholding, The threshold is derived in a Bayesian framework, and the prior used on the wavelet coefficients is the generalized Gaussian distribution (GGD) widely used in image processing applications, The proposed threshold is simple and closed-form, and it is adaptive to each subband because it depends on data-driven estimates of the parameters. Experimental results show that the proposed method, called BayesShrink, is typically within 5% of the MSE of the best soft-thresholding benchmark with the image assumed known, It also outperforms Donoho and Johnstone's SureShrink most of the time. The second part of the paper attempts to further validate recent claims that lossy compression can be used for denoising, The BayesShrink threshold can aid in the parameter selection of a coder designed with the intention of denoising, and thus achieving simultaneous denoising and compression. Specifically, the zero-zone in the quantization step of compression is analogous to the threshold value in the thresholding function. The remaining coder design parameters are chosen based on a criterion derived from Rissanen's minimum description length (MDL) principle, Experiments show that this compression method does indeed remove noise significantly, especially for large noise power, However, it introduces quantization noise and should he used only if bitrate were an additional concern to denoising.",no,happy,happy,no,yes,no,yes,happy,happy,no,yes,yes,no,no,no,yes,yes,yes,yes,yes,yes,no,yes,yes,yes,yes,yes,yes,yes,no,yes,yes,yes
2,"Matrix factorization techniques have been frequently applied in information retrieval, computer vision, and pattern recognition. Among them, Nonnegative Matrix Factorization (NMF) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts based in the human brain. On the other hand, from the geometric perspective, the data is usually sampled from a low-dimensional manifold embedded in a high-dimensional ambient space. One then hopes to find a compact representation, which uncovers the hidden semantics and simultaneously respects the intrinsic geometric structure. In this paper, we propose a novel algorithm, called Graph Regularized Nonnegative Matrix Factorization (GNMF), for this purpose. In GNMF, an affinity graph is constructed to encode the geometrical information and we seek a matrix factorization, which respects the graph structure. Our empirical study shows encouraging results of the proposed algorithm in comparison to the state-of-the-art algorithms on real-world problems.",yes,no,no,yes,no,no,no,no,no,no,yes,yes,no,no,no,no,yes,yes,no,yes,no,no,yes,no,yes,yes,no,yes,yes,no,yes,yes,yes
3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4,"Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.",yes,no,no,yes,yes,no,yes,yes,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,yes,yes,yes,yes,yes,yes,yes,yes,yes
5,"Historically the function of biomaterials has been to replace diseased or damaged tissues. First generation biomaterials were selected to be as bio-inert as possible and thereby minimize formation of scar tissue at the interface with host tissues. Bioactive glasses were discovered in 1969 and provided for the first time an alternative; second generation, interfacial bonding of an implant with host tissues. Tissue regeneration and repair using the gene activation properties of Bioglass(R) provide a third generation of biomaterials. This article reviews the 40 year history of the development of bioactive glasses, with emphasis on the first composition, 45S5 Bioglass(R), that has been in clinical use since 1985. The steps of discovery, characterization, in vivo and in vitro evaluation, clinical studies and product development are summarized along with the technology transfer processes.",yes,no,no,yes,no,no,yes,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,no,no,yes,no,yes,yes,yes,no,yes,no,yes,yes,yes,yes
6,"Dynamic texture (DT) is an extension of texture to the temporal domain. Description and recognition of DTs have attracted growing attention. In this paper, a novel approach for recognizing DTs is proposed and its simplifications and extensions to facial image analysis are also considered. First, the textures are modeled with volume local binary patterns (VLBP), which are an extension of the LBP operator widely used in ordinary texture analysis, combining motion and appearance. To make the approach computationally simple and easy to extend, only the co-occurrences of the local binary patterns on three orthogonal planes (LBP-TOP) are then considered. A block-based method is also proposed to deal with specific dynamic events such as facial expressions in which local information and its spatial locations should also be taken into account. In experiments with two DT databases, DynTex and Massachusetts Institute of Technology (MIT), both the VLBP and LBP-TOP clearly outperformed the earlier approaches. The proposed block-based method was evaluated with the Cohn-Kanade facial expression database with excellent results. The advantages of our approach include local processing, robustness to monotonic gray-scale changes, and simple computation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7,"Arsenic derived from natural sources occurs in groundwater in many countries, affecting the health of millions of people. The combined effects of As(V) reduction and diagenesis of iron oxide minerals on arsenic mobility are investigated in this study by comparing As(V) and As(III) sorption onto amorphous iron oxide (HFO), goethite, and magnetite at varying solution compositions. Experimental data are modeled with a diffuse double layer surface complexation model, and the extracted model parameters are used to examine the consistency of our results with those previously reported. Sorption of As(V) onto HFO and goethite is more favorable than that of As(III) below pH 5-6, whereas, above pH 7-8, As(III) has a higher affinity for the solids. The pH at which As(V) and As(III) are equally sorbed depends on the solid-to-solution ratio and type and specific surface area of the minerals and is shifted to lower pH values in the presence of phosphate, which competes for sorption sites. The sorption data indicate that, under most of the chemical conditions investigated in this study, reduction of As(V) in the presence of HFO or goethite would have only minor effects on or even decrease its mobility in the environment at near-neutral pH conditions. As(V) and As(III) sorption isotherms indicate similar surface site densities on the three oxides. Intrinsic surface complexation constants for As(V) are higher for goethite than HFO, whereas As(III) binding is similar for both of these oxides and also for magnetite. However, decrease in specific surface area and hence sorption site density that accompanies transformation of amorphous iron oxides to more crystalline phases could increase arsenic mobility.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8,"We present a source localization method based on a sparse representation of sensor measurements with an overcomplete basis composed of samples from the array manifold. We enforce sparsity by imposing penalties based on the l(1)-norm. A number of recent theoretical results on sparsifying properties of l(1) penalties justify this choice. Explicitly enforcing the sparsity of the representation is motivated by a desire to obtain a sharp estimate of the spatial spectrum that exhibits super-resolution. We propose to use the singular value decomposition (SVD) of the data matrix to summarize multiple time or frequency samples. Our formulation leads to an optimization problem, which we solve efficiently in a second-order cone (SOC) programming framework by an interior point implementation. We propose a grid refinement method to mitigate the effects of limiting estimates to a grid of spatial locations and introduce an automatic selection criterion for the regularization parameter involved in our approach. We demonstrate the effectiveness of the method on simulated data by plots of spatial spectra and by comparing the estimator variance to the Cramar-Rao bound (CRB). We observe that our approach has a number of advantages over other source localization techniques, including increased resolution, improved robustness to noise, limitations in data quantity, and correlation of the sources, as well as not requiring an accurate initialization.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9,"Soil hydraulic properties are necessary for many studies of water and solute transport but often cannot be measured because of practical and/or financial constraints. We describe a computer program, ROSETTA, which implements five hierarchical pedotransfer functions (PTFs) for the estimation of water retention, and the saturated and unsaturated hydraulic conductivity. The hierarchy in PTFs allows the estimation of van Genuchten water retention parameters and the saturated hydraulic conductivity using limited (textural classes only) to more extended (texture, bulk density, and one or two water retention points) input data. ROSETTA is based on neural network analyses combined with the bootstrap method, thus allowing the program to provide uncertainty estimates of the predicted hydraulic parameters. The general performance Of ROSETTA was characterized with coefficients of determination, and root mean square errors (RMSEs). The RMSE values decreased from 0.078 to 0.044 cm(3) cm(-3) for water retention when more predictors were used. The RMSE for the saturated conductivity similarly decreased from 0.739 to 0.647 (dimensionless log(10) units). The RMSE values for unsaturated conductivity ranged between 0.79 and 1.06, depending on whether measured or estimated retention parameters were used as predictors. Calculated mean errors showed that the PTFs underestimated water retention and the unsaturated hydraulic conductivity at relatively high suctions. ROSETTA's uncertainty estimates can be used as an indication of model reliability when no hydraulic data are available. The ROSETTA program comes with a graphical user interface that allows user-friendly access to the PTFs, and can be downloaded from the US Salinity Laboratory website: http://www.ussl.ars.usda.gov/. (C) 2001 Elsevier Science B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10,"The chemical, physical, and mechanical characteristics of nickel-based superalloys are reviewed with emphasis on the use of this class of materials within turbine engines. The role of major and minor alloying additions in multicomponent commercial cast and wrought superalloys is discussed. Microstructural stability and phases observed during processing and in subsequent elevated-temperature service are summarized. Processing paths and recent advances in processing are addressed. Mechanical properties and deformation mechanisms are reviewed, including tensile properties, creep, fatigue, and cyclic crack growth.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11,"In this paper, we present a fully automatic brain tumor segmentation method based on Deep Neural Networks (DNNs). The proposed networks are tailored to glioblastomas (both low and high grade) pictured in MR images. By their very nature, these tumors can appear anywhere in the brain and have almost any kind of shape, size, and contrast. These reasons motivate our exploration of a machine learning solution that exploits a flexible, high capacity DNN while being extremely efficient. Here, we give a description of different model choices that we've found to be necessary for obtaining competitive performance. We explore in particular different architectures based on Convolutional Neural Networks (CNN), i.e. DNNs specifically adapted to image data. We present a novel CNN architecture which differs from those traditionally used in computer vision. Our CNN exploits both local features as well as more global contextual features simultaneously. Also, different from most traditional uses of CNNs, our networks use a final layer that is a convolutional implementation of a fully connected layer which allows a 40 fold speed up. We also describe a 2-phase training procedure that allows us to tackle difficulties related to the imbalance of tumor labels. Finally, we explore a cascade architecture in which the output of a basic CNN is treated as an additional source of information for a subsequent CNN. Results reported on the 2013 BRATS test data-set reveal that our architecture improves over the currently published state-of-the-art while being over 30 times faster. (C) 2016 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12,"Alternative vehicles, such as plug-in hybrid electric vehicles, are becoming more popular. The batteries of these plug-in hybrid electric vehicles are to be charged at home from a standard outlet or on a corporate car park. These extra electrical loads have an impact on the distribution grid which is analyzed in terms of power losses and voltage deviations. Without coordination of the charging, the vehicles are charged instantaneously when they are plugged in or after a fixed start delay. This uncoordinated power consumption on a local scale can lead to grid problems. Therefore, coordinated charging is proposed to minimize the power losses and to maximize the main grid load factor. The optimal charging profile of the plug-in hybrid electric vehicles is computed by minimizing the power losses. As the exact forecasting of household loads is not possible, stochastic programming is introduced. Two main techniques are analyzed: quadratic and dynamic programming.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13,"With the continuous increase in complexity and expense of industrial systems, there is less tolerance for performance degradation, productivity decrease, and safety hazards, which greatly necessitates to detect and identify any kinds of potential abnormalities and faults as early as possible and implement real-time fault-tolerant operation for minimizing performance degradation and avoiding dangerous situations. During the last four decades, fruitful results have been reported about fault diagnosis and fault-tolerant control methods and their applications in a variety of engineering systems. The three-part survey paper aims to give a comprehensive review of real-time fault diagnosis and fault-tolerant control, with particular attention on the results reported in the last decade. In this paper, fault diagnosis approaches and their applications are comprehensively reviewed from model-and signal-based perspectives, respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14,"This is the second-part paper of the survey on fault diagnosis and fault-tolerant techniques, where fault diagnosis methods and applications are overviewed, respectively, from the knowledge-based and hybrid/active viewpoints. With the aid of the first-part survey paper, the second-part review paper completes a whole overview on fault diagnosis techniques and their applications. Comments on the advantages and constraints of various diagnosis techniques, including model-based, signal-based, knowledge-based, and hybrid/active diagnosis techniques, are also given. An overlook on the future development of fault diagnosis is presented.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15,"Wide-band-gap oxides such as SrTiO3 are shown to be critical tests of theories of Schottky barrier heights based on metal-induced gap states and charge neutrality levels. This theory is reviewed and used to calculate the Schottky barrier heights and band offsets for many important high dielectric constant oxides on Pt and Si. Good agreement with experiment is found for barrier heights. The band offsets fur electrons on Si are found to be small for many key oxides such as SrTiO3 and Ta2O5 which limit their utility as Sate oxides in future silicon field effect transistors. The calculations are extended to screen other proposed oxides such as BaZrO3. ZrO2, HfO2, La2O3, Y2O3, HfSiO4, and ZrSiO4. Predictions are also given for barrier heights of the ferroelectric oxides Pb1-xZrxTiO3 and SrBi2Ta2O9 which are used in nonvolatile memories. (C) 2000 American Vacuum Society.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16,"Heat transfer and fluid flow due to buoyancy forces in a partially heated enclosure using nanofluids is carried out using different types of nanoparticles. The flush mounted heater is located to the left vertical wall with a finite length. The temperature of the right vertical wall is lower than that of heater while other walls are insulated. The finite volume technique is used to solve the governing equations. Calculations were performed for Rayleigh number (10(3) <= Ra <= 5 x 10(5)), height of heater (0.1 <= h <= 0.75), location of heater (0.25 <= y(p) <= 0.75), aspect ratio (0.5 <= A <= 2) and volume fraction of nanoparticles (0 <= phi <= 0.2). Different types of nanoparticles were tested. An increase in mean Nusselt number was found with the volume fraction of nanoparticles for the whole range of Rayleigh number. Heat transfer also increases with increasing of height of heater. It was found that the heater location affects the flow and temperature fields when using nanofluids. It was found that the heat transfer enhancement, using nanofluids, is more pronounced at low aspect ratio than at high aspect ratio. (C) 2008 Elsevier Inc. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18,"A unified correlation for computation of higher heating value (HHV) from elemental analysis of fuels is proposed in this paper. This correlation has been derived using 225 data points and validated for additional 50 data points. The entire spectrum of fuels ranging from gaseous, liquid, coals, biomass material, char to residue-derived fuels has been considered in derivation of present correlation. The validity of this correlation has been established for fuels having wide range of elemental composition, i.e. C - 0.00-92.25%, H - 0.43-2,5.15%, 0 - 0.00-50.00%, N - 0.00-5.60%, S - 0.00-94.08% and Ash - 0.00-71.4%. The correlation offers an average absolute error of 1.45% and bias error as 0.00% and thereby establishes its versatility. Complete details of few salient data points, the methodology used for derivation of the correlation and the base assumptions made for derivation are the important constituents of this work. A summary of published correlations along with their basis also forms an important component of present work. (C) 2001 Published by Elsevier Science Ltd.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19,"Spatial modulation (SM) is a recently developed transmission technique that uses multiple antennas. The basic idea is to map a block of information bits to two information carrying units: 1) a symbol that was chosen from a constellation diagram and 2) a unique transmit antenna number that was chosen from a set of transmit antennas. The use of the transmit antenna number as an information-bearing unit increases the overall spectral efficiency by the base-two logarithm of the number of transmit antennas. At the receiver, a maximum receive ratio combining algorithm is used to retrieve the transmitted block of information bits. Here, we apply SM to orthogonal frequency division multiplexing (OFDM) transmission. We develop an analytical approach for symbol error ratio (SER) analysis of the SM algorithm in independent identically distributed (i.i.d.) Rayleigh channels. The analytical and simulation results closely match. The performance and the receiver complexity of the SM-OFDM technique are compared to those of the vertical Bell Labs layered space-time (V-BLAST-OFDM) and Alamouti-OFDM algorithms. V-BLAST uses minimum mean square error (MMSE) detection with ordered successive interference cancellation. The combined effect of spatial correlation, mutual antenna coupling, and Rician fading on both coded and uncoded systems are presented. It is shown that, for the same spectral efficiency, SM results in a reduction of around 90% in receiver complexity as compared to V-BLAST and nearly the same receiver complexity as Alamouti. In addition, we show that SM achieves better performance in all studied channel conditions, as compared with other techniques. It is also shown to efficiently work for any configuration of transmit and receive antennas, even for the case of fewer receive antennas than transmit antennas.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
20,"In the last decade, the Jensen inequality has been intensively used in the context of time-delay or sampled-data systems since it is an appropriate tool to derive tractable stability conditions expressed in terms of linear matrix inequalities (LMIs). However, it is also well-known that this inequality introduces an undesirable conservatism in the stability conditions and looking at the literature, reducing this gap is a relevant issue and always an open problem. In this paper, we propose an alternative inequality based on the Fourier Theory, more precisely on the Wirtinger inequalities. It is shown that this resulting inequality encompasses the Jensen one and also leads to tractable LMI conditions. In order to illustrate the potential gain of employing this new inequality with respect to the Jensen one, two applications on time-delay and sampled-data stability analysis are provided. (C) 2013 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21,"Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320 x 240 image labeling in less than a second, including feature extraction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22,"This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23,"Communication at millimeter wave (mmWave) frequencies is defining a new era of wireless communication. The mmWave band offers higher bandwidth communication channels versus those presently used in commercial wireless systems. The applications of mmWave are immense: wireless local and personal area networks in the unlicensed band, 5G cellular systems, not to mention vehicular area networks, ad hoc networks, and wearables. Signal processing is critical for enabling the next generation of mmWave communication. Due to the use of large antenna arrays at the transmitter and receiver, combined with radio frequency and mixed signal power constraints, new multiple-input multiple-output (MIMO) communication signal processing techniques are needed. Because of the wide bandwidths, low complexity transceiver algorithms become important. There are opportunities to exploit techniques like compressed sensing for channel estimation and beamforming. This article provides an overview of signal processing challenges in mmWave wireless systems, with an emphasis on those faced by using MIMO communication at higher carrier frequencies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24,"Empirical mode decomposition (EMD) has recently been pioneered by Huang et al. for adaptively representing nonstationary signals as sums of zero-mean amplitude modulation frequency modulation components. In-order to better understand the way EMD behaves in stochastic situations involving broadband noise, we report here on numerical experiments based on fractional Gaussian noise. In such a case, it turns out that EMD acts essentially as a dyadic filter bank resembling those involved in wavelet decompositions. It is also pointed out that the hierarchy of the extracted modes may be similarly exploited for getting access to the Hurst exponent.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25,"We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26,"This paper proposes an optimisation algorithm called Grasshopper Optimisation Algorithm (GOA) and applies it to challenging problems in structural optimisation. The proposed algorithm mathematically models and mimics the behaviour of grasshopper swarms in nature for solving optimisation problems. The GOA algorithm is first benchmarked on a set of test problems including CEC2005 to test and verify its performance qualitatively and quantitatively. It is then employed to find the optimal shape for a 52-bar truss, 3-bar truss, and cantilever beam to demonstrate its applicability. The results show that the proposed algorithm is able to provide superior results compared to well-knowri and recent algorithms in the literature. The results of the real applications also prove the merits of GOA in solving real problems with unknown search spaces. (C) 2017 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27,"Algorithms developed by the author for recognizing persons by their iris patterns have now been tested in many field and laboratory trials, producing no false matches in several million comparison tests. The recognition principle is the failure of a test of statistical independence on iris phase structure encoded by multi-scale quadrature wavelets. The combinatorial complexity of this phase information across different persons spans about 249 degrees of freedom and generates a discrimination entropy of about 3.2 b/mm(2) over the, iris, enabling real-time decisions about personal identity with extremely high confidence. The high confidence levels are important because they allow very large databases to be searched exhaustively (one-to-many ""identification mode"") without making false matches, despite so many chances. Biometrics that lack this property can only survive one-to-one (""verification"") or few comparisons. This paper explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28,"We discuss an Adams-type predictor-corrector method for the numerical solution of fractional differential equations. The method may be used both for linear and for nonlinear problems, and it may be extended to multi-term equations (involving more than one differential operator) too.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29,"Almost all mobile communication systems today use spectrum in the range of 300 MHz-3 GHz. In this article, we reason why the wireless community should start looking at the 3-300 GHz spectrum for mobile broadband applications. We discuss propagation and device technology challenges associated with this band as well as its unique advantages for mobile communication. We introduce a millimeter-wave mobile broadband (MMB) system as a candidate next-generation mobile communication system. We demonstrate the feasibility for MMB to achieve gigabit-per-second data rates at a distance up to 1 km in an urban mobile environment. A few key concepts in MMB network architecture such as the MMB base station grid, MMB inter-BS backhaul link, and a hybrid MMB + 4G system are described. We also discuss beamforming techniques and the frame structure of the MMB air interface.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31,"Making recognition more reliable under uncontrolled lighting conditions is one of the most important challenges for practical face recognition systems. We tackle this by combining the strengths of robust illumination normalization, local texture-based face representations, distance transform based matching, kernel-based feature extraction and multiple feature fusion. Specifically, we make three main contributions: 1) we present a simple and efficient preprocessing chain that eliminates most of the effects of changing illumination while still preserving the essential appearance details that are needed for recognition; 2) we introduce local ternary patterns (LTP), a generalization of the local binary pattern (LBP) local texture descriptor that is more discriminant and less sensitive to noise in uniform regions, and we show that replacing comparisons based on local spatial histograms with a distance transform based similarity metric further improves the performance of LBP/LTP based face recognition; and 3) we further improve robustness by adding Kernel principal component analysis (PCA) feature extraction and incorporating rich local appearance cues from two complementary sources-Gabor wavelets and LBP-showing that the combination is considerably more accurate than either feature set alone. The resulting method provides state-of-the-art performance on three data sets that are widely used for testing recognition under difficult illumination conditions: Extended Yale-B, CAS-PEAL-R1, and Face Recognition Grand Challenge version 2 experiment 4 (FRGC-204). For example, on the challenging FRGC-204 data set it halves the error rate relative to previously published methods, achieving a face verification rate of 88.1% at 0.1% false accept rate. Further experiments show that our preprocessing method outperforms several existing preprocessors for a range of feature sets, data sets and lighting conditions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32,"This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33,"The present status and prospects for further development of polycrystalline or amorphous transparent conducting oxide (TCO) semiconductors used for practical thin-film transparent electrode applications are presented in this paper. The important TCO semiconductors are impurity-doped ZnO, In2O3 and SnO2 as well as multicomponent oxides consisting of combinations of ZnO, In2O3 and SnO2, including some ternary compounds existing in their systems. Development of these and other TCO semiconductors is important because the expanding need for transparent electrodes for optoelectronic device applications is jeopardizing the availability of indium-tin-oxide (ITO), whose main constituent, indium, is a very expensive and scarce material. Al- and Ga-doped ZnO (AZO and GZO) semiconductors are promising as alternatives to ITO for thin-film transparent electrode applications. In particular, AZO thin films, with a low resistivity of the order of 10(-5) Omega cm and source materials that are inexpensive and non-toxic, are the best candidates. However, further development of the deposition techniques, such as magnetron sputtering or vacuum arc plasma evaporation, as well as of the targets is required to enable the preparation of AZO and GZO films on large area substrates with a high deposition rate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34,"Type-2 fuzzy sets let us model and minimize the effects of uncertainties in rule-base fuzzy logic systems. However, they are difficult to understand for a variety of reasons which we enunciate. In this paper, we strive to overcome the difficulties by: 1) establishing a small set of terms that let us easily communicate about type-2 fuzzy sets and also let us define such sets very precisely, 2) presenting a new representation for type-2 fuzzy sets, and 3) using this new representation to derive formulas for union, intersection and complement of type-2 fuzzy sets without having to use the Extension Principle.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35,"The cellular microenvironment plays an integral role in improving the function of microengineered tissues. Control of the microarchitecture in engineered tissues can be achieved through photopatterning of cell-laden hydrogels. However, despite high pattern fidelity of photopolymerizable hydrogels, many such materials are not cell-responsive and have limited biodegradability. Here, we demonstrate gelatin methacrylate (GelMA) as an inexpensive, cell-responsive hydrogel platform for creating cell-laden microtissues and microfluidic devices. Cells readily bound to, proliferated, elongated, and migrated both when seeded on micropatterned GelMA substrates as well as when encapsulated in microfabricated GelMA hydrogels. The hydration and mechanical properties of GelMA were demonstrated to be tunable for various applications through modification of the methacrylation degree and gel concentration. The pattern fidelity and resolution of GelMA were high and it could be patterned to create perfusable microfluidic channels. Furthermore, GelMA micropatterns could be used to create cellular micropatterns for in vitro cell studies or 3D microtissue fabrication. These data suggest that GelMA hydrogels could be useful for creating complex, cell-responsive microtissues, such as endothelialized microvasculature, or for other applications that require cell-responsive microengineered hydrogels. (c) 2010 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36,"LT-codes are a new class of codes introduced by Luby for the purpose of scalable and fault-tolerant distribution of data over computer networks. In this paper, we introduce Raptor codes, an extension of LT-codes with linear time encoding and decoding. We will exhibit a class of universal Raptor codes: for a given integer k and any real epsilon > 0, Raptor codes in this class produce a potentially infinite stream of symbols such that any subset of symbols of size k(1 + epsilon) is sufficient to recover the original k symbols with high probability. Each output symbol is generated using 0(log(1/epsilon)) operations, and the original symbols are recovered from the collected ones with 0(k log(1/epsilon)) operations. We will also introduce novel techniques for the analysis of the error probability of the decoder for finite length Raptor codes. Moreover, we will introduce and analyze systematic versions of Raptor codes, i.e., versions in which the first output elements of the coding system coincide with the original k elements.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37,"A novel reversible data hiding algorithm, which can recover the original image without any distortion from the marked image after the hidden data have been extracted, is presented in this paper. This algorithm utilizes the zero or the minimum points of the histogram of an image and slightly modifies the pixel grayscale values to embed data into the image. It can embed more data than many of the existing reversible data hiding algorithms. It is proved analytically and shown experimentally that the peak signal-to-noise ratio (PSNR) of the marked image generated by this method versus the original image is guaranteed to be above 48 dB. This lower bound of PSNR is much higher than that of all reversible data hiding techniques reported in the literature. The computational complexity of our proposed technique is low and the execution time is short. The algorithm has been successfully applied to a wide range of images, including commonly used images, medical images, texture images, aerial images and all of the 1096 images in CorelDraw database. Experimental results and performance comparison with other reversible data hiding schemes are presented to demonstrate the validity of the proposed algorithm.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38,"Kubios HRV is an advanced and easy to use software for heart rate variability (HRV) analysis. The software supports several input data formats for electrocardiogram (ECG) data and beat-to-beat RR interval data. It includes an adaptive QRS detection algorithm and tools for artifact correction, trend removal and analysis sample selection. The software computes all the commonly used time-domain and frequency-domain HRV parameters and several nonlinear parameters. There are several adjustable analysis settings through which the analysis methods can be optimized for different data. The ECG derived respiratory frequency is also computed, which is important for reliable interpretation of the analysis results. The analysis results can be saved as an ASCII text file (easy to import into MS Excel or SPSS), Matlab MAT-file, or as a PDF report. The software is easy to use through its compact graphical user interface. The software is available free of charge for Windows and Linux operating systems at http://kubios.uerfi. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39,"The manner in which a mutually acceptable co-existence of biomaterials and tissues is developed and sustained has been the focus of attention in biomaterials science for many years, and forms the foundation of the subject of biocompatibility. There are many ways in which materials and tissues can be brought into contact such that this co-existence may be compromised, and the search for biomaterials that are able to provide for the best performance in devices has been based upon the understanding of all the interactions within biocompatibility phenomena. Our understanding of the mechanisms of biocompatibility has been restricted whilst the focus of attention has been long-term implantable devices. In this paper, over 50 years of experience with such devices is analysed and it is shown that, in the vast majority of circumstances, the sole requirement for biocompatibility in a medical device intended for long-term contact with the tissues of the human body is that the material shall do no harm to those tissues, achieved through chemical and biological inertness. Rarely has an attempt to introduce biological activity into a biomaterial been clinically successful in these applications. This essay then turns its attention to the use of biomaterials in tissue engineering, sophisticated cell, drug and gene delivery systems and applications in biotechnology, and shows that here the need for specific and direct interactions between biomaterials and tissue components has become necessary, and with this a new paradigm for biocompatibility has emerged. It is believed that once the need for this change is recognised, so our understanding of the mechanisms of biocompatibility will markedly improve. (C) 2008 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40,"This paper reviews some main results and progress in distributed multi-agent coordination, focusing on papers published in major control systems and robotics journals since 2006. Distributed coordination of multiple vehicles, including unmanned aerial vehicles, unmanned ground vehicles, and unmanned underwater vehicles, has been a very active research subject studied extensively by the systems and control community. The recent results in this area are categorized into several directions, such as consensus, formation control, optimization, and estimation. After the review, a short discussion section is included to summarize the existing research and to propose several promising research directions along with some open problems that are deemed important for further investigations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41,"Discrete and temporarily stable natural reflectors or permanent scatterers (PS) can be identified from long temporal series of interferometric SAR images even with baselines larger than the so-called critical baseline. This subset of image pixels can be exploited successfully for high accuracy differential measurements. We discuss the use of PS in urban areas, like Pomona, CA, showing subsidence and absidence effects. A new approach to the estimation of the atmospheric phase contributions, and the local displacement field is proposed based on simple statistical assumptions. New solutions are presented in order to cope with nonlinear motion of the targets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42,"Previous work has demonstrated that the image variation of many objects ( human faces in particular) under variable lighting can be effectively modeled by low-dimensional linear spaces, even when there are multiple light sources and shadowing. Basis images spanning this space are usually obtained in one of three ways: A large set of images of the object under different lighting conditions is acquired, and principal component analysis (PCA) is used to estimate a subspace. Alternatively, synthetic images are rendered from a 3D model ( perhaps reconstructed from images) under point sources and, again, PCA is used to estimate a subspace. Finally, images rendered from a 3D model under diffuse lighting based on spherical harmonics are directly used as basis images. In this paper, we show how to arrange physical lighting so that the acquired images of each object can be directly used as the basis vectors of a low-dimensional linear space and that this subspace is close to those acquired by the other methods. More specifically, there exist configurations of k point light source directions, with k typically ranging from 5 to 9, such that, by taking k images of an object under these single sources, the resulting subspace is an effective representation for recognition under a wide range of lighting conditions. Since the subspace is generated directly from real images, potentially complex and/or brittle intermediate steps such as 3D reconstruction can be completely avoided; nor is it necessary to acquire large numbers of training images or to physically construct complex diffuse ( harmonic) light fields. We validate the use of subspaces constructed in this fashion within the context of face recognition.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43,"This paper proposes a novel algorithm for multiview stereopsis that outputs a dense set of small rectangular patches covering the surfaces visible in the images. Stereopsis is implemented as a match, expand, and filter procedure, starting from a sparse set of matched keypoints, and repeatedly expanding these before using visibility constraints to filter away false matches. The keys to the performance of the proposed algorithm are effective techniques for enforcing local photometric consistency and global visibility constraints. Simple but effective methods are also proposed to turn the resulting patch model into a mesh which can be further refined by an algorithm that enforces both photometric consistency and regularization constraints. The proposed approach automatically detects and discards outliers and obstacles and does not require any initialization in the form of a visual hull, a bounding box, or valid depth ranges. We have tested our algorithm on various data sets including objects with fine surface details, deep concavities, and thin structures, outdoor scenes observed from a restricted set of viewpoints, and ""crowded"" scenes where moving obstacles appear in front of a static structure of interest. A quantitative evaluation on the Middlebury benchmark [1] shows that the proposed method outperforms all others submitted so far for four out of the six data sets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44,"Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present all method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets. (C) 2006 Elsevier Inc. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45,"The computational modeling of failure mechanisms in solids due to fracture based on sharp crack discontinuities suffers in situations with complex crack topologies. This can be overcome by a diffusive crack modeling based on the introduction of a crack phase field. Following our recent work [C. Miehe, F. Welschinger, M. Hofacker, Thermodynamically-consistent phase field models of fracture: Variational principles and multi-field fe implementations, International Journal for Numerical Methods in Engineering DOI:10.1002/nme.2861] on phase-field-type fracture, we propose in this paper a new variational framework for rate-independent diffusive fracture that bases on the introduction of a local history field. It contains a maximum reference energy obtained in the deformation history, which may be considered as a measure for the maximum tensile strain obtained in history. It is shown that this local variable drives the evolution of the crack phase field. The introduction of the history field provides a very transparent representation of the balance equation that governs the diffusive crack topology. In particular, it allows for the construction of a new algorithmic treatment of diffusive fracture. Here, we propose an extremely robust operator split scheme that successively updates in a typical time step the history field, the crack phase field and finally the displacement field. A regularization based on a viscous crack resistance that even enhances the robustness of the algorithm may easily be added. The proposed algorithm is considered to be the canonically simple scheme for the treatment of diffusive fracture in elastic solids. We demonstrate the performance of the phase field formulation of fracture by means of representative numerical examples. (C) 2010 Elsevier By. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46,"Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by Maximum Likelihood (ML) and Maximum A Posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47,"Millimeter wave (mmWave) cellular systems will enable gigabit-per-second data rates thanks to the large bandwidth available at mmWave frequencies. To realize sufficient link margin, mmWave systems will employ directional beamforming with large antenna arrays at both the transmitter and receiver. Due to the high cost and power consumption of gigasample mixed-signal devices, mmWave precoding will likely be divided among the analog and digital domains. The large number of antennas and the presence of analog beamforming requires the development of mmWave-specific channel estimation and precoding algorithms. This paper develops an adaptive algorithm to estimate the mmWave channel parameters that exploits the poor scattering nature of the channel. To enable the efficient operation of this algorithm, a novel hierarchical multi-resolution codebook is designed to construct training beamforming vectors with different beamwidths. For single-path channels, an upper bound on the estimation error probability using the proposed algorithm is derived, and some insights into the efficient allocation of the training power among the adaptive stages of the algorithm are obtained. The adaptive channel estimation algorithm is then extended to the multi-path case relying on the sparse nature of the channel. Using the estimated channel, this paper proposes a new hybrid analog/ digital precoding algorithm that overcomes the hardware constraints on the analog-only beamforming, and approaches the performance of digital solutions. Simulation results show that the proposed low-complexity channel estimation algorithm achieves comparable precoding gains compared to exhaustive channel training algorithms. The results illustrate that the proposed channel estimation and precoding algorithms can approach the coverage probability achieved by perfect channel knowledge even in the presence of interference.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48,"A new view-based approach to the representation and recognition of human movement is presented. The basis of the representation is a temporal template-a static vector-image where the vector value at each point is a function of the motion properties at the corresponding spatial location in an image sequence. Using aerobics exercises as a test domain, we explore the representational power of a simple, two component version of the templates: The first value is a binary value indicating the presence of motion and the second value is a function of the recency of motion in a sequence. We then develop a recognition method matching temporal templates against stored instances of Views of known actions. The method automatically performs temporal segmentation, is invariant to linear changes in speed, and runs in real-time on standard platforms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49,"The future of mobile communications looks exciting with the potential new use cases and challenging requirements of future 6th generation (6G) and beyond wireless networks. Since the beginning of the modern era of wireless communications, the propagation medium has been perceived as a randomly behaving entity between the transmitter and the receiver, which degrades the quality of the received signal due to the uncontrollable interactions of the transmitted radio waves with the surrounding objects. The recent advent of reconfigurable intelligent surfaces in wireless communications enables, on the other hand, network operators to control the scattering, refiection, and refraction characteristics of the radio waves, by overcoming the negative effects of natural wireless propagation. Recent results have revealed that reconfigurable intelligent surfaces can effectively control the wavefront, e.g., the phase, amplitude, frequency, and even polarization, of the impinging signals without the need of complex decoding, encoding, and radio frequency processing operations. Motivated by the potential of this emerging technology, the present article is aimed to provide the readers with a detailed overview and historical perspective on state-of-the-art solutions, and to elaborate on the fundamental differences with other technologies, the most important open research issues to tackle, and the reasons why the use of reconfigurable intelligent surfaces necessitates to rethink the communication-theoretic models currently employed in wireless networks. This article also explores theoretical performance limits of reconfigurable intelligent surface-assisted communication systems using mathematical techniques and elaborates on the potential use cases of intelligent surfaces in 6G and beyond wireless networks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
50,"The data of interest are assumed to be represented as N-dimensional real vectors, and these vectors are compressible in some linear basis B, implying that the signal can be reconstructed accurately using only a small number M << N of basis-function coefficients associated with B. Compressive sensing is a framework whereby one does not measure one of the aforementioned N-dimensional signals directly, but rather a set of related measurements, with the new measurements a linear combination of the original underlying N-dimensional signal. The number of required compressive-sensing measurements is typically much smaller than N, offering the potential to simplify the sensing system. Let f denote the unknown underlying N-dimensional signal, and g a vector of compressive-sensing measurements, then one may approximate f accurately by utilizing knowledge of the (under-determined) linear relationship between f and g, in addition to knowledge of the fact that f is compressible in B. In this paper we employ a Bayesian formalism for estimating the underlying signal f based on compressive-sensing measurements g. The proposed framework has the following properties: i) in addition to estimating the underlying signal f, ""error bars"" are also estimated, these giving a measure of confidence in the inverted signal; ii) using knowledge of the error bars, a principled means is provided for determining when a sufficient number of compressive-sensing measurements have been performed; iii) this setting lends itself naturally to a framework whereby the compressive sensing measurements are optimized adaptively and hence not determined randomly; and iv) the framework accounts for additive noise in the compressive-sensing measurements and provides an estimate of the noise variance. In this paper we present the underlying theory, an associated algorithm, example results, and provide comparisons to other compressive-sensing inversion algorithms in the literature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
51,"This review discusses the problems of sulfur reduction in highway and non-road fuels and presents an overview of new approaches and emerging technologies for ultra-deep desulfurization of refinery streams for ultra-clean (ultra-low-sulfur) gasoline, diesel fuels and jet fuels. The issues of gasoline and diesel deep desulfurization are becoming more serious because the crude oils refined in the US are getting higher in sulfur contents and heavier in density, while the regulated sulfur limits are becoming lower and lower. Current gasoline desulfurization problem is dominated by the issues of sulfur removal from FCC naphtha, which contributes about 35% of gasoline pool but over 90% of sulfur in gasoline. Deep reduction of gasoline sulfur (from 330 to 30 ppm) must be made without decreasing octane number or losing gasoline yield. The problem is complicated by the high olefins contents of FCC naphtha which contributes to octane number enhancement but can be saturated under HDS conditions. Deep reduction of diesel sulfur (from 500 to < 15 ppm sulfur) is dictated largely by 4,6-dimethyldibenzothiophene, which represents the least reactive sulfur compounds that have substitutions on both 4- and 6-positions. The deep HDS problem of diesel streams is exacerbated by the inhibiting effects of co-existing polyaromatics and nitrogen compounds in the feed as well as H2S in the product. The approaches to deep desulfurization include catalysts and process developments for hydrodesulfurization (HDS), and adsorbents or reagents and methods for non-HDS-type processing schemes. The needs for dearomatization of diesel and jet fuels are also discussed along with some approaches. Overall, new and more effective approaches and continuing catalysis and processing research are needed for producing affordable ultra-clean (ultra-low-sulfur and low-aromatics) transportation fuels and non-road fuels, because meeting the new government sulfur regulations in 2006-2010 (15 ppm sulfur in highway diesel fuels by 2006 and non-road diesel fuels by 20 10; 30 ppm sulfur in gasoline by 2006) is only a milestone. Desulfurization research should also take into consideration of the fuel-cell fuel processing needs, which will have a more stringent requirement on desulfurization (e.g., < 1 ppm sulfur) than IC engines. The society at large is stepping on the road to zero sulfur fuel, so researchers should begin with the end in mind and try to develop long-term solutions. (C) 2003 Elsevier B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52,"We describe a method to estimate the capacity limit of fiber-optic communication systems (or ""fiber channels"") based on information theory. This paper is divided into two parts. Part 1 reviews fundamental concepts of digital communications and information theory. We treat digitization and modulation followed by information theory for channels both without and with memory. We provide explicit relationships between the commonly used signal-to-noise ratio and the optical signal-to-noise ratio. We further evaluate the performance of modulation constellations such as quadrature-amplitude modulation, combinations of amplitude-shift keying and phase-shift keying, exotic constellations, and concentric rings for an additive white Gaussian noise channel using coherent detection. Part 2 is devoted specifically to the ""fiber channel."" We review the physical phenomena present in transmission over optical fiber networks, including sources of noise, the need for optical filtering in optically-routed networks, and, most critically, the presence of fiber Kerr nonlinearity. We describe various transmission scenarios and impairment mitigation techniques, and define a fiber channel deemed to be the most relevant for communication over optically-routed networks. We proceed to evaluate a capacity limit estimate for this fiber channel using ring constellations. Several scenarios are considered, including uniform and optimized ring constellations, different fiber dispersion maps, and varying transmission distances. We further present evidences that point to the physical origin of the fiber capacity limitations and provide a comparison of recent record experiments with our capacity limit estimation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53,"A novel method is proposed for performing multilabel, interactive image segmentation. Given a small number of pixels with user-defined (or predefined) labels, one can analytically and quickly determine the probability that a random walker starting at each unlabeled pixel will first reach one of the prelabeled pixels. By assigning each pixel to the label for which the greatest probability is calculated, a high-quality image segmentation may be obtained. Theoretical properties of this algorithm are developed along with the corresponding connections to discrete potential theory and electrical circuits. This algorithm is formulated in discrete space (i.e., on a graph) using combinatorial analogues of standard operators and principles from continuous potential theory, allowing it to be applied in arbitrary dimension on arbitrary graphs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
54,"Multiuser diversity is a form of diversity inherent in a wireless network, provided by independent time-varying channels across the different users. The diversity benefit is exploited by tracking the channel fluctuations of the users and scheduling tran missions to users when their instantaneous channel quality is near the peak. The diversity gain increases with the dynamic range of the fluctuations and is thus limited in environments with little scattering and/or slow fading. In such environments, we propose the use of multiple transmit antennas to induce large and fast channel fluctuations so that multiuser diversity can still be exploited. The scheme can be interpreted as opportunistic beamforming and we show that true beamforming gains can be achieved when there are sufficient users, even though very limited channel feedback is needed. Furthermore, in a cellular system, the scheme plays an additional role of opportunistic nulling of the interference created on users of adjacent cells. We discuss the design implications of implementing this scheme in a complete wireless system.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
55,"Millimeter-wave (mmW) frequencies between 30 and 300 GHz are a new frontier for cellular communication that offers the promise of orders of magnitude greater bandwidths combined with further gains via beamforming and spatial multiplexing from multielement antenna arrays. This paper surveys measurements and capacity studies to assess this technology with a focus on small cell deployments in urban environments. The conclusions are extremely encouraging; measurements in New York City at 28 and 73 GHz demonstrate that, even in an urban canyon environment, significant non-line-of-sight (NLOS) outdoor, street-level coverage is possible up to approximately 200 m from a potential low-power microcell or picocell base station. In addition, based on statistical channel models from these measurements, it is shown that mmW systems can offer more than an order of magnitude increase in capacity over current state-of-the-art 4G cellular networks at current cell densities. Cellular systems, however, will need to be significantly redesigned to fully achieve these gains. Specifically, the requirement of highly directional and adaptive transmissions, directional isolation between links, and significant possibilities of outage have strong implications on multiple access, channel structure, synchronization, and receiver design. To address these challenges, the paper discusses how various technologies including adaptive beamforming, multihop relaying, heterogeneous network architectures, and carrier aggregation can be lever-aged in the mmW context.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56,"Small plastic detritus, termed ""microplastics"", are a widespread and ubiquitous contaminant of marine ecosystems across the globe. Ingestion of microplastics by marine biota, including mussels, worms, fish, and seabirds, has been widely reported, but despite their vital ecological role in marine food-webs, the impact of microplastics on zooplankton remains under-researched. Here, we show that microplastics are ingested by, and may impact upon, zooplankton. We used bioimaging techniques to document ingestion, egestion, and adherence of microplastics in a range of zooplankton common to the northeast Atlantic, and employed feeding rate studies to determine the impact of plastic detritus on algal ingestion rates in copepods. Using fluorescence and coherent anti-Stokes Raman scattering (CARS) microscopy we identified that thirteen zooplankton taxa had the capacity to ingest 1.7-30.6 mu m polystyrene beads, with uptake varying by taxa, life-stage and bead-size. Post-ingestion, copepods egested faecal pellets laden with microplastics. We further observed microplastics adhered to the external carapace and appendages of exposed zooplankton. Exposure of the copepod Centropages typicus to natural assemblages of algae with and without microplastics showed that 7.3 mu m microplastics (>4000 mL(-1)) significantly decreased algal feeding. Our findings imply that marine microplastic debris can negatively impact upon zooplankton function and health.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
57,"A plethora of definitions for innovation types has resulted in an ambiguity in the way the terms 'innovation' and 'innovativeness' are p operationalized and utilized in the new product development literature. The terms radical, really-new, incremental and discontinuous are used ubiquitously to identify innovations. One must question, what is the difference between these different classifications? To date consistent definitions for these innovation types have not emerged from the new product research community. A review of the literature from the marketing, engineering, and new product development disciplines attempts to put some clarity and continuity to the use of these terms. This review shows that it is important to consider both a marketing and technological perspective as well as a macrolevel and microlevel perspective when identifying innovations. Additionally, it is shown when strict classifications from the extant literature are applied, a significant shortfall appears in empirical work directed toward radical and really new innovations. A method for classifying innovations is suggested so that practitioners and academies can talk with a common understanding of how a specific innovation type is identified and how the innovation process may be unique for that particular innovation type. A recommended list of measures based on extant literature is provided for future empirical research concerning technological innovations and innovativeness. (C) 2002 PDMA. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58,"Attractive features of time-hopping spread-spectrum multiple-access systems employing impulse signal technology are outlined, and emerging design issues are described. Performance of such communications systems in terms of achievable transmission rate and multiple-access capability are estimated for both analog and digital data modulation formats under ideal multiple-access channel conditions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59,"With the severe spectrum shortage in conventional cellular bands, millimeter wave (mmW) frequencies between 30 and 300 GHz have been attracting growing attention as a possible candidate for next-generation micro-and picocellular wireless networks. The mmW bands offer orders of magnitude greater spectrum than current cellular allocations and enable very high-dimensional antenna arrays for further gains via beamforming and spatial multiplexing. This paper uses recent real-world measurements at 28 and 73 GHz in New York, NY, USA, to derive detailed spatial statistical models of the channels and uses these models to provide a realistic assessment of mmW micro-and picocellular networks in a dense urban deployment. Statistical models are derived for key channel parameters, including the path loss, number of spatial clusters, angular dispersion, and outage. It is found that, even in highly non-line-of-sight environments, strong signals can be detected 100-200 m from potential cell sites, potentially with multiple clusters to support spatial multiplexing. Moreover, a system simulation based on the models predicts that mmW systems can offer an order of magnitude increase in capacity over current state-of-the-art 4G cellular networks with no increase in cell density from current urban deployments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
60,"Wireless microsensor networks have been identified as one of the most important technologies for the 21st century. This paper traces the history of research in sensor networks over the past three decades, including two important programs of the Defense Advanced Research Projects Agency (DARPA) spanning this period: the Distributed Sensor Networks (DSN) and the Sensor Information Technology (SensIT) programs. Technology trends that impact the development of sensor networks are reviewed, and new applications such as infrastructure security. habitat monitoring, and traffic control are presented. Technical challenges in sensor network development include network discovery, control and routing, collaborative signal and information processing, tasking and querying, and security. The paper concludes by presenting some recent research results in sensor network algorithms, including localized algorithms and directed diffusion, distributed tracking in wireless ad hoc networks, and distributed classification using local agents.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
61,"The paper presents a compact Matlab implementation of a topology optimization code for compliance minimization of statically loaded structures. The total number of Matlab input lines is 99 including optimizer and Finite Element subroutine. The 99 lines are divided into 36 lines for the main program, 12 lines for the Optimality Criteria based optimizer, 16 lines for a mesh-independency filter and 35 lines for the finite element code. In fact, excluding comment lines and lines associated with output and finite element analysis, it is shown that only 49 Matlab input lines are required for solving a well-posed topology optimization problem. By adding three additional lines, the program can solve problems with multiple load cases. The code is intended for educational purposes. The complete Matlab code is given in the Appendix and can be down-loaded from the web-site http://www.topopt.dtu.dk.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62,"Purpose - Following the call for awareness of accepted reporting practices by Ringle, Sarstedt, and Straub in 2012, the purpose of this paper is to review and analyze the use of partial least squares structural equation modeling (PLS-SEM) in Industrial Management & Data Systems (IMDS) and extend MIS Quarterly (MISQ) applications to include the period 2012-2014. Design/methodology/approach - Review of PLS-SEM applications in information systems (IS) studies published in IMDS and MISQ for the period 2010-2014 identifying a total of 57 articles reporting the use of or commenting on PLS-SEM. Findings - The results indicate an increased maturity of the IS field in using PLS-SEM for model complexity and formative measures and not just small sample sizes and non-normal data. Research limitations/implications - Findings demonstrate the continued use and acceptance of PLS-SEM as an accepted research method within IS. PLS-SEM is discussed as the preferred SEM method when the research objective is prediction. Practical implications - This update on PLS-SEM use and recent developments will help authors to better understand and apply the method. Researchers are encouraged to engage in complete reporting procedures. Originality/value - Applications of PLS-SEM for exploratory research and theory development are increasing. IS scholars should continue to exercise sound practice by reporting reasons for using PLS-SEM and recognizing its wider applicability for research. Recommended reporting guidelines following Ringle et al. (2012) and Gefen et al. (2011) are included. Several important methodological updates are included as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63,"An experimental system was built to investigate convective heat trouser and flow features of the nanofluid in a tube. Both the convective heat transfer coefficient and friction factor of the sample nanofluids for the turbulent flow are measured, respectively. The effects of such factors as the volume fraction of suspended nanoparticles and the Reynolds number on the heat transfer and flow features are discussed in detail. A new type of convective heat transfer correlation is proposed to correlate experimental data of heat transfer for nanofluids.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65,"This paper presents control and coordination algorithms for groups of vehicles. The focus is on autonomous vehicle networks performing distributed sensing tasks, where each vehicle plays the role of a mobile tunable sensor. The paper proposes gradient descent algorithms for a class of utility functions which encode optimal coverage and sensing policies. The resulting closed-loop behavior is adaptive, distributed, asynchronous, and verifiably correct.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
66,"Microporous, non-woven poly(epsilon-caprolactone) (PCL) scaffolds were made by electrostatic fiber spinning. In this process, polymer fibers with diameters down to the nanometer range, or nanofibers, are formed by subjecting a fluid jet to a high electric field. Mesenchymal stem cells (MSCs) derived from the bone marrow of neonatal rats were cultured, expanded and seeded oil electrospun PCL scaffolds. The cell-polymer constructs were cultured with osteogenic supplements under dynamic culture conditions for up to 4 weeks. The cell-polymer constructs maintained the size and shape of the original scaffolds. Scanning electron microscopy (SEM), histological and immunohistochemical examinations were performed. Penetration of cells and abundant extracellular matrix were observed in the cell-polymer constructs after 1 week. SEM showed that the surfaces of the cell-polymer constructs were covered with cell multilayers at 4 weeks. In addition, mineralization and type I collagen were observed at 4 weeks. This suggests that electrospun PCL is a potential candidate scaffold for bone tissue engineering. (C) 2003 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
67,"The development of an electroencephalograph (EEG)-based brain-computer interface (BCI) requires rapid and reliable discrimination of EEG patterns, e.g., associated with imaginary movement. One-sided hand movement imagination results in EEG changes located at contra- and ipsilateral central areas, We demonstrate that spatial filters for multichannel EEG effectively extract discriminatory information from two populations of single-trial EEG, recorded during left- and right-hand movement imagery. The best classification results for three subjects are 90.8%, 92.7%, and 99.7%. The spatial filters are estimated from a set of data by the method of common spatial patterns and reflect the specific activation of cortical areas, The method performs a weighting of the electrodes according to their importance for the classification task. The high recognition rates and computational simplicity make it a promising method for an EEG-based brain-computer interface.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68,"Monte Carlo simulation is an essential tool in emission tomography that can assist in the design of new medical imaging devices, the optimization of acquisition protocols and the development or assessment of image reconstruction algorithms and correction techniques. GATE, the Geant4 Application for Tomographic Emission, encapsulates the Geant4 libraries to achieve a modular, versatile, scripted simulation toolkit adapted to the field of nuclear medicine. In particular, GATE allows the description of time-dependent phenomena such as source or detector movement, and source decay kinetics. This feature makes it possible to simulate time curves under realistic acquisition conditions and to test dynamic reconstruction algorithms. This paper gives a detailed description of the design and development of GATE by the OpenGATE collaboration, whose continuing objective is to improve, document and validate GATE by simulating commercially available imaging systems for PET and SPECT. Large effort is also invested in the ability and the flexibility to model novel detection systems or systems still under design. A public release of GATE licensed under the GNU Lesser General Public License can be downloaded at http:/www-lphe.epfl.ch/GATE/. Two benchmarks developed for PET and SPECT to test the installation of GATE and to serve as a tutorial for the users are presented. Extensive validation of the GATE simulation platform has been started, comparing simulations and measurements on commercially available acquisition systems. References to those results are listed. The future prospects towards the gridification of GATE and its extension to other domains such as dosimetry are also discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
69,"Many of the popular building energy simulation programs around the world are reaching maturity - some use simulation methods land even code) that originated in the 1960s. For more than two decades, the US government supported development of two hourly building energy simulation programs, BLAST and DOE-2. Designed in the days of mainframe computers, expanding their capabilities further has become difficult, time-consuming, and expensive. At the same time, the 30 years have seen significant advances in analysis and computational methods and power - providing an opportunity for significant improvement in these tools. In 1996, a US federal agency began developing a new building energy simulation tool, EnergyPlus, building on development experience with two existing programs: DOE-2 and BLAST. EnergyPlus includes a number of innovative simulation features - such as variable time steps, user-configurable modular systems that are integrated with a heat and mass balance-based zone simulation - and input and output data structures tailored to facilitate third party module and interface development. Other planned simulation capabilities include multizone airflow, and electric power and solar thermal and photovoltaic simulation. Beta testing of EnergyPlus began in late 1999 and the first release is scheduled for early 2001. (C) 2001 Elsevier Science B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
70,"We consider the uplink (UL) and downlink (DL) of non-cooperative multi-cellular time-division duplexing (TDD) systems, assuming that the number N of antennas per base station (BS) and the number K of user terminals (UTs) per cell are large. Our system model accounts for channel estimation, pilot contamination, and an arbitrary path loss and antenna correlation for each link. We derive approximations of achievable rates with several linear precoders and detectors which are proven to be asymptotically tight, but accurate for realistic system dimensions, as shown by simulations. It is known from previous work assuming uncorrelated channels, that as N -> infinity while K is fixed, the system performance is limited by pilot contamination, the simplest precoders/detectors, i.e., eigenbeamforming (BF) and matched filter (MF), are optimal, and the transmit power can be made arbitrarily small. We analyze to which extent these conclusions hold in the more realistic setting where N is not extremely large compared to K. In particular, we derive how many antennas per UT are needed to achieve eta% of the ultimate performance limit with infinitely many antennas and how many more antennas are needed with MF and BF to achieve the performance of minimum mean-square error (MMSE) detection and regularized zero-forcing (RZF), respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
71,"The Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) is a two-wavelength polarization lidar performs global profiling of aerosols and clouds in the troposphere and lower stratosphere. CALIOP is the primary instrument on the Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observations (CALIPSO) satellite, which has flown in formation with the NASA A-train constellation of satellites since May 2006. The global, multiyear dataset obtained from CALIOP provides a new view of the earth's atmosphere and will lead to an improved understanding of the role of aerosols and clouds in the climate system. A suite of algorithms has been developed to identify aerosol and cloud layers and to retrieve a variety of optical and microphysical properties. CALIOP represents a significant advance over previous space lidars, and the algorithms that have been developed have many innovative aspects to take advantage of its capabilities. This paper provides a brief overview of the CALIPSO mission, the CALIOP instrument and data products, and an overview of the algorithms used to produce these data products.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
72,"Coding strategies that exploit node cooperation are developed for relay networks. Two basic schemes are studied: the relays decode-and-forward the source message to the destination, or they compress-and-forward their channel outputs to the destination. The decode-and-forward scheme is a variant of multihopping, but in addition to having the relays successively decode the message, the transmitters cooperate and each receiver uses several or all of its past channel output blocks to decode. For the compress-and-forward scheme, the relays take advantage of the statistical dependence between their channel outputs and the destination's channel output. The strategies are applied to wireless channels, and it is shown that decode-and-forward achieves the ergodic capacity with phase fading if phase information is available only locally, and if the relays are near the source node. The ergodic capacity coincides with the rate of a distributed antenna array with full cooperation even though the transmitting antennas are not colocated. The capacity results generalize broadly, including to multiantenna transmission with Rayleigh fading, single-bounce fading, certain quasi-static fading problems, cases where partial channel knowledge is available at the transmitters, and cases where local user cooperation is permitted. The results further extend to multisource and multidestination networks such as multiaccess and broadcast relay channels.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
73,"We investigate the effects of fading correlations in multielement antenna (MEA) communication systems, Pioneering studies show-ed that if the fades connecting pairs of transmit and receive antenna elements are independently, identically distributed, MEA's offer a large increase in capacity compared to single-antenna systems. An MEA system can be described in terms of spatial eigenmodes, which are single-input single-output subchannels. The channel capacity of an MEA is the sum of capacities of these subchannels. We will show that the fading correlation affects the MEA capacity by modifying the distributions of the gains of these subchannels. The fading correlation depends on the physical parameters of MEA and the scatterer characteristics. in this paper, to characterize the fading correlation, we employ an abstract model, which is appropriate for modeling narrow-band Rayleigh fading in filed wireless systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74,"In classic pattern recognition problems, classes are mutually exclusive by definition. Classification errors occur when the classes overlap in the feature space. We examine a different situation, occurring when the classes are, by definition, not mutually exclusive. Such problems arise in semantic scene and document classification and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a different treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classification; furthermore, our work appears to generalize to other classification problems of the same nature. (C) 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75,"Wireless sensor networks will be widely deployed in the near future. While much research has focused on making these networks feasible and useful, security has received little attention. We present a suite of security protocols optimized for sensor networks: SPINS. SPINS has two secure building blocks: SNEP and muTESLA. SNEP includes: data confidentiality, two-party data authentication, and evidence of data freshness. muTESLA provides authenticated broadcast for severely resource-constrained environments. We implemented the above protocols, and show that they are practical even on minimal hardware: the performance of the protocol suite easily matches the data rate of our network. Additionally, we demonstrate that the suite can be used for building higher level protocols.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
76,"Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77,"Mobile edge computing (MEC) is an emergent architecture where cloud computing services are extended to the edge of networks leveraging mobile base stations. As a promising edge technology, it can be applied to mobile, wireless, and wire-line scenarios, using software and hardware platforms, located at the network edge in the vicinity of end-users. MEC provides seamless integration of multiple application service providers and vendors toward mobile subscribers, enterprises, and other vertical segments. It is an important component in the 5G architecture which supports variety of innovative applications and services where ultralow latency is required. This paper is aimed to present a comprehensive survey of relevant research and technological developments in the area of MEC. It provides the definition of MEC, its advantages, architectures, and application areas; where we in particular highlight related research and future directions. Finally, security and privacy issues and related existing solutions are also discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78,"The computational modeling of failure mechanisms in solids due to fracture based on sharp crack discontinuities suffers in situations with complex crack topologies. This can be overcome by a diffusive crack modeling based on the introduction of a crack phase-field. In this paper, we outline a thermodynamically consistent framework for phase-field models of crack propagation in elastic solids, develop incremental variational principles and consider their numerical implementations by multi-field finite element methods. We start our investigation with an intuitive and descriptive derivation of a regularized crack surface functional that Gamma-converges for vanishing length-scale parameter to a sharp crack topology functional. This functional provides the basis for the definition of suitable convex dissipation functions that govern the evolution of the crack phase-field. Here, we propose alternative rate-independent and viscous over-force models that ensure the local growth of the phase-field. Next, we define an energy storage function whose positive tensile part degrades with increasing phase-field. With these constitutive functionals at hand, we derive the coupled balances of quasi-static stress equilibrium and gradient-type phase-field evolution in the solid from the argument of virtual power. Here, we consider a canonical two-field setting for rate-independent response and a time-regularized three-field formulation with viscous over-force response. It is then shown that these balances follow as the Euler equations of incremental variational principles that govern the multi-field problems. These principles make the proposed formulation extremely compact and provide a perfect base for the finite element implementation, including features such as the symmetry of the monolithic tangent matrices. We demonstrate the performance of the proposed phase-field formulations of fracture by means of representative numerical examples. Copyright (C) 2010 John Wiley & Sons, Ltd.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79,"The decision to build a Fischer-Tropsch (FT) plant is still fraught with risk because it has to be based on the perceived future price and availability of petroleum crude oil and on local politics. The most expensive section of an FT complex is the production of purified syngas and so its composition should match the overall usage ratio of the FT reactions, which in turn depends on the product selectivity. The kinetics, reactor requirements, control of selectivity and the life of cobalt and iron catalysts are discussed and compared. Control of the FT conditions coupled with appropriate downstream processes results in high yields of gasoline, excellent quality diesel fuel or high value linear alpha-olefins. The history of the various FT options and of the improvements in FT reactor technologies over the last 50 years is reviewed. It appears that ""new"" technologies are re-discovered in cycles of 15-30 years and it often takes the same time for the implementation of new concepts. (C) 2002 EIsevier Science B.V. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
80,"The paradigmatic shift from a Web of manual interactions to a Web of programmatic interactions driven by Web services is creating unprecedented opportunities for the formation of online Business-to-Business (B2B) collaborations. In particular, the creation of value-added services by composition of existing ones is gaining a significant momentum. Since many available Web services provide overlapping or identical functionality, albeit with different Quality of Service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. This paper presents a middleware platform which addresses the issue of selecting Web services for the purpose of their composition in a way that maximizes user satisfaction expressed as utility functions over QoS attributes, while satisfying the constraints set by the user and by the structure of the composite service. Two selection approaches are described and compared: one based on local (task-level) selection of services and the other based on global allocation of tasks to services using integer programming.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
81,"Mutual information transfer characteristics of soft in/soft out decoders are proposed as a tool to better understand the convergence behavior of iterative decoding schemes. The exchange of extrinsic information is visualized as a decoding trajectory in the extrinsic information transfer chart (EXIT chart). This allows the prediction of turbo cliff position and bit error rate after an arbitrary number of iterations. The influence of code memory, code polynomials as well as different constituent codes on the convergence behavior is studied for parallel concatenated codes. A code search based on the EXIT chart technique has been performed yielding new recursive systematic convolutional constituent codes exhibiting turbo cliffs at lower signal-to-noise ratios than attainable by previously known constituent codes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82,We discuss the biological plausibility and computational efficiency of some of the most useful models of spiking and bursting neurons. We compare their applicability to large-scale simulations of cortical neural networks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
83,"The global electrical energy consumption is rising and there is a steady increase of the demand on the power capacity, efficient production, distribution and utilization of energy. The traditional power systems are changing globally, a large number of dispersed generation (DG) units, including both renewable and nonrenewable energy sources such as wind turbines, photovoltaic (PV) generators, fuel cells, small hydro, wave generators, and gas/steam powered combined heat and power stations, are being integrated into power systems at distribution level. Power electronic, the technology of efficiently processing electric power, plays an essential part in the integration of the dispersed generation units for good efficiency and high performance of the power systems. This paper reviews the applications of power electronics in the integration of DG units, in particularly, wind power, fuel cells and PV generators.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84,"Status and future outlook of III-V compound semiconductor visible-spectrum light-emitting diodes (LEDs) are presented. Light extraction techniques are reviewed and extraction efficiencies are quantified in the 60%+ (AlGaInP) and similar to 80% (InGaN) regimes for state-of-the-art devices. The phosphor-based white LED concept is reviewed and recent performance discussed, showing that high-power white LEDs now approach the 100-lm/W regime. Devices employing multiple phosphors for ""warm"" white color temperatures (similar to 3000-4000 K) and high color rendering (CRI > 80), which provide properties critical for many illumination applications, are discussed. Recent developments in chip design, packaging, and high current performance lead to very high luminance devices (similar to 50 Mcd/m(2) white at 1 A forward current in 1 x 1 mm(2) chip) that are suitable for application to automotive forward lighting. A prognosis for future LED performance levels is considered given further improvements in internal quantum efficiency, which to date lag achievements in light extraction efficiency for InGaN LEDs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85,"We propose a new method for reconstruction of sparse signals with and without noisy perturbations, termed the subspace pursuit algorithm. The algorithm has two important characteristics: low computational complexity, comparable to that of orthogonal matching pursuit techniques when applied to very sparse signals, and reconstruction accuracy of the same order as that of linear programming (LP) optimization methods. The presented analysis shows that in the noiseless setting, the proposed algorithm can exactly reconstruct arbitrary sparse signals provided that the sensing matrix satisfies the restricted isometry property with a constant parameter. In the noisy setting and in the case that the signal is not exactly sparse, it can be shown that the mean-squared error of the reconstruction is upper-bounded by constant multiples of the measurement and signal perturbation energies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86,"The kinetics and mechanism of methylene blue adsorption on commercial activated carbon (CAC) and indigenously prepared activated carbons from bamboo dust, coconut shell, groundnut shell, rice husk, and straw, have been studied. The effects of various experimental parameters have been investigated using a batch adsorption technique to obtain information on treating effluents from the dye industry. The extent of dye removal increased with decrease in the initial concentration of the dye and particle size of the adsorbent and also increased with increase in contact time, amount of adsorbent used and the initial pH of the solution. Adsorption data were modeled using the Freundlich and Langmuir adsorption isotherms and first order kinetic equations. The kinetics of adsorption were found to be first order with regard to intra-particle diffusion rate. The adsorption capacities of indigenous activated carbons have been compared with that of the commercial activated carbon. The results indicate that such carbons could be employed as low cost alternatives to commercial activated carbon in wastewater treatment for the removal of colour and dyes. (C) 2001 Published by Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
87,"In this paper, we consider a multi-agent consensus problem with an active leader and variable interconnection topology. The state of the considered leader not only keeps changing but also may not be measured. To track such a leader, a neighbor-based local controller together with a neighbor-based state-estimation rule is given for each autonomous agent. Then we prove that, with the proposed control scheme, each agent can follow the leader if the (acceleration) input of the active leader is known, and the tracking error is estimated if the input of the leader is unknown. (c) 2006 Elsevier Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
88,"For 100 years, there has been no change in the basic structure of the electrical power grid. Experiences have shown that the hierarchical, centrally controlled grid of the 20th Century is ill-suited to the needs of the 21st Century. To address the challenges of the existing power grid, the new concept of smart grid has emerged. The smart grid can be considered as a modern electric power grid infrastructure for enhanced efficiency and reliability through automated control, high-power converters, modern communications infrastructure, sensing and metering technologies, and modern energy management techniques based on the optimization of demand, energy and network availability, and so on. While current power systems are based on a solid information and communication infrastructure, the new smart grid needs a different and much more complex one, as its dimension is much larger. This paper addresses critical issues on smart grid technologies primarily in terms of information and communication technology (ICT) issues and opportunities. The main objective of this paper is to provide a contemporary look at the current state of the art in smart grid communications as well as to discuss the still-open research issues in this field. It is expected that this paper will provide a better understanding of the technologies, potential advantages and research challenges of the smart grid and provoke interest among the research community to further explore this promising research area.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
89,"Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/similar to rbg/rcnn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90,This essay discusses some of the issues involved in the identification and predictions of hydrological models given some calibration data. The reasons for the incompleteness of traditional calibration methods are discussed. The argument is made that the potential for multiple acceptable models as representations of hydrological and other environmental systems (the equifinality thesis) should be given more serious consideration than hitherto. It proposes some techniques for an extended GLUE methodology to make it more rigorous and outlines some of the research issues still to be resolved. (c) 2005 Elsevier Ltd All rights reserved.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
91,"The theoretically optimal approach to multisensor-multitarget detection, tracking, and identification is a suitable generalization of the recursive Bayes nonlinear filter. Even in single-target problems, this optimal filter is so computationally challenging that it must usually be approximated. Consequently, multitarget Bayes filtering will never be of practical interest without the development of drastic but principled approximation strategies. In single-target problems, the computationally fastest approximate filtering approach is the constant-gain Kalman filter. This filter propagates a first-order statistical moment-the posterior expectation-in the place of the posterior distribution. The purpose of this paper is to propose an analogous strategy for multitarget systems: propagation of a first-order statistical moment of the multitarget posterior. This moment, the probability hypothesis density (PHD), is the function whose integral in any region of state space is the expected number of targets in that region. We derive recursive Bayes filter equations for the PHD that account for multiple sensors, nonconstant probability of detection, Poisson false alarms, and appearance, spawning, and disappearance of targets. We also show that the PHD is a best-fit approximation of the multitarget posterior in an information-theoretic sense.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92,"The self-organizing map (SOM) is an excellent tool in exploratory phase of data mining. It projects input space on prototypes of a low-dimensional regular grid that can be effectively utilized to visualize and explore properties of the data. When the number of SOM units is large, to facilitate quantitative analysis of the map and the data, similar units need to be grouped, i.e., clustered. In this paper, different approaches to clustering of the SOM are considered, In particular, the use of hierarchical agglomerative clustering and partitive clustering using Ic-means are investigated. The two-stage procedure-first using SOM to produce the prototypes that are then clustered in the second stage-is found to perform well when compared with direct clustering of the data and to reduce the computation time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93,"Emerging recognition of two fundamental errors underpinning past polices for natural resource issues heralds awareness of the need for a worldwide fundamental change in thinking and in practice of environmental management. The first error has been an implicit assumption that ecosystem responses to human use are linear predictable and controllable. The second has been an assumption that human and natural systems can be treated independently. However, evidence that has been accumulating in diverse regions all over the world suggests that natural and social systems behave in nonlinear ways, exhibit marked thresholds in their dynamics, and that social-ecological systems act as strongly coupled, complex and evolving integrated systems. This article is a summary of a report prepared on behalf of the Environmental Advisory Council to the Swedish Government, as input to the process of the World Summit on Sustainable Development (WSSD) in Johannesburg, South Africa in 26 August 4 September 2002. We use the concept of resilience-the capacity to buffer change, learn and develop-as a framework for understanding how to sustain and enhance adaptive capacity in a complex world of rapid transformations. Two useful tools for resilience-building in social-ecological systems are structured scenarios and active adaptive management. These tools require and facilitate a social context with flexible and open institutions and multi-level governance systems that allow for learning and increase adaptive capacity without foreclosing future development options.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94,"This paper presents a technology review of voltage-source-converter topologies for industrial medium-voltage drives. In this highly active area, different converter topologies and circuits have found their application in the market. This paper covers the high-power voltage-source inverter and the most used multilevel-inverter topologies, including the neutral-point-clamped, cascaded, H-bridge, and flying-capacitor converters. This paper presents the operating principle of each topology and a review of the most relevant modulation methods, focused mainly on those used by industry. In addition, the latest advances and future trends of the technology are discussed. It is concluded that the topology and modulation-method selection are closely related to each particular application, leaving a space on the market for all the different solutions, depending on their unique features and limitations like power or voltage level, dynamic performance, reliability, costs, and other technical specifications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
95,"Metamaterials are typically engineered by arranging a set of small scatterers or apertures in a regular array throughout a region of space, thus obtaining some desirable bulk electromagnetic behavior. The desired property is often one that is not normally found naturally (negative refractive index, near-zero index, etc.). Over the past ten years, metamaterials have moved from being simply a theoretical concept to a field with developed and marketed applications. Three-dimensional metamaterials can be extended by arranging electrically small scatterers or holes into a two-dimensional pattern at a surface or interface. This surface version of a metamaterial has been given the name metasurface (the term metafilm has also been employed for certain structures). For many applications, metasurfaces can be used in place of metamaterials. Metasurfaces have the advantage of taking up less physical space than do full three-dimensional metamaterial structures; consequently, metasurfaces offer the possibility of less-lossy structures. In this overview paper, we discuss the theoretical basis by which metasurfaces should be characterized, and discuss their various applications. We will see how metasurfaces are distinguished from conventional frequency-selective surfaces. Metasurfaces have a wide range of potential applications in electromagnetics (ranging from low microwave to optical frequencies), including: (1) controllable ""smart"" surfaces, (2) miniaturized cavity resonators, (3) novel wave-guiding structures, (4) angular-independent surfaces, (5) absorbers, (6) biomedical devices, (7) terahertz switches, and (8) fluid-tunable frequency-agile materials, to name only a few. In this review, we will see that the development in recent years of such materials and/or surfaces is bringing us closer to realizing the exciting speculations made over one hundred years ago by the work of Lamb, Schuster, and Pocklington, and later by Mandel'shtam and Veselago.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96,"In vivo loads acting at the hip joint have so F;rr only been measured in few patients and without detailed documentation of gait data. Such information is required to test and improve wear, strength and fixation stability of hip implants. Measurements of hip contact forces with instrumented implants and synchronous analysts of gait patterns and ground reaction forces were performed in four patients during the most frequent activities of daily living. From the individual data sets an average was calculated. The paper focuses on the loading of the femoral implant component but complete data are additionally stored on an associated compact disc. It contains complete gait and hip contact force data as well as calculated muscle activities during walking and stair climbing and the frequencies of daily activities observed in hip patients. The mechanical loading and function of the hip joint and proximal femur is thereby completely documented. The average patient loaded his hip joint with 238% BW (percent of body weight) when walking at about 4 km/h and with slightly less when standing on one leg. This is below the levels previously reported for two other patients (Bergmann et al., Clinical Biomechanics 26 (1993) 969-990). When climbing upstairs the joint contact force is 251% BW which is less than 260% BW when going downstairs. Inwards torsion of the implant is probably critical For the stem fixation. On average it is 23% larger when going upstairs than during normal level walking. The inter- and intra-individual variations during stair climbing are large and the highest torque values are 83% larger than during normal walking. Because the hip joint loading: during all other common activities of most hip patients are comparably small (except during stumbling), implants should mainly; be tested with loading conditions that mimic walking and stair climbing. (C) 2001 Elsevier Science Ltd. All rights reserved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97,"A novel robust adaptive controller for multi-input multi-output (MIMO) feedback linearizable nonlinear systems possessing unknown nonlinearities, capable of guaranteeing a prescribed performance, is developed in this paper. By prescribed performance we mean that the tracking error should converge to an arbitrarily small residual set, with convergence rate no less than a prespecified value, exhibiting a maximum overshoot less than a sufficiently small prespecified constant. Visualizing the prescribed performance characteristics as tracking error constraints, the key idea is to transform the ""constrained"" system into an equivalent ""unconstrained"" one, via an appropriately defined output error transformation. It is shown that stabilization of the ""unconstrained"" system is sufficient to solve the stated problem. Besides guaranteeing a uniform ultimate boundedness property for the transformed output error and the uniform boundedness for all other signals in the closed loop, the proposed robust adaptive controller is smooth with easily selected parameter values and successfully bypasses the loss of controllability issue. Simulation results on a two-link robot, clarify and verify the approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98,"We describe an automated method to locate and outline blood vessels in images of the ocular fundus, Such a tool should prove useful to eye care specialists for purposes of patient screening, treatment evaluation, and clinical study. Our method differs from previously known methods in that it uses local and global vessel features cooperatively to segment the vessel network. We evaluate our method using hand-labeled ground truth segmentations of 20 images. A plot of the operating characteristic shows that our method reduces false positives by as much as 15 times over basic thresholding of a matched filter response (MFR), at up to a 75% true positive rate. For a baseline, we also compared the ground truth against a second hand-labeling, yielding a 90% true positive and a 4% false positive detection rate, on average. These numbers suggest there is still room for a 15% true positive rate improvement, with the same false positive rate, over our method. We are making all our images and hand labelings publicly available for interested researchers to use in evaluating related methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
99,"In-band full-duplex (IBFD) operation has emerged as an attractive solution for increasing the throughput of wireless communication systems and networks. With IBFD, a wireless terminal is allowed to transmit and receive simultaneously in the same frequency band. This tutorial paper reviews the main concepts of IBFD wireless. One of the biggest practical impediments to IBFD operation is the presence of self-interference, i.e., the interference that the modem's transmitter causes to its own receiver. This tutorial surveys a wide range of IBFD self-interference mitigation techniques. Also discussed are numerous other research challenges and opportunities in the design and analysis of IBFD wireless systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
