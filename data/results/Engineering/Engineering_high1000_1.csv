Publication Type,Authors,Title,Abstract,DOI,ID,rule1,rule2,rule3,rule4,rule5,rule6,rule7,rule8,rule9,rule10,rule11,rule12,rule13,rule14,rule15,rule16,rule17,rule18,rule19,rule20,rule21,rule22,rule23,rule24,rule25,rule26,rule27,rule28,rule29,rule30,rule31,rule32
J,"Ren, SQ; He, KM; Girshick, R; Sun, J",Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,"State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.",10.1109/TPAMI.2016.2577031,0,yes,no,yes,yes,yes,no,yes,yes,no,yes,yes,no,yes,yes,no,yes,yes,yes,yes,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,,
J,"Wang, Z; Bovik, AC; Sheikh, HR; Simoncelli, EP",Image quality assessment: From error visibility to structural similarity,"Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.(1)",10.1109/TIP.2003.819861,1,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,yes,yes,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,no,yes,no,yes,yes,yes,
J,"Donoho, DL",Compressed sensing,"Suppose x is an unknown vector in R-m (a digital image or signal); we plan to measure n general linear functionals of x and then reconstruct. If x is known to be compressible by transform coding with a known transform, and we reconstruct via the nonlinear procedure defined here, the number of measurements n can be dramatically smaller than the size m. Thus, certain natural classes of images with m pixels need only n = O(m(1/4) log(5/2)(m)) nonadaptive nonpixel samples for faithful recovery, as opposed to the usual m pixel samples. More specifically, suppose x has a sparse representation in some orthonormal basis (e.g., wavelet, Fourier) or tight frame (e.g., curvelet, Gabor)-so the coefficients belong to an l(p), ball for O < p <= 1. The N most important coefficients in that expansion allow reconstruction with l(2) error O(N1/2-1/p). It is possible to design n = O (N log (m)) nonadaptive measurements allowing reconstruction with accuracy comparable to that attainable with direct knowledge of the N most important coefficients. Moreover, a good approximation to those N important coefficients is extracted from the n measurements by solving a linear program-Basis Pursuit in signal processing. The nonadaptive measurements have the character of ""random"" linear combinations of basis/frame elements. Our results use the notions of optimal recovery, of n-widths, and information-based complexity. We estimate the Gel'fand n-widths of l(p) balls in high-dimensional Euclidean space in the case 0 < p <= 1, and give a criterion identifying near-optimal subspaces for Gel'fand n-widths. We show that ""most"" subspaces are near-optimal, and show that convex optimization (Basis Pursuit) is a near-optimal way to extract information derived from these near-optimal subspaces.",10.1109/TIT.2006.871582,2,yes,yes,no,yes,yes,no,yes,no,no,yes,yes,no,no,no,no,yes,yes,yes,no,no,yes,no,yes,yes,yes,yes,no,yes,yes,no,no,
J,"Pan, SJ; Yang, QA",A Survey on Transfer Learning,"A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.",10.1109/TKDE.2009.191,3,yes,no,yes,yes,yes,no,yes,yes,no,yes,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,yes,yes,yes,yes,yes,
J,"Chen, LC; Papandreou, G; Kokkinos, I; Murphy, K; Yuille, AL","DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs","In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed ""DeepLab"" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.",10.1109/TPAMI.2017.2699184,4,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Candès, EJ; Romberg, J; Tao, T",Robust uncertainty principles:: Exact signal reconstruction from highly incomplete frequency information,"This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f is an element of C-N and a randomly chosen set of frequencies Q. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set Q? A typical result of this paper is as follows. Suppose that f is a superposition of vertical bar T vertical bar spikes f(t) = E-tau is an element of T f(tau)delta(t - tau) obeying vertical bar T vertical bar <= C-M (.) (logN)(-1 .) vertical bar ohm vertical bar for some constant C-M > 0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1 - O(N-m), f can be reconstructed exactly as the solution to the l(1) minimization problem min/g Sigma(N-1)/t=0 vertical bar g(t)vertical bar, s.t. (g) over cap(omega) = (f) over cap(omega) for al omega is an element of ohm In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for Cm which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of vertical bar T vertical bar spikes may be recovered by convex programming from almost every set of frequencies of size O(vertical bar T vertical bar (.) log N). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1 - O(N-M) would in general require a number of frequency samples at least proportional to vertical bar T vertical bar (.) log N. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples-provided that the number of jumps (discontinuities) obeys the condition above-by minimizing other convex functionals such as the total variation of f.",10.1109/TIT.2005.862083,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J,"Mirjalili, S; Mirjalili, SM; Lewis, A",Grey Wolf Optimizer,"This work proposes a new meta-heuristic called Grey Wolf Optimizer (GWO) inspired by grey wolves (Canis lupus). The GWO algorithm mimics the leadership hierarchy and hunting mechanism of grey wolves in nature. Four types of grey wolves such as alpha, beta, delta, and omega are employed for simulating the leadership hierarchy. In addition, the three main steps of hunting, searching for prey, encircling prey, and attacking prey, are implemented. The algorithm is then benchmarked on 29 well-known test functions, and the results are verified by a comparative study with Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), Differential Evolution (DE), Evolutionary Programming (EP), and Evolution Strategy (ES). The results show that the GWO algorithm is able to provide very competitive results compared to these well-known meta-heuristics. The paper also considers solving three classical engineering design problems (tension/compression spring, welded beam, and pressure vessel designs) and presents a real application of the proposed method in the field of optical engineering. The results of the classical engineering design problems and real application prove that the proposed algorithm is applicable to challenging problems with unknown search spaces. (C) 2013 Elsevier Ltd. All rights reserved.",10.1016/j.advengsoft.2013.12.007,6,yes,yes,no,yes,yes,yes,yes,no,no,no,yes,yes,yes,no,no,yes,yes,yes,yes,no,yes,no,no,yes,yes,no,yes,yes,yes,yes,no,
J,"Ojala, T; Pietikäinen, M; Mäenpää, T",Multiresolution gray-scale and rotation invariant texture classification with local binary patterns,"This paper presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed ""uniform,"" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the ""uniform"" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns. These operators characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measures that characterize the contrast of local image texture. The joint distributions of these orthogonal measures are shown to be very powerful tools for rotation invariant texture analysis.",10.1109/TPAMI.2002.1017623,7,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,yes,no,no,yes,yes,yes,yes,yes,no,yes,yes,
J,"Badrinarayanan, V; Kendall, A; Cipolla, R",SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,"We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.",10.1109/TPAMI.2016.2644615,8,yes,no,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,yes,yes,no,yes,yes,yes,no,yes,yes,
J,"Zhang, ZY",A flexible new technique for camera calibration,"We propose a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at Least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. it advances 3D computer vision one more step from laboratory environments to real world use. The corresponding software is available from the author's Web page.",10.1109/34.888718,9,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,no,yes,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,
J,"Shi, JB; Malik, J",Normalized cuts and image segmentation,"We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.",10.1109/34.868688,10,yes,yes,no,no,yes,yes,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,no,yes,no,no,no,no,yes,no,no,no,no,yes,yes,
J,"Akyildiz, IF; Su, W; Sankarasubramaniam, Y; Cayirci, E",Wireless sensor networks: a survey,"This paper describes the concept of sensor networks which has been made viable by the convergence of micro-electro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are also discussed. (C) 2002 Published by Elsevier Science B.V.",10.1016/S1389-1286(01)00302-4,11,yes,yes,no,yes,no,no,no,no,yes,no,no,yes,no,no,no,no,no,no,no,yes,no,no,no,no,no,no,yes,no,yes,yes,no,no
J,"Olfati-Saber, R; Murray, RM",Consensus problems in networks of agents with switching topology and time-delays,"In this paper, we discuss consensus problems for networks of dynamic agents with fixed and switching topologies. We analyze three cases: 1) directed networks with fixed topology; 2) directed networks with switching topology; and 3) undirected networks with communication time-delays and fixed topology. We introduce two consensus protocols for networks with and without time-delays and provide a convergence analysis in all three cases. We establish a direct connection between the algebraic connectivity (or Fiedler eigenvalue) of the network and the performance or negotiation speed) of a linear consensus protocol. This required the generalization of the notion of algebraic connectivity of undirected graphs to digraphs. It turns out that balanced digraphs play a key role in addressing average-consensus problems. We introduce disagreement functions for convergence analysis of consensus protocols. A disagreement function is a Lyapunov function for the disagreement network dynamics. We proposed a simple disagreement function that is a common Lyapunov function for the disagreement dynamics of a directed network with switching topology. A distinctive feature of this work is to address consensus problems for networks with directed information flow. We provide analytical tools that rely on algebraic graph theory, matrix theory, and control theory. Simulations are provided that demonstrate the effectiveness of our theoretical results.",10.1109/TAC.2004.834113,12,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,yes,no,no,no,yes,yes,yes,yes,no,yes,no,no,yes,yes,yes,yes,no,no,yes,yes,
J,"Laneman, JN; Tse, DNC; Wornell, GW",Cooperative diversity in wireless networks: Efficient protocols and outage behavior,"We develop and analyze low-complexity cooperative diversity protocols that combat fading induced by multipath propagation in wireless networks. The underlying techniques exploit space diversity available through cooperating terminals' relaying signals for one another. We outline several strategies employed by the cooperating radios, including fixed relaying schemes such as amplify-and-forward and decode-and-forward, selection relaying schemes that adapt based upon channel measurements between the cooperating terminals, and incremental relaying schemes that adapt based upon limited feedback from the destination terminal. We develop performance characterizations in terms of outage events and associated outage probabilities, which measure robustness of the transmissions to fading, focusing on the high signal-to-noise ratio (SNR) regime. Except for fixed decode-and-forward, all of our cooperative diversity protocols are efficient in the sense that they achieve full diversity (i.e., second-order diversity in the case of two terminals), and, moreover, are close to optimum (within 1.5 dB) in certain regimes. Thus, using distributed antennas, we can provide the powerful benefits of space diversity without need for physical arrays, though at a loss of spectral efficiency due to half-duplex operation and possibly at the cost of additional receive hardware. Applicable to any wireless setting, including cellular or ad hoc networks-wherever space constraints preclude the use of physical arrays-the performance characterizations reveal that large power or energy savings result from the use of these protocols.",10.1109/TIT.2004.838089,13,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,yes,no,no,no,yes,yes,yes,yes,no,yes,no,yes,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Mirjalili, S; Lewis, A",The Whale Optimization Algorithm,"This paper proposes a novel nature-inspired meta-heuristic optimization algorithm, called Whale Optimization Algorithm (WOA), which mimics the social behavior of humpback whales. The algorithm is inspired by the bubble-net hunting strategy. WOA is tested with 29 mathematical optimization problems and 6 structural design problems. Optimization results prove that the WOA algorithm is very competitive compared to the state-of-art meta-heuristic algorithms as well as conventional methods. The source codes of the WOA algorithm are publicly available at hrtp://www.alimirjalili,com/WOA.html (C) 2016 Elsevier Ltd. All rights reserved.",10.1016/j.advengsoft.2016.01.008,14,yes,yes,no,no,yes,yes,yes,yes,no,no,yes,no,yes,no,no,no,yes,yes,yes,no,no,no,yes,yes,yes,yes,yes,no,no,yes,yes,
J,"Haykin, S",Cognitive radio: Brain-empowered wireless communications,"Cognitive radio is viewed as a novel approach for improving the utilization of a precious natural resource: the radio electromagnetic spectrum. The cognitive radio, built on a software-defined radio, is defined as an intelligent wireless communication system that is aware of M environment and uses the methodology of understanding-by-building to learn from the environment and adapt to statistical variations in the input stimuli, with two primary objectives in mind: highly reliable communication whenever and wherever needed; efficient utilization of the radio spectrum. Following the discussion of interference temperature as a new metric for the quantification and management of interference, the paper addresses three fundamental cognitive tasks. 1) Radio-scene analysis. 2) Channel-state estimation and predictive modeling. 3) Transmit-power control and dynamic spectrum management. This paper also discusses the emergent behavior of cognitive radio.",10.1109/JSAC.2004.839380,15,yes,yes,no,no,yes,no,yes,yes,no,no,yes,no,no,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,yes,
J,"Bay, H; Ess, A; Tuytelaars, T; Van Gool, L",Speeded-Up Robust Features (SURF),"This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. (C) 2007 Elsevier Inc. All rights reserved.",10.1016/j.cviu.2007.09.014,16,yes,yes,no,yes,yes,yes,yes,no,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Arulampalam, MS; Maskell, S; Gordon, N; Clapp, T",A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking,"Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system: Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or ""particle"") representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods:. Several variants of the particle filter such as SIR, ASIR; and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example.",10.1109/78.978374,17,yes,no,yes,yes,no,no,no,no,no,no,yes,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,yes,,
J,"Comaniciu, D; Meer, P",Mean shift: A robust approach toward feature space analysis,"A general nonparametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure, the mean shift. We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators of location is also established. Algorithms for two low-level vision tasks, discontinuity preserving smoothing and image segmentation, are described as applications. In these algorithms, the only user set parameter is the resolution of the analysis and either gray level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.",10.1109/34.1000236,18,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,no,yes,yes,no,no,no,yes,,
J,"Chan, TF; Vese, LA",Active contours without edges,"In this paper, we propose a new model for active contours to detect objects in a given image, based on techniques of curve evolution, Mumford-Shah functional for segmentation and level sets. Our model can detect objects whose boundaries are not necessarily defined by gradient. We minimize an energy which can he seen as a particular case of the minimal partition problem, In the level set formulation, the problem becomes a ""mean-curvature flow""-like evolving the active contour, which will stop on the desired boundary. However, the stopping term does not depend on the gradient of the. image, as in the classical active contour models, hut is instead related to a particular segmentation of the image. We will give a numerical algorithm using finite differences. Finally, we will present various experimental results and in particular some examples for which the classical snakes methods based on the gradient are not applicable. Also, the initial curve can be anywhere in the image, and interior contours are automatically detected.",10.1109/83.902291,19,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,no,yes,no,no,no,no,yes,yes,
J,"Akyildiz, IF; Su, WL; Sankarasubramaniam, Y; Cayirci, E",A survey on sensor networks,"Recent advancement in wireless communications and electronics has enabled the development of low-cost sensor networks. The sensor networks can be used for various application areas (e.g., health, military, home). For different application areas, there are different technical issues that researchers are currently resolving. The current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. This article also points out the open research issues and intends to spark new interests and developments in this field.",10.1109/MCOM.2002.1024422,20,yes,yes,no,yes,yes,no,no,yes,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,yes,no,yes,no,yes,
J,"Kokubo, T; Takadama, H",How useful is SBF in predicting in vivo bone bioactivity?,"The bone-bonding ability of a material is often evaluated by examining the ability of apatite to form on its surface in a simulated body fluid (SBF) with ion concentrations nearly equal to those of human blood plasma. However, the validity of this method for evaluating bone-bonding ability has not been assessed systematically. Here, the history of SBF, correlation of the ability of apatite to form oil various materials in SBF with their in vivo bone bioactivities, and some examples of the development of novel bioactive materials based on apatite formation in SBF are reviewed. It was concluded that examination of apatite formation on a material in SBF is useful for predicting the in vivo bone bioactivity of a material, and the number of animals used in and the duration of animal experiments call be reduced remarkably by using this method. (c) 2006 Elsevier Ltd. All rights reserved.",10.1016/j.biomaterials.2006.01.017,21,yes,yes,no,yes,yes,no,yes,no,yes,yes,yes,yes,no,no,no,no,yes,yes,no,yes,no,no,no,no,yes,no,yes,no,yes,yes,,
J,"Wright, J; Yang, AY; Ganesh, A; Sastry, SS; Ma, Y",Robust Face Recognition via Sparse Representation,"We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by l(1)-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as Eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.",10.1109/TPAMI.2008.79,22,yes,yes,no,yes,yes,no,yes,no,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,no,yes,no,no,yes,yes,
J,"Aharon, M; Elad, M; Bruckstein, A",K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation,"In recent years there has been a growing interest in the study of sparse representation of signals. Using an over-complete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method-the K-SVD algorithm-generalizing the K-means clustering process. K-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The K-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data.",10.1109/TSP.2006.881199,23,yes,no,no,yes,yes,no,yes,yes,no,no,yes,no,yes,no,no,no,yes,no,yes,no,no,no,no,no,no,no,yes,no,yes,yes,yes,
J,"Candès, EJ; Wakin, MB",An introduction to compressive sampling,,10.1109/MSP.2007.914731,24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J,"Peng, HC; Long, FH; Ding, C","Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy","Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e. g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.",10.1109/TPAMI.2005.159,25,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,yes,yes,no,no,yes,yes,yes,yes,no,no,no,no,yes,no,no,yes,no,no,yes,yes,
J,"Litjens, G; Kooi, T; Bejnordi, BE; Setio, AAA; Ciompi, F; Ghafoorian, M; van der Laak, JAWM; van Ginneken, B; Sánchez, CI",A survey on deep learning in medical image analysis,"Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research. (C) 2017 Elsevier B.V. All rights reserved.",10.1016/j.media.2017.07.005,26,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,no,yes,no,no,no,no,yes,no,yes,yes,no,yes,yes,yes,
J,"Atzori, L; Iera, A; Morabito, G",The Internet of Things: A survey,"This paper addresses the Internet of Things. Main enabling factor of this promising paradigm is the integration of several technologies and communications solutions. Identification and tracking technologies, wired and wireless sensor and actuator networks, enhanced communication protocols (shared with the Next Generation Internet), and distributed intelligence for smart objects are just the most relevant. As one can easily imagine. any serious contribution to the advance of the Internet of Things must necessarily be the result of synergetic activities conducted in different fields of knowledge, such as telecommunications, informatics, electronics and social science. In such a complex scenario, this survey is directed to those who want to approach this complex discipline and contribute to its development. Different visions of this Internet of Things paradigm are reported and enabling technologies reviewed. What emerges is that still major issues shall be faced by the research community. The most relevant among them are addressed in details. (C) 2010 Elsevier B.V. All rights reserved.",10.1016/j.comnet.2010.05.010,27,yes,yes,no,yes,yes,no,no,yes,no,no,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,no,no,yes,yes,no,no,no,yes,,
J,"Tropp, JA; Gilbert, AC",Signal recovery from random measurements via orthogonal matching pursuit,"This paper demonstrates theoretically and empirically that a greedy algorithm called Orthogonal Matching Pursuit (OMP) can reliably recover a signal with m nonzero entries in dimension d given O(m 1n d) random linear measurements of that signal. This is a massive improvement over previous results, which require O(m(2)) measurements. The new results for OMP are comparable with recent results for another approach called Basis Pursuit (BP). In some settings, the OMP algorithm is faster and easier to implement, so it is an attractive alternative to BP for signal recovery problems.",10.1109/TIT.2007.909108,28,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Achanta, R; Shaji, A; Smith, K; Lucchi, A; Fua, P; Süsstrunk, S",SLIC Superpixels Compared to State-of-the-Art Superpixel Methods,"Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.",10.1109/TPAMI.2012.120,29,yes,yes,no,yes,yes,no,yes,no,yes,no,yes,yes,no,no,no,yes,yes,yes,yes,no,no,yes,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Kolpin, DW; Furlong, ET; Meyer, MT; Thurman, EM; Zaugg, SD; Barber, LB; Buxton, HT","Pharmaceuticals, hormones, and other organic wastewater contaminants in US streams, 1999-2000: A national reconnaissance","To provide the first nationwide reconnaissance of the occurrence of pharmaceuticals, hormones, and other organic wastewater contaminants (OWCs) in water resources, the U.S. Geological Survey used five newly developed analytical methods to measure concentrations of 95 OWCs in water samples from a network of 139 streams across 30 states during 1999 and 2000. The selection of sampling sites was biased toward streams susceptible to contamination (i.e. downstream of intense urbanization and livestock production). OWCs were prevalent during this study, being found in 80% of the streams sampled. The compounds detected represent a wide range of residential, industrial, and agricultural origins and uses with 82 of the 95 OWCs being found during this study. The most frequently detected compounds were coprostanol (fecal steroid), cholesterol (plant and animal steroid), N,N-diethyltoluamide (insect repellant), caffeine (stimulant), triclosan (antimicrobial disinfectant), tri(2-chloroethyl)phosphate (fire retardant), and 4-nonylphenol (nonionic detergent metabolite). Measured concentrations for this study were generally low and rarely exceeded drinking-water guidelines, drinking-water health advisories, or aquatic-life criteria. Many compounds, however, do not have such guidelines established. The detection of multiple OWCs was common for this study, with a median of seven and as many as 38 OWCs being found in a given water sample. Little is known about the potential interactive effects (such as synergistic or antagonistic toxicity) that may occur from complex mixtures of OWCs in the environment. In addition, results of this study demonstrate the importance of obtaining data on metabolites to fully understand not only the fate and transport of OWCs in the hydrologic system but also their ultimate overall effect on human health and the environment.",10.1021/es011055j,30,yes,no,no,no,yes,yes,yes,no,yes,no,yes,yes,yes,no,yes,yes,yes,yes,no,yes,no,no,yes,yes,yes,yes,no,no,yes,yes,no,
J,"Dabov, K; Foi, A; Katkovnik, V; Egiazarian, K",Image denoising by sparse 3-D transform-domain collaborative filtering,"We propose a novel image denoising strategy based on an enhanced sparse representation in transform domain. The enhancement of the sparsity is achieved by grouping similar 2-D image fragments (e.g., blocks) into 3-D data arrays which we call ""groups."" Collaborative filtering is a special procedure developed to deal with these 3-D groups. We realize it using the three successive steps: 3-D transformation of a group, shrinkage of the transform spectrum, and inverse 3-D transformation. The result is a 3-D estimate that consists of the jointly filtered grouped image blocks. By attenuating the noise, the collaborative filtering reveals even the finest details shared by grouped blocks and, at the same time, it preserves the essential unique features of each individual block. The filtered blocks are then returned to their original positions. Because these blocks are overlapping, for each pixel, we obtain many different estimates which need to be combined. Aggregation is a particular averaging procedure which is exploited to take advantage of this redundancy. A significant improvement is obtained by a specially developed collaborative Wiener filtering. An algorithm based on this novel denoising strategy and its efficient implementation are presented in full detail; an extension to color-image denoising is also developed. The experimental results demonstrate that this computationally scalable algorithm achieves state-of-the-art denoising performance in terms of both peak signal-to-noise ratio and subjective visual quality.",10.1109/TIP.2007.901238,31,yes,no,no,no,yes,yes,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,no,yes,yes,,
J,"Jadbabaie, A; Lin, J; Morse, AS",Coordination of groups of mobile autonomous agents using nearest neighbor rules,"In a recent Physical Review Letters article, Vicsek et al. propose a simple but compelling discrete-time model of n autonomous agents (i.e., points or particles) all moving in the plane with the same speed but with different headings. Each agent's heading is updated using a local rule based on the average of its own heading plus the headings of its ""neighbors."" In their paper, Vicsek et al. provide simulation results which demonstrate that the nearest neighbor rule they are studying can cause all agents to eventually move in the same direction despite the absence of centralized coordination and despite the fact that each agent's set of nearest neighbors change with time as the system evolves. This paper provides a theoretical explanation for this observed behavior. In addition, convergence results are derived for several other similarly inspired models. The Vicsek model proves to be a graphic example of a switched linear system which is stable, but for which there does not exist a common quadratic Lyapunov function.",10.1109/TAC.2003.812781,32,yes,yes,no,yes,yes,no,no,yes,no,no,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,no,yes,no,yes,yes,no,
J,"Heinzelman, WB; Chandrakasan, AP; Balakrishnan, H",An application-specific protocol architecture for wireless microsensor networks,"Networking together hundreds or thousands of cheap microsensor nodes allows users to accurately monitor a remote environment by intelligently combining the data from the individual nodes. These networks require robust wireless communication protocols that are energy efficient and provide low latency. In this paper, we develop and analyze low-energy adaptive clustering hierarchy (LEACH), a protocol architecture for microsensor networks that combines the ideas of energy-efficient cluster-based routing and media access together with application-specific data aggregation to achieve good performance in terms of system lifetime, latency, and application-perceived quality. LEACH includes a new, distributed cluster formation technique that enables self-organization of large numbers of nodes, algorithms for adapting clusters and rotating cluster head positions to evenly distribute the energy load among all the nodes, and techniques to enable distributed signal processing to save communication resources. Our results show that LEACH can improve system lifetime by an order of magnitude compared with general-purpose multihop approaches.",10.1109/TWC.2002.804190,33,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,yes,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,no,no,yes,no,yes,yes,yes,
J,"Dong, C; Loy, CC; He, KM; Tang, XO",Image Super-Resolution Using Deep Convolutional Networks,"We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.",10.1109/TPAMI.2015.2439281,34,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Andrews, JG; Buzzi, S; Choi, W; Hanly, SV; Lozano, A; Soong, ACK; Zhang, JC",What Will 5G Be?,"What will 5G be? What it will not be is an incremental advance on 4G. The previous four generations of cellular technology have each been a major paradigm shift that has broken backward compatibility. Indeed, 5G will need to be a paradigm shift that includes very high carrier frequencies with massive bandwidths, extreme base station and device densities, and unprecedented numbers of antennas. However, unlike the previous four generations, it will also be highly integrative: tying any new 5G air interface and spectrum together with LTE and WiFi to provide universal high-rate coverage and a seamless user experience. To support this, the core network will also have to reach unprecedented levels of flexibility and intelligence, spectrum regulation will need to be rethought and improved, and energy and cost efficiencies will become even more critical considerations. This paper discusses all of these topics, identifying key challenges for future research and preliminary 5G standardization activities, while providing a comprehensive overview of the current literature, and in particular of the papers appearing in this special issue.",10.1109/JSAC.2014.2328098,35,yes,no,no,yes,yes,no,no,no,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,no,no,yes,no,no,
J,"Buongiorno, J",Convective transport in nanofluids,"Nanofluids are engineered colloids made of a base fluid and nanoparticles (1 - 100 nm). Nanofluids have higher thermal conductivity and single-phase heat transfer coefficients than their base fluids. In particular the heat transfer coefficient increases appear to go beyond the mere thermal-conductivity effect, and cannot be predicted by traditional pure-fluid correlations such as Dittus-Boelter's. In the nanofluid literature this behavior is generally attributed to thermal dispersion and intensified turbulence, brought about by nanoparticle motion. To test the validity of this assumption, we have considered seven slip mechanisms that can produce a relative velocity between the nanoparticles and the base fluid. These are inertia, Brownian diffusion, thermophoresis, diffusiophoresis, Magnus effect, fluid drainage, and gravity. We concluded that, of these seven, only Brownian diffusion and thermophoresis are important slip mechanisms in nanofluids. Based on this finding, we developed a two-component four-equation nonhomogeneous equilibrium model for mass, momentum, and heat transport in nanofluids. A nondimensional analysis of the equations suggests that energy transfer by nanoparticle dispersion is negligible, and thus cannot explain the abnormal heat transfer coefficient increases. Furthermore, a comparison of the nanoparticle and turbulent eddy time and length scales clearly indicates that the nanoparticles move homogeneously with the fluid in the presence of turbulent eddies, so an effect on turbulence intensity is also doubtful. Thus, we propose an alternative explanation for the abnormal heat transfer coefficient increases: the nanofluid properties may vary significantly within the boundary layer because of the effect of the temperature gradient and thermophoresis. For a heated fluid, these effects can result in a significant decrease of viscosity within the boundary layer thus leading to heat transfer enhancement. A correlation structure that captures these effects is proposed.",10.1115/1.2150834,36,yes,no,yes,yes,yes,no,yes,no,yes,yes,yes,yes,no,no,no,no,yes,yes,no,yes,yes,yes,no,no,yes,yes,yes,no,yes,yes,,
J,"Yang, HP; Yan, R; Chen, HP; Lee, DH; Zheng, CG","Characteristics of hemicellulose, cellulose and lignin pyrolysis","The pyrolysis characteristics of three main components (hemicellulose, cellulose and lignin) of biomass were investigated using, respectively, a thermogravimetric analyzer (TGA) with differential scanning calorimetry (DSC) detector and a pack bed. The releasing of main gas products from biomass pyrolysis in TGA was on-line measured using Fourier transform infrared (FTIR) spectroscopy. In thermal analysis, the pyrolysis of hemicellulose and cellulose occurred quickly, with the weight loss of hemicellulose mainly happened at 220-315 degrees C and that of cellulose at 315-400 degrees C. However, lignin was more difficult to decompose, as its weight loss happened in a wide temperature range (from 160 to 900 degrees C) and the generated solid residue was very high (similar to 40 wt.%). From the viewpoint of energy consumption in the course of pyrolysis, cellulose behaved differently from hemicellulose and lignin; the pyrolysis of the former was endothermic while that of the latter was exothermic. The main gas products from pyrolyzing the three components were similar, including CO2, CO, CH4 and some organics. The releasing behaviors of H-2 and the total gas yield were measured using Micro-GC when pyrolyzing the three components in a packed bed. It was observed that hemicellulose had higher CO2 yield, cellulose generated higher CO yield, and lignin owned higher H-2 and CH4 yield. A better understanding to the gas products releasing from biomass pyrolysis could be achieved based on this in-depth investigation on three main biomass components. (c) 2006 Elsevier Ltd. All rights reserved.",10.1016/j.fuel.2006.12.013,37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J,"Wiegand, T; Sullivan, GJ; Bjontegaard, G; Luthra, A",Overview of the H.264/AVC video coding standard,"H.264/AVC is newest video coding standard of the ITU-T Video Coding Experts Group and the ISO/IEC Moving Picture Experts Group. The main goals of the H.264/AVC standardization effort have been enhanced compression performance and provision of a ""network-friendly"" video representation addressing ""conversational"" (video telephony) and ""nonconversational"" (storage, broadcast, or streaming) applications. H.264/AVC has achieved a significant improvement in rate-distortion efficiency relative-to existing standards. This article provides an overview of the technical features of H.264/AVC, describes profiles and applications for the standard, and outlines the history of the standardization process.",10.1109/TCSVT.2003.815165,38,yes,yes,no,yes,yes,no,yes,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,no,
J,"Sullivan, GJ; Ohm, JR; Han, WJ; Wiegand, T",Overview of the High Efficiency Video Coding (HEVC) Standard,High Efficiency Video Coding (HEVC) is currently being prepared as the newest video coding standard of the ITU-T Video Coding Experts Group and the ISO/IEC Moving Picture Experts Group. The main goal of the HEVC standardization effort is to enable significantly improved compression performance relative to existing standards-in the range of 50% bit-rate reduction for equal perceptual video quality. This paper provides an overview of the technical features and characteristics of the HEVC standard.,10.1109/TCSVT.2012.2221191,39,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,no,yes,no,no,no,no,yes,yes,,
J,"Bianchi, G","Performance analysis,of the IEEE 802.11 distributed coordination function","Recently, the IEEE has standardized the 802.11 protocol for Wireless Local Area Networks. The primary medium access control (MAC) technique of 802.11 is called distributed coordination function (DCF), DCF is a carrier sense multiple access with collision avoidance (CSMA/CA) scheme with binary slotted exponential backoff. This paper provides a simple, but nevertheless extremely accurate, analytical model to compute the 802.11 DCF throughput, in the assumption of finite number of terminals and ideal channel conditions. The proposed analysis applies to both the packet transmission schemes employed by DCF, namely, the basic access and the RTS/CTS access mechanisms. In addition, it also applies to a combination of the two schemes, in which packets longer than a given threshold are transmitted according to the RTS/CTS mechanism. By means of the proposed model, in this paper we provide an extensive throughput performance evaluation of both access mechanisms of the 802.11 protocol.",10.1109/49.840210,40,yes,yes,no,no,yes,no,yes,yes,no,no,yes,no,no,no,no,yes,yes,no,yes,no,no,no,no,no,no,yes,yes,no,no,yes,yes,
J,"Jenkinson, M; Smith, S",A global optimisation method for robust affine registration of brain images,"Registration is an important component of medical image analysis and for analysing large amounts of data it is desirable to have fully automatic registration methods. Many different automatic registration methods have been proposed to date, and almost all share a common mathematical framework - one of optimising a cost function. To date little attention has been focused on the optimisation method itself, even though the success of most registration methods hinges on the quality of this optimisation. This paper examines the assumptions underlying the problem of registration for brain images using inter-modal voxel similarity measures. It is demonstrated that the use of local optimisation methods together with the standard multi-resolution approach is not sufficient to reliably find the global minimum. To address this problem, a global optimisation method is proposed that is specifically tailored to this form of registration. A full discussion of all the necessary implementation details is included as this is an important part of any practical method. Furthermore, results are presented for inter-modal, inter-subject registration experiments that show that the proposed method is more reliable at finding the global minimum than several of the currently available registration packages in common usage. (C) 2001 Elsevier Science B.V. All rights reserved.",10.1016/S1361-8415(01)00036-6,41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J,"Gupta, P; Kumar, PR",The capacity of wireless networks,"When n identical randomly located nodes, each capable of transmitting at W bits per second and using a fixed range, form a wireless network, the throughput lambda(n) obtainable by each node for a randomly chosen destination is Theta (W/root n log n) bits per second under a noninterference protocol. If the nodes are optimally placed in a disk of unit area, traffic patterns are optimally assigned, and each transmission's range is optimally chosen, the bit-distance product that can be transported by the network per second is Theta(W root An) bit-meters per second. Thus even under optimal circumstances, the throughput is only Theta(W/root n) bits per second for each node for a destination nonvanishingly far away, Similar results also hold under an alternate physical model where a required signal-to-interference ratio is specified for successful receptions. Fundamentally, it is the need for every node all over the domain to share whatever portion of the channel it is utilizing with nodes in its local neighborhood that is the reason for the constriction in capacity, Splitting the channel into several subchannels does not change any of the results. Some implications may be worth considering by designers. Since the throughput furnished to each user diminishes to zero as the number of users is increased, perhaps networks connecting smaller numbers of users, or featuring connections mostly with nearby neighbors, may be more likely to be find acceptance.",10.1109/18.825799,42,yes,yes,no,no,yes,no,yes,no,no,no,yes,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,no,yes,no,yes,yes,no,no,
J,"Ahlswede, R; Cai, N; Li, SYR; Yeung, RW",Network information flow,"We introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be mulitcast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. In this paper, we study the problem with one information source, and we have obtained a simple characterization of the admissible coding rate region. Our result can be regarded as the Max-flow Min-cut Theorem for network information flow. Contrary to one's intuition, our work reveals that it is in general not optimal to regard the information to be multicast as a ""fluid'' which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding, bandwidth can in general be saved. This finding may have significant impact on future design of switching systems.",10.1109/18.850663,43,yes,no,no,yes,yes,no,yes,yes,yes,no,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,no,no,yes,yes,yes,yes,yes,yes,,
J,"Ren, W; Beard, RW",Consensus seeking in multiagent systems under dynamically changing interaction topologies,This note considers the problem of information consensus among multiple agents in the presence of limited and unreliable information exchange with dynamically changing interaction topologies. Both discrete and continuous update schemes are proposed for information consensus. This note shows that information consensus under dynamically changing interaction topologies can be achieved asymptotically if the union of the directed interaction graphs have a spanning tree frequently enough as the system evolves.,10.1109/TAC.2005.846556,44,yes,yes,no,no,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,no,no,no,no,no,no,yes,yes,
J,"Rappaport, TS; Sun, S; Mayzus, R; Zhao, H; Azar, Y; Wang, K; Wong, GN; Schulz, JK; Samimi, M; Gutierrez, F",Millimeter Wave Mobile Communications for 5G Cellular: It Will Work!,"The global bandwidth shortage facing wireless carriers has motivated the exploration of the underutilized millimeter wave (mm-wave) frequency spectrum for future broadband cellular communication networks. There is, however, little knowledge about cellular mm-wave propagation in densely populated indoor and outdoor environments. Obtaining this information is vital for the design and operation of future fifth generation cellular networks that use the mm-wave spectrum. In this paper, we present the motivation for new mm-wave cellular systems, methodology, and hardware for measurements and offer a variety of measurement results that show 28 and 38 GHz frequencies can be used when employing steerable directional antennas at base stations and mobile devices.",10.1109/ACCESS.2013.2260813,45,yes,no,yes,no,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,yes,yes,yes,no,no,no,yes,yes,no,no,no,no,no,yes,yes,
J,"Hsu, CW; Lin, CJ",A comparison of methods for multiclass support vector machines,"Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such ""all-together"" methods. We then compare their performance with three methods based on binary classifications: ""one-against-all,"" ""one-against-one,"" and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the ""one-against-one"" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors.",10.1109/72.991427,46,yes,no,yes,yes,yes,no,yes,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,no,yes,no,yes,yes,yes,
J,"Zhang, YY; Brady, M; Smith, S",Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm,"The finite mixture (FM) model is the most commonly used model for statistical segmentation of brain magnetic resonance (MR) images because of its simple mathematical form and the piecewise constant nature of ideal brain MR images. However, being a histogram-based model, the FM has an intrinsic limitation-no spatial information is taken into account. This causes the FM model to work only on well-defined images with low levels of noise; unfortunately, this is often not the the case due to artifacts such as partial volume effect and bias field distortion. Under these conditions, FM model-based methods produce unreliable results. In this paper, we propose a novel hidden Markov random held (HMRF) model, which is a stochastic process generated by a MRF whose state sequence cannot be observed directly but which can be indirectly estimated through observations. Mathematically, it can be shown that the FM model is a degenerate version of the HMRF model. The advantage of the HMRF model derives from the way in which the spatial information is encoded through the mutual influences of neighboring sites. Although MRF modeling has been employed in MR image segmentation by other researchers, most reported methods are limited to using MRF as a general prior in an FM model-based approach. To fit the HMRF model, an EM algorithm is used, We show that by incorporating both the HMRF model and the Ehl algorithm into a HMRF-EM framework, an accurate and robust segmentation can be achieved. More importantly, the HMRF-EM framework can easily be combined with other techniques. As an example, we show how the bias held correction algorithm of Guillemaud and Brady (1997) can be incorporated into this framework to achieve a three-dimensional fully automated approach for brain MR image segmentation.",10.1109/42.906424,47,yes,no,yes,yes,yes,no,yes,no,yes,yes,yes,yes,no,no,no,no,no,no,no,yes,no,yes,no,no,no,yes,no,yes,yes,yes,yes,
J,"Zhang, K; Zuo, WM; Chen, YJ; Meng, DY; Zhang, L",Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,"The discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable denoising performance. In this paper, we take one step forward by investigating the construction of feed-forward denoising convolutional neural networks (DnCNNs) to embrace the progress in very deep architecture, learning algorithm, and regularization method into image denoising. Specifically, residual learning and batch normalization are utilized to speed up the training process as well as boost the denoising performance. Different from the existing discriminative denoising models which usually train a specific model for additive white Gaussian noise at a certain noise level, our DnCNN model is able to handle Gaussian denoising with unknown noise level (i.e., blind Gaussian denoising). With the residual learning strategy, DnCNN implicitly removes the latent clean image in the hidden layers. This property motivates us to train a single DnCNN model to tackle with several general image denoising tasks, such as Gaussian denoising, single image super-resolution, and JPEG image deblocking. Our extensive experiments demonstrate that our DnCNN model can not only exhibit high effectiveness in several general image denoising tasks, but also be efficiently implemented by benefiting from GPU computing.",10.1109/TIP.2017.2662206,48,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,,
J,"Candes, EJ; Tao, T",Decoding by linear programming,"This paper considers a natural error correcting problem with real valued input/output. We wish to recover an input vector f is an element of R-n from corrupted measurements y = Af + e. Here, A is an m by n (coding) matrix and e is an arbitrary and unknown vector of errors. Is it possible to recover f exactly from the data y?",10.1109/TIT.2005.858979,49,yes,yes,no,no,yes,no,yes,yes,no,no,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,no,,
J,"Boykov, Y; Veksler, O; Zabih, R",Fast approximate energy minimization via graph cuts,"Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. In this paper, we consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy.",10.1109/34.969114,50,yes,no,no,yes,yes,no,yes,yes,no,no,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,no,yes,no,yes,yes,yes,no,yes,yes,
J,"Dragomiretskiy, K; Zosso, D",Variational Mode Decomposition,"During the late 1990s, Huang introduced the algorithm called Empirical Mode Decomposition, which is widely used today to recursively decompose a signal into different modes of unknown but separate spectral bands. EMD is known for limitations like sensitivity to noise and sampling. These limitations could only partially be addressed by more mathematical attempts to this decomposition problem, like synchrosqueezing, empirical wavelets or recursive variational decomposition. Here, we propose an entirely non-recursive variational mode decomposition model, where the modes are extracted concurrently. The model looks for an ensemble of modes and their respective center frequencies, such that the modes collectively reproduce the input signal, while each being smooth after demodulation into baseband. In Fourier domain, this corresponds to a narrow-band prior. We show important relations to Wiener filter denoising. Indeed, the proposed method is a generalization of the classic Wiener filter into multiple, adaptive bands. Our model provides a solution to the decomposition problem that is theoretically well founded and still easy to understand. The variational model is efficiently optimized using an alternating direction method of multipliers approach. Preliminary results show attractive performance with respect to existing mode decomposition models. In particular, our proposed model is much more robust to sampling and noise. Finally, we show promising practical decomposition results on a series of artificial and real data.",10.1109/TSP.2013.2288675,51,yes,no,no,yes,yes,no,yes,yes,no,yes,yes,no,yes,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Allison, J; Amako, K; Apostolakis, J; Araujo, H; Dubois, PA; Asai, M; Barrand, G; Capra, R; Chauvie, S; Chytracek, R; Cirrone, GAP; Cooperman, G; Cosmo, G; Cuttone, G; Daquino, GG; Donszelmann, M; Dressel, M; Folger, G; Foppiano, F; Generowicz, J; Grichine, V; Guatelli, S; Gumplinger, P; Heikkinen, A; Hrivnacova, I; Howard, A; Incerti, S; Ivanchenko, V; Johnson, T; Jones, F; Koi, T; Kokoulin, R; Kossov, M; Kurashige, H; Lara, V; Larsson, S; Lei, F; Link, O; Longo, F; Maire, M; Mantero, A; Mascialino, B; McLaren, I; Lorenzo, PM; Minamimoto, K; Murakami, K; Nieminen, P; Pandola, L; Parlati, S; Peralta, L; Perl, J; Pfeiffer, A; Pia, MG; Ribon, A; Rodrigues, P; Russo, G; Sadilov, S; Santin, G; Sasaki, T; Smith, D; Starkov, N; Tanaka, S; Tcherniaev, E; Tomé, B; Trindade, A; Truscott, P; Urban, L; Verderi, M; Walkden, A; Wellisch, JP; Williams, DC; Wright, D; Yoshida, H",Geant4 developments and applications,"Geant4 is a software toolkit for the simulation of the passage of particles through matter. It is used by a large number of experiments and projects in a variety of application domains, including high energy physics, astrophysics and space science, medical physics and radiation protection. Its functionality and modeling capabilities continue to be extended, while its performance is enhanced. An overview of recent developments in diverse areas of the toolkit is presented. These include performance optimization for complex setups; improvements for the propagation in fields; new options for event biasing; and additions and improvements in geometry, physics processes and interactive capabilities.",10.1109/TNS.2006.869826,52,yes,no,yes,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,no,no,yes,no,no,no,no,no,no,yes,yes,no,yes,yes,no,
J,"Chen, W; Westerhoff, P; Leenheer, JA; Booksh, K",Fluorescence excitation - Emission matrix regional integration to quantify spectra for dissolved organic matter,"Excitation-emission matrix (EEM) fluorescence spectroscopy has been widely used to characterize dissolved organic matter (DOM) in water and soil. However, interpreting the >10,000 wave length-dependent fluorescence intensity data points represented in EEMs has posed a significant challenge. Fluorescence regional integration, a quantitative technique that integrates the volume beneath an EEM, was developed to analyze EEMs. EEMs were delineated into five excitation-emission regions based on fluorescence of model compounds, DOM fractions, and marine waters or freshwaters. Volumetric integration under the EEM within each region, normalized to the projected excitation-emission area within that region and dissolved organic carbon concentration, resulted in a normalized region-specific EEM volume (Phi(i,n)). Solid-state carbon nuclear magnetic resonance (C-13 NMR), Fourier transform infrared (FTIR) analysis, ultraviolet-visible absorption spectra, and EEMs were obtained for standard Suwannee River fulvic acid and 15 hydrophobic or hydrophilic acid, neutral, and base DOM fractions plus nonfractionated DOM from wastewater effluents and rivers in the southwestern United States. DOM fractions fluoresced in one or more EEM regions. The highest cumulative EEM volume (Phi(T,n) = SigmaPhi(i,n)) was observed for hydrophobic neutral DOM fractions, followed by lower Phi(T,n) values for hydrophobic acid, base, and hydrophilic acid DOM fractions, respectively. An extracted wastewater biomass DOM sample contained aromatic protein- and humic-like material and was characteristic of bacterial-soluble microbial products. Aromatic carbon and the presence of specific aromatic compounds (as indicated by solid-state C-13 NMR and FTIR data) resulted in EEMs that aided in differentiating wastewater effluent DOM from drinking water DOM.",10.1021/es034354c,53,yes,no,no,yes,no,no,no,no,yes,no,yes,yes,yes,yes,no,yes,yes,yes,no,yes,no,no,yes,yes,yes,no,no,no,no,yes,no,
J,"Geuzaine, C; Remacle, JF",Gmsh: A 3-D finite element mesh generator with built-in pre- and post-processing facilities,"Gmsh is an open-source 3-D finite element grid generator with a build-in CAD engine and post-processor. Its design goal is to provide a fast, light and user-friendly meshing tool with parametric input and advanced visualization capabilities. This paper presents the overall philosophy, the main design choices and some of the original algorithms implemented in Gmsh. Copyright (C) 2009 John Wiley & Sons, Ltd.",10.1002/nme.2579,54,yes,yes,no,no,yes,no,yes,yes,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,no,
J,"Candes, EJ; Tao, T",Near-optimal signal recovery from random projections: Universal encoding strategies?,"Suppose we are given a vector f in a class F subset of R-N e.g., a class of digital signals or digital images. How many linear measurements do we need to make about f to be able to recover f to within precision is an element of in the Euclidean (l(2)) metric? This paper shows that if the objects of interest are sparse in a fixed basis or compressible, then it is possible to reconstruct f to within very high accuracy from a small number of random measurements by solving a simple linear program. More precisely, suppose that the,nth largest entry of the vector If I (or of its coefficients in a fixed basis) obeys vertical bar f vertical bar((n)) < R (.) n(-1/P), where R > 0 and p > 0. Suppose that we take measurements y(k) = < f, X-k >; k = 1..., K, where the X-k are N-dimensional Gaussian vectors with independent standard normal entries. Then for each f obeying the decay estimate above for some 0 < p < 1 and with overwhelming probability, our reconstruction f(#) defined as the solution to the constraints y(k) = < f(#), X-k > with minimal l(1) norm, obeys parallel to f - f(#)parallel to(l2) <= C-p (.) R (.) (K/log N)(-r), r = 1/p - 1/2 There is a sense in which this result is optimal; it is generally impossible to obtain a higher accuracy from any set of K measurements whatsoever. The methodology extends to various other random measurement ensembles; for example, we show that similar results hold if one observes a few randomly sampled Fourier coefficients of f. In fact, the results are quite general and require only two hypotheses on the measurement ensemble which are detailed.",10.1109/TIT.2006.885507,55,yes,yes,no,no,yes,yes,yes,yes,no,no,yes,no,no,no,yes,yes,yes,yes,yes,no,no,no,yes,yes,yes,yes,yes,no,no,no,no,
J,"Sendonaris, A; Erkip, E; Aazhang, B",User cooperation diversity - Part 1: System description,"Mobile users' data rate and quality of service are limited by the fact that, within the duration of any given call, they experience severe variations in signal attenuation, thereby necessitating the use of some type of diversity. In this two-part paper, we propose a new form of spatial diversity, in which diversity gains are achieved via the cooperation of mobile users. Part I describes the user cooperation strategy, while Part II focuses on implementation issues and performance analysis. Results show that, even though the interuser channel is noisy, cooperation leads not only to an increase in capacity for both users but also to a more robust system, where users' achievable rates are less susceptible to channel variations.",10.1109/TCOMM.2003.818096,56,yes,yes,no,no,yes,no,yes,yes,no,yes,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,
J,"Hughes, TJR; Cottrell, JA; Bazilevs, Y","Isogeometric analysis: CAD, finite elements, NURBS, exact geometry and mesh refinement","The concept of isogeometric analysis is proposed. Basis functions generated from NURBS (Non-Uniform Rational B-Splines) are employed to construct an exact geometric model. For purposes of analysis, the basis is refined and/or its order elevated without changing the geometry or its parameterization. Analogues of finite element h- and p-refinement schemes are presented and a new, more efficient, higher-order concept, k-refinement, is introduced. Refinements are easily implemented and exact geometry is maintained at all levels without the necessity of subsequent communication with a CAD (Computer Aided Design) description. In the context of structural mechanics, it is established that the basis functions are complete with respect to affine transformations, meaning that all rigid body motions and constant strain states are exactly represented. Standard patch tests are likewise satisfied. Numerical examples exhibit optimal rates of convergence for linear elasticity problems and convergence to thin elastic shell solutions. A k-refinement strategy is shown to converge toward monotone solutions for advection-diffusion processes with sharp internal and boundary layers, a very surprising result. It is argued that isogeometric analysis is a viable alternative to standard, polynomial-based, finite element analysis and possesses several advantages. (c) 2005 Elsevier B.V. All rights reserved.",10.1016/j.cma.2004.10.008,57,yes,yes,no,yes,yes,no,no,no,yes,no,yes,yes,no,no,no,yes,yes,yes,no,yes,no,no,yes,yes,yes,yes,no,yes,yes,yes,yes,
J,"Robeson, LM",The upper bound revisited,"The empirical upper bound relationship for membrane separation of gases initially published in 1991 has been reviewed with the myriad of data now presently available. The upper bound correlation follows the relationship P-i = ka(ij)(n), where P-i is the permeability of the fast gas, alpha(ij) (P-i/P-j) is the separation factor, k is referred to as the ""front factor"" and n is the slope of the log-log plot of the noted relationship. Below this line on a plot of log aij versus log P-i, virtually all the experimental data points exist. In spite of the intense investigation resulting in a much larger clataset than the original correlation, the upper bound position has had only minor shifts in position for many gas pairs. Where more significant shifts are observed, they are almost exclusively due to data now in the literature on a series of perfluorinated polymers and involve many of the gas pairs comprising He. The shift observed is primarily due to a change in the front factor, k, whereas the slope of the resultant upper bound relationship remains similar to the prior data correlations. This indicates a different solubility selectivity relationship for perfluorinated polymers compared to hydrocarbon/aromatic polymers as has been noted in the literature. Two additional upper bound relationships are included in this analysis; CO2/N-2 and N-2/CH4. In addition to the perfluorinated polymers resulting in significant upper bound shifts, minor shifts were observed primarily due to polymers exhibiting rigid, glassy structures including ladder-type polymers. The upper bound correlation can be used to qualitatively determine where the permeability process changes from solution-diffusion to Knudsen diffusion. (C) 2008 Elsevier B.V. All rights reserved.",10.1016/j.memsci.2008.04.030,58,yes,no,yes,yes,yes,no,yes,no,yes,no,yes,yes,yes,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,no,yes,no,
J,"Marzetta, TL",Noncooperative Cellular Wireless with Unlimited Numbers of Base Station Antennas,"A cellular base station serves a multiplicity of single-antenna terminals over the same time-frequency interval. Time-division duplex operation combined with reverse-link pilots enables the base station to estimate the reciprocal forward-and reverse-link channels. The conjugate-transpose of the channel estimates are used as a linear precoder and combiner respectively on the forward and reverse links. Propagation, unknown to both terminals and base station, comprises fast fading, log-normal shadow fading, and geometric attenuation. In the limit of an infinite number of antennas a complete multi-cellular analysis, which accounts for inter-cellular interference and the overhead and errors associated with channel-state information, yields a number of mathematically exact conclusions and points to a desirable direction towards which cellular wireless could evolve. In particular the effects of uncorrelated noise and fast fading vanish, throughput and the number of terminals are independent of the size of the cells, spectral efficiency is independent of bandwidth, and the required transmitted energy per bit vanishes. The only remaining impairment is inter-cellular interference caused by re-use of the pilot sequences in other cells ( pilot contamination) which does not vanish with unlimited number of antennas.",10.1109/TWC.2010.092810.091092,59,yes,yes,no,no,yes,no,no,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Zimmerman, RD; Murillo-Sánchez, CE; Thomas, RJ","MATPOWER: Steady-State Operations, Planning, and Analysis Tools for Power Systems Research and Education","MATPOWER is an open-source Matlab-based power system simulation package that provides a high-level set of power flow, optimal power flow (OPF), and other tools targeted toward researchers, educators, and students. The OPF architecture is designed to be extensible, making it easy to add user-defined variables, costs, and constraints to the standard OPF problem. This paper presents the details of the network modeling and problem formulations used by MATPOWER, including its extensible OPF architecture. This structure is used internally to implement several extensions to the standard OPF problem, including piece-wise linear cost functions, dispatchable loads, generator capability curves, and branch angle difference limits. Simulation results are presented for a number of test cases comparing the performance of several available OPF solvers and demonstrating MATPOWER's ability to solve large-scale AC and DC OPF problems.",10.1109/TPWRS.2010.2051168,60,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,no,yes,no,no,no,no,no,no,no,no,no,no,yes,yes,
J,"Scarselli, F; Gori, M; Tsoi, AC; Hagenbuchner, M; Monfardini, G",The Graph Neural Network Model,"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function T(G, n) is an element of R-m that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.",10.1109/TNN.2008.2005605,61,yes,yes,no,yes,yes,no,no,yes,no,no,yes,yes,no,no,no,no,yes,no,yes,no,no,no,no,no,no,yes,yes,no,no,yes,yes,
J,"Han, JQ",From PID to Active Disturbance Rejection Control,"Active disturbance rejection control (ADRC) can be summarized as follows: it inherits from proportional-integral-derivative (PID) the quality that makes it such a success: the error driven, rather than model-based, control law; it takes from modern control theory its best offering: the state observer; it embraces the power of nonlinear feedback and puts it to full use; it is a useful digital control technology developed out of an experimental platform rooted in computer simulations. ADRC is made possible only when control is taken as an experimental science, instead of a mathematical one. It is motivated by the ever increasing demands from industry that requires the control technology to move beyond PID, which has dominated the practice for over 80 years. Specifically, there are four areas of weakness in PID that we strive to address: 1) the error computation; 2) noise degradation in the derivative control; 3) oversimplification and the loss of performance in the control law in the form of a linear weighted sum; and 4) complications brought by the integral control. Correspondingly, we propose four distinct measures: 1) a simple differential equation as a transient trajectory generator; 2) a noise-tolerant tracking differentiator; 3) the nonlinear control laws; and 4) the concept and method of total disturbance estimation and rejection. Together, they form a new set of tools and a new way of control design. Times and again in experiments and on factory floors, ADRC proves to be a capable replacement of PID with unmistakable advantage in performance and practicality, providing solutions to pressing engineering problems of today. With the new outlook and possibilities that ADRC represents, we further believe that control engineering may very well break the hold of classical PID and enter a new era, an era that brings back the spirit of innovations.",10.1109/TIE.2008.2011621,62,yes,no,no,yes,yes,no,yes,yes,no,yes,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Mikolajczyk, K; Schmid, C",A performance evaluation of local descriptors,"In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [3], steerable filters [12], PCA-SIFT [19], differential invariants [20], spin images [21], SIFT [26], complex filters [37], moment invariants [43], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.",10.1109/TPAMI.2005.188,63,yes,no,no,yes,yes,no,yes,yes,no,yes,yes,no,no,no,no,yes,yes,yes,yes,no,yes,no,no,yes,yes,yes,yes,yes,no,yes,yes,
J,"Julier, SJ; Uhlmann, JK",Unscented filtering and nonlinear estimation,"The extended Kalman filter (EKF) is probably the most widely used estimation algorithm for nonlinear systems. However more than 35 years of experience in the estimation community has shown that is difficult to implement, difficult to tune, and only reliable for systems that are almost linear on the time scale of the updates. Many of these difficulties arise from its use of linearization. To overcome this limitation, the unscented transformation (UT) was developed as a method to propagate mean and covariance information through nonlinear transformations. It is more accurate, easier to implement, and uses the same order of calculations as linearization. This paper reviews the motivation, development, use, and implications of the UT.",10.1109/JPROC.2003.823141,64,yes,no,yes,yes,no,no,no,no,yes,no,yes,yes,no,no,no,no,no,no,no,yes,no,no,no,yes,yes,yes,yes,no,yes,yes,,
J,"Saliba, M; Matsui, T; Seo, JY; Domanski, K; Correa-Baena, JP; Nazeeruddin, MK; Zakeeruddin, SM; Tress, W; Abate, A; Hagfeldt, A; Grätzel, M","Cesium-containing triple cation perovskite solar cells: improved stability, reproducibility and high efficiency","Today's best perovskite solar cells use a mixture of formamidinium and methylammonium as the monovalent cations. With the addition of inorganic cesium, the resulting triple cation perovskite compositions are thermally more stable, contain less phase impurities and are less sensitive to processing conditions. This enables more reproducible device performances to reach a stabilized power output of 21.1% and similar to 18% after 250 hours under operational conditions. These properties are key for the industrialization of perovskite photovoltaics.",10.1039/c5ee03874j,65,yes,yes,no,yes,no,no,no,no,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,yes,yes,yes,no,no,no,no,yes,no,
J,"Pérez-Lombard, L; Ortiz, J; Pout, C",A review on buildings energy consumption information,"The rapidly growing world energy use has already raised concerns over supply difficulties, exhaustion of energy resources and heavy environmental impacts (ozone layer depletion, global warming, climate change, etc.). The global contribution from buildings towards energy consumption, both residential and commercial, has steadily increased reaching figures between 20% and 40% in developed countries, and has exceeded the other major sectors: industrial and transportation. Growth in population, increasing demand for building services and comfort levels, together with the rise in time spent inside buildings, assure the upward trend in energy demand will continue in the future. For this reason, energy efficiency in buildings is today a prime objective for energy policy at regional, national and international levels. Among building services, the growth in HVAC systems energy use is particularly significant (50% of building consumption and 20% of total consumption in the USA). This paper analyses available information concerning energy consumption in buildings, and particularly related to HVAC systems. Many questions arise: Is the necessary information available? Which are the main building types? What end uses should be considered in the breakdown? Comparisons between different countries are presented specially for commercial buildings. The case of offices is analysed in deeper detail. (c) 2007 Elsevier B.V. All rights reserved.",10.1016/j.enbuild.2007.03.007,66,yes,no,yes,no,yes,no,no,no,yes,yes,yes,yes,no,no,no,no,no,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,
J,"Rodríguez, J; Lai, JS; Peng, FZ","Multilevel inverters:: A survey of topologies, controls, and applications","Multilevel inverter technology has emerged recently as a very important alternative in the area of high-power medium-voltage energy control. This paper presents the most important topologies like diode-clamped inverter (neutral-point clamped), capacitor-clamped (flying capacitor), and cascaded multicell with separate dc sources. Emerging topologies like asymmetric hybrid cells and soft-switched multilevel inverters are also discussed. This paper also presents the most relevant control and modulation methods developed for this family of converters: multilevel sinusoidal pulsewidth modulation, multilevel selective harmonic elimination, and space-vector modulation. Special attention is dedicated to the latest and more relevant applications of these converters such as laminators, conveyor belts, and unified power-How controllers. The need of an active front end at the input side for those inverters supplying regenerative loads is also discussed, and the circuit topology options are also presented. Finally, the peripherally developing areas such as high-voltage high-power devices and optical sensors and other opportunities for future development are addressed.",10.1109/TIE.2002.801052,67,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,no,no,yes,yes,no,yes,yes,yes,
J,"Henriques, JF; Caseiro, R; Martins, P; Batista, J",High-Speed Tracking with Kernelized Correlation Filters,"The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies-any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the discrete Fourier transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new kernelized correlation filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call dual correlation filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source.",10.1109/TPAMI.2014.2345390,68,yes,no,yes,yes,yes,no,yes,no,yes,no,yes,yes,yes,no,no,no,yes,yes,yes,no,no,no,yes,yes,yes,yes,no,yes,yes,yes,yes,
J,"He, KM; Sun, J; Tang, XO",Single Image Haze Removal Using Dark Channel Prior,"In this paper, we propose a simple but effective image prior-dark channel prior to remove haze from a single input image. The dark channel prior is a kind of statistics of outdoor haze-free images. It is based on a key observation-most local patches in outdoor haze-free images contain some pixels whose intensity is very low in at least one color channel. Using this prior with the haze imaging model, we can directly estimate the thickness of the haze and recover a high-quality haze-free image. Results on a variety of hazy images demonstrate the power of the proposed prior. Moreover, a high-quality depth map can also be obtained as a byproduct of haze removal.",10.1109/TPAMI.2010.168,69,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,no,no,no,yes,yes,,
J,"Belongie, S; Malik, J; Puzicha, J",Shape matching and object recognition using shape contexts,"We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by 1) solving for correspondences between points on the two shapes, 2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits, and the COIL data set.",10.1109/34.993558,70,yes,yes,no,no,yes,yes,yes,yes,no,no,yes,yes,yes,no,no,yes,yes,no,yes,no,yes,no,no,yes,yes,no,no,no,no,yes,yes,
J,"Hutmacher, DW",Scaffolds in tissue engineering bone and cartilage,"Musculoskeletal tissue, bone and cartilage are under extensive investigation in tissue engineering research. A number of biodegradable and bioresorbable materials, as well as scaffold designs, have been experimentally and/or clinically studied. Ideally, a scaffold should have the following characteristics: (i) three-dimensional and highly porous with an interconnected pore network for cell growth and flow transport of nutrients and metabolic waste; (ii) biocompatible and bioresorbable with a controllable degradation and resorption rate to match cell/tissue growth in vitro and/or in vivo; (iii) suitable surface chemistry for cell attachment, proliferation, and differentation and (iv) mechanical properties to match those of the tissues at the site of implantation. This paper reviews research on the tissue engineering of bone and cartilage from the polymeric scaffold point of view. (C) 2000 Elsevier Science Ltd. All rights reserved.",10.1016/S0142-9612(00)00121-6,71,yes,no,yes,yes,no,no,no,no,yes,no,yes,yes,no,no,no,no,no,no,no,yes,no,no,no,no,no,no,no,no,no,no,no,
J,"Kschischang, FR; Frey, BJ; Loeliger, HA",Factor graphs and the sum-product algorithm,"Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of ""local"" functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph. In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph, Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative ""turbo"" decoding algorithm, Pearl's belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms.",10.1109/18.910572,72,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,no,no,no,no,yes,yes,yes,yes,
J,"Wang, Z; Bovik, AC",A universal image quality index,"We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http:Hanchovy.ece.utexas.edu/-zwang/research/quaIity-index/demo.html.",10.1109/97.995823,73,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,
J,"He, KM; Sun, J; Tang, XO",Guided Image Filtering,"In this paper, we propose a novel explicit image filter called guided filter. Derived from a local linear model, the guided filter computes the filtering output by considering the content of a guidance image, which can be the input image itself or another different image. The guided filter can be used as an edge-preserving smoothing operator like the popular bilateral filter [1], but it has better behaviors near edges. The guided filter is also a more generic concept beyond smoothing: It can transfer the structures of the guidance image to the filtering output, enabling new filtering applications like dehazing and guided feathering. Moreover, the guided filter naturally has a fast and nonapproximate linear time algorithm, regardless of the kernel size and the intensity range. Currently, it is one of the fastest edge-preserving filters. Experiments show that the guided filter is both effective and efficient in a great variety of computer vision and computer graphics applications, including edge-aware smoothing, detail enhancement, HDR compression, image matting/ feathering, dehazing, joint upsampling, etc.",10.1109/TPAMI.2012.213,74,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,
J,"Elad, M; Aharon, M",Image denoising via sparse and redundant representations over learned dictionaries,"We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods.",10.1109/TIP.2006.881969,75,yes,yes,no,no,yes,yes,yes,no,no,no,yes,yes,yes,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,no,no,yes,yes,
J,"Yang, JC; Wright, J; Huang, TS; Ma, Y",Image Super-Resolution Via Sparse Representation,"This paper presents a new approach to single-image superresolution, based upon sparse signal representation. Research on image statistics suggests that image patches can be well-represented as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. Inspired by this observation, we seek a sparse representation for each patch of the low-resolution input, and then use the coefficients of this representation to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild conditions, the sparse representation can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low-and high-resolution image patches, we can enforce the similarity of sparse representations between the low-resolution and high-resolution image patch pair with respect to their own dictionaries. Therefore, the sparse representation of a low-resolution image patch can be applied with the high-resolution image patch dictionary to generate a high-resolution image patch. The learned dictionary pair is a more compact representation of the patch pairs, compared to previous approaches, which simply sample a large amount of image patch pairs [1], reducing the computational cost substantially. The effectiveness of such a sparsity prior is demonstrated for both general image super-resolution (SR) and the special case of face hallucination. In both cases, our algorithm generates high-resolution images that are competitive or even superior in quality to images produced by other similar SR methods. In addition, the local sparse modeling of our approach is naturally robust to noise, and therefore the proposed algorithm can handle SR with noisy inputs in a more unified framework.",10.1109/TIP.2010.2050625,76,yes,no,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,yes,
J,"Pope, CA; Dockery, DW",Health effects of fine particulate air pollution: Lines that connect,"Efforts to understand and mitigate the health effects of particulate matter (PM) air pollution have a rich and interesting history. This review focuses on six substantial lines of research that have been pursued since 1997 that have helped elucidate our understanding about the effects of PM on human health. There has been substantial progress in the evaluation of PM health effects at different time-scales of exposure and in the exploration of the shape of the concentration-response function. There has also been emerging evidence of PM-related cardiovascular health effects and growing knowledge regarding interconnected general pathophysiological pathways that link PM exposure with cardiopulmonary morbidity and mortality. Despite important gaps in scientific knowledge and continued reasons for some skepticism, a comprehensive evaluation of the research findings provides persuasive evidence that exposure to fine particulate air pollution has adverse effects on cardiopulmonary health. Although much of this research has been motivated by environmental public health policy, these results have important scientific, medical, and public health implications that are broader than debates over legally mandated air quality standards.",10.1080/10473289.2006.10464485,77,yes,yes,no,yes,yes,no,yes,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,no,no,no,yes,yes,,
J,"Ferretti, A; Prati, C; Rocca, F",Permanent scatterers in SAR interferometry,"Temporal and geometrical decorrelation often prevents SAR interferometry from being an operational tool for surface deformation monitoring and topographic profile reconstruction. Moreover, atmospheric disturbances can strongly compromise the accuracy of the results. In this paper, we present a complete procedure for the identification and exploitation of stable natural reflectors or permanent scatterers (PSs) starting from long temporal series of interferometric SAR images. When, as it often happens, the dimension of the PS is smaller than the resolution cell, the coherence is goad even for interferograms with baselines larger than the decorrelation one, and all the available images of the ESA ERS data set can be successfully exploited. On these pixels, submeter DEM accuracy and millimetric terrain motion detection can be achieved, since atmospheric phase screen (APS) contributions can be estimated and removed. Examples are then shown of small motion measurements, DEM refinement, and BPS estimation and removal in the case of a sliding area in Ancona, Italy ERS data have been used.",10.1109/36.898661,78,yes,no,yes,no,yes,no,yes,no,yes,no,yes,yes,yes,no,no,yes,yes,no,no,yes,yes,yes,yes,yes,yes,yes,yes,no,yes,yes,yes,
J,"Felzenszwalb, PF; Girshick, RB; McAllester, D; Ramanan, D",Object Detection with Discriminatively Trained Part-Based Models,"We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.",10.1109/TPAMI.2009.167,79,yes,yes,no,yes,yes,no,yes,no,yes,no,yes,yes,yes,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,no,yes,yes,no,yes,yes,
J,"Akyildiz, IF; Lee, WY; Vuran, MC; Mohanty, S",NeXt generation/dynamic spectrum access/cognitive radio wireless networks: A survey,"Today's wireless networks are characterized by a fixed spectrum assignment policy. However, a large portion of the assigned spectrum is used sporadically and geographical variations in the utilization of assigned spectrum ranges from 15% to 85% with a high variance in time. The limited available spectrum and the inefficiency in the spectrum usage necessitate a new communication paradigm to exploit the existing wireless spectrum opportunistically. This new networking paradigm is referred to as NeXt Generation (xG) Networks as well as Dynamic Spectrum Access (DSA) and cognitive radio networks. The term xG networks is used throughout the paper. The novel functionalities and current research challenges of the xG networks are explained in detail. More specifically, a brief overview of the cognitive radio technology is provided and the xG network architecture is introduced. Moreover, the xG network functions such as spectrum management, spectrum mobility and spectrum sharing are explained in detail. The influence of these functions on the performance of the upper layer protocols such as routing and transport are investigated and open research issues in these areas are also outlined. Finally, the cross-layer design challenges in xG networks are discussed. (c) 2006 Elsevier B.V. All rights reserved.",10.1016/j.comnet.2006.05.001,80,yes,no,yes,yes,yes,no,yes,no,no,yes,yes,no,no,no,no,no,no,no,no,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,no,
J,"Ahonen, T; Hadid, A; Pietikäinen, M",Face description with local binary patterns:: Application to face recognition,This paper presents a novel and efficient facial image representation based on local binary pattern (LBP) texture features. The face image is divided into several regions from which the LBP feature distributions are extracted and concatenated into an enhanced feature vector to be used as a face descriptor. The performance of the proposed method is assessed in the face recognition problem under different challenges. Other applications and several extensions are also discussed.,10.1109/TPAMI.2006.244,81,yes,yes,no,no,yes,yes,yes,yes,no,no,yes,no,no,no,no,yes,yes,no,yes,no,no,no,no,no,yes,yes,no,yes,yes,no,yes,
J,"Potyondy, DO; Cundall, PA",A bonded-particle model for rock,"A numerical model for rock is proposed in which the rock is represented by a dense packing of non-uniformized circular or spherical particles that are bonded together at their contact points and whose mechanical behavior is simulated by the distinct-element method Using the two- and three-dimensional discontinuum programs PFC2D and PFC3D. The microproperties consist of stiffness and strength parameters for the particles and the bonds. Damage is represented explicitly as broken bonds. which form, and coalesce into macroscopic fractures when load is applied. The model reproduces many features of rock behavior includinq elasticity fracturing, acoustic emission, damage accumulation producing material anisotropy, hysteresis, dilation. post-peak softening and strength increase with confinement. These behaviors are emergent properties of the model that arise from a relatively simple Set of microproperties. A material-,genesis procedure and microproperties to represent Lac du Bonnet granite are presented. The behaviour of this model is described for two- and three-dimensional biaxial. triaxial and Brazilian tests and for two-dimensional tunnel simulations in which breakout notches form in the region of maximum compressive stress. The sensitivity of the to microproperties, including particle size, is investigated. Particle size is not a free paranieter that only controls resolution: instead. it affects the fracture toughness and thereby influences damage processes (such as notch formation) in which damage localizes at macrofracture tips experiencing extensile loading. (C) 2004 Elsevier Ltd. All rights reserved.",10.1016/j.ijrmms.2004.09.011,82,yes,no,no,no,yes,no,yes,no,yes,no,yes,yes,yes,yes,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,no,no,yes,yes,,
J,"Tustison, NJ; Avants, BB; Cook, PA; Zheng, YJ; Egan, A; Yushkevich, PA; Gee, JC",N4ITK: Improved N3 Bias Correction,"A variant of the popular nonparametric nonuniform intensity normalization (N3) algorithm is proposed for bias field correction. Given the superb performance of N3 and its public availability, it has been the subject of several evaluation studies. These studies have demonstrated the importance of certain parameters associated with the B-spline least-squares fitting. We propose the substitution of a recently developed fast and robust B-spline approximation routine and a modified hierarchical optimization scheme for improved bias field correction over the original N3 algorithm. Similar to the N3 algorithm, we also make the source code, testing, and technical documentation of our contribution, which we denote as ""N4ITK,"" available to the public through the Insight Toolkit of the National Institutes of Health. Performance assessment is demonstrated using simulated data from the publicly available Brainweb database, hyperpolarized He-3 lung image data, and 9.4T postmortem hippocampus data.",10.1109/TMI.2010.2046908,83,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,yes,no,no,no,yes,no,no,no,no,no,no,no,no,yes,yes,no,yes,yes,yes,
J,"Zhang, L; Zhang, L; Mou, XQ; Zhang, D",FSIM: A Feature Similarity Index for Image Quality Assessment,"Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.",10.1109/TIP.2011.2109730,84,yes,no,yes,yes,yes,no,yes,no,yes,no,yes,yes,no,no,no,yes,yes,yes,no,yes,no,no,yes,yes,yes,no,yes,no,yes,yes,,
J,"Olfati-Saber, R",Flocking for multi-agent dynamic systems: Algorithms and theory,"In this paper, we present a theoretical framework for design and analysis of distributed flocking algorithms. Two cases of flocking in free-space and presence of multiple obstacles are considered. We present three flocking algorithms: two for free-flocking and one for constrained flocking. A comprehensive analysis of the first two algorithms is provided. We demonstrate the first algorithm embodies all three rules of Reynolds. This is a formal approach to extraction of interaction rules that lead to the emergence of collective behavior. We show that the first algorithm generically leads to regular fragmentation, whereas the second and third algorithms both lead to flocking. A systematic method is provided for construction of cost functions (or collective potentials) for flocking. These collective potentials penalize deviation from a class of lattice-shape objects called alpha-lattices. We use a multi-species framework for construction of collective potentials that consist of flock-members, or alpha-agents, and virtual agents associated with alpha-agents called beta- and gamma-agents. We show that migration of flocks can be performed using a peer-to-peer network of agents, i.e., ""flocks need no leaders."" A ""universal"" definition of flocking for particle systems with similarities to Lyapunov stability is given. Several simulation results are provided that demonstrate performing 2-D and 3-D flocking, split/rejoin maneuver, and squeezing maneuver for hundreds of agents using the proposed algorithms.",10.1109/TAC.2005.864190,85,yes,no,no,no,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,no,yes,yes,yes,
J,"Arbeláez, P; Maire, M; Fowlkes, C; Malik, J",Contour Detection and Hierarchical Image Segmentation,"This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.",10.1109/TPAMI.2010.161,86,yes,yes,no,no,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,no,yes,yes,no,no,no,yes,yes,
J,"Henseler, J; Hubona, G; Ray, PA",Using PLS path modeling in new technology research: updated guidelines,"Purpose - Partial least squares (PLS) path modeling is a variance-based structural equation modeling (SEM) technique that is widely applied in business and social sciences. Its ability to model composites and factors makes it a formidable statistical tool for new technology research. Recent reviews, discussions, and developments have led to substantial changes in the understanding and use of PLS. The paper aims to discuss these issues. Design/methodology/approach - This paper aggregates new insights and offers a fresh look at PLS path modeling. It presents new developments, such as consistent PLS, confirmatory composite analysis, and the heterotrait-monotrait ratio of correlations. Findings - PLS path modeling is the method of choice if a SEM contains both factors and composites. Novel tests of exact fit make a confirmatory use of PLS path modeling possible. Originality/value - This paper provides updated guidelines of how to use PLS and how to report and interpret its results.",10.1108/IMDS-09-2015-0382,87,yes,no,yes,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,yes,yes,no,no,no,no,no,yes,yes,yes,no,yes,yes,,
J,"Avants, BB; Epstein, CL; Grossman, M; Gee, JC",Symmetric diffeomorphic image registration with cross-correlation: Evaluating automated labeling of elderly and neurodegenerative brain,"One of the most challenging problems in modern neuroimaging is detailed characterization of neurodegeneration. Quantifying spatial and longitudinal atrophy patterns is an important component of this process. These spatiotemporal signals will aid in discriminating between related diseases, such as frontotemporal dementia (FTD) and Alzheimer's disease (AD), which manifest themselves in the same at-risk population. Here, we develop a novel symmetric image normalization method (SyN) for maximizing the cross-correlation within the space of diffeomorphic maps and provide the Euler-Lagrange equations necessary for this optimization. We then turn to a careful evaluation of our method. Our evaluation uses gold standard, human cortical segmentation to contrast SyN's performance with a related elastic method and with the standard ITK implementation of Thirion's Demons algorithm. The new method compares favorably with both approaches, in particular when the distance between the template brain and the target brain is large. We then report the correlation of volumes gained by algorithmic cortical labelings of FTD and control subjects with those gained by the manual rater. This comparison shows that, of the three methods tested, SyN's volume measurements are the most strongly correlated with volume measurements gained by expert labeling. This study indicates that SyN, with cross-correlation, is a reliable method for normalizing and making anatomical measurements in volumetric MRI of patients and at-risk elderly individuals. Published by Elsevier B.V.",10.1016/j.media.2007.06.004,88,yes,no,yes,no,yes,no,yes,no,yes,yes,yes,yes,yes,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,,
J,"Blaabjerg, F; Teodorescu, R; Liserre, M; Timbus, AV",Overview of control and grid synchronization for distributed power generation systems,"Renewable energy sources like wind, sun, and hydro are seen as a reliable alternative to the traditional energy sources such as oil, natural gas, or coal. Distributed power generation systems (DPGSs) based on renewable energy sources experience a large development worldwide, with Germany, Denmark, Japan, and USA as leaders in the development in this field. Due to the increasing number of DPGSs connected to the utility network, new and stricter standards in respect to power quality, safe running, and islanding protection are issued. As a consequence, the control of distributed generation systems should be improved to meet the requirements for grid interconnection. This paper gives an overview of the structures for the DPGS based on fuel cell, photovoltaic, and wind turbines. In addition, control structures of the grid-side converter are presented, and the possibility of compensation for low-order harmonics is also discussed. Moreover, control strategies when running on grid faults are treated. This paper ends up with an overview of synchronization methods and a discussion about their importance in the control.",10.1109/TIE.2006.881997,89,yes,no,yes,yes,yes,no,yes,yes,no,no,yes,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,yes,no,
J,"Kanungo, T; Mount, DM; Netanyahu, NS; Piatko, CD; Silverman, R; Wu, AY",An efficient <i>k</i>-means clustering algorithm:: Analysis and implementation,"In k-means clustering, we are given a set of n data points in d-dimensional space R-d and an integer k and the problem is to determine a set of k points in R-d, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's algorithm. In this paper, we present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation.",10.1109/TPAMI.2002.1017616,90,yes,yes,no,yes,yes,no,yes,yes,no,no,yes,no,yes,no,no,no,yes,yes,yes,no,no,no,no,no,no,no,no,yes,yes,yes,yes,
J,"Cootes, TF; Edwards, GJ; Taylor, CJ",Active appearance models,We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors.,10.1109/34.927467,91,yes,yes,no,no,yes,yes,yes,yes,no,no,yes,no,yes,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,no,yes,yes,
J,"Mittal, A; Soundararajan, R; Bovik, AC","Making a ""Completely Blind"" Image Quality Analyzer","An important aim of research on the blind image quality assessment (IQA) problem is to devise perceptual models that can predict the quality of distorted images with as little prior knowledge of the images or their distortions as possible. Current state-of-the-art ""general purpose"" no reference (NR) IQA algorithms require knowledge about anticipated distortions in the form of training examples and corresponding human opinion scores. However we have recently derived a blind IQA model that only makes use of measurable deviations from statistical regularities observed in natural images, without training on human-rated distorted images, and, indeed without any exposure to distorted images. Thus, it is ""completely blind."" The new IQA model, which we call the Natural Image Quality Evaluator (NIQE) is based on the construction of a ""quality aware"" collection of statistical features based on a simple and successful space domain natural scene statistic (NSS) model. These features are derived from a corpus of natural, undistorted images. Experimental results show that the new index delivers performance comparable to top performing NR IQA models that require training on large databases of human opinions of distorted images. A software release is available at http://live.ece.utexas.edu/research/quality/niqe_release.zip.",10.1109/LSP.2012.2227726,92,yes,no,no,yes,yes,no,yes,yes,no,no,yes,no,yes,no,no,no,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,,
J,"Georghiades, AS; Belhumeur, PN; Kriegman, DJ",From few to many: Illumination cone models for face recognition under variable lighting and pose,"We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render-or synthesize-images of the face under novel poses and illumination conditions. The pose space is then sampled and, for each pose. the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone (based on Euclidean distance within the image space). We test our face recognition method on 4,050 images from the Yale Face Database B; these images contain 405 viewing conditions (9 poses x 45 illumination conditions) for 10 individuals. The method performs almost without error, except on the most extreme lighting directions, and significantly outperforms popular recognition methods that do not use a generative model.",10.1109/34.927464,93,yes,yes,no,no,yes,yes,yes,yes,no,no,yes,no,yes,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,no,yes,yes,
J,"Berardino, P; Fornaro, G; Lanari, R; Sansosti, E",A new algorithm for surface deformation monitoring based on small baseline differential SAR interferograms,"We present a new differential synthetic aperture radar (SAR) interferometry algorithm for monitoring the temporal evolution of surface deformations. The presented technique is based on an appropriate combination of differential interferograms produced by data pairs characterized by a small orbital separation (baseline) in order to limit the spatial decorrelation phenomena. The application of the singular value decomposition method allows us to easily ""link"" independent SAR acquisition datasets, separated by large baselines, thus increasing the observation temporal sampling rate. The availability of both spatial and temporal information in the processed data is used to identify and filter out atmospheric phase artifacts. We present results obtained on the data acquired from 1992 to 2000 by the European Remote Sensing satellites and relative to the Campi Flegrei caldera and to the city of Naples, Italy that demonstrate the capability of the proposed approach to follow the dynamics of the detected deformations.",10.1109/TGRS.2002.803792,94,yes,yes,no,yes,yes,yes,yes,yes,no,no,yes,no,yes,no,yes,no,yes,yes,yes,no,no,no,no,yes,yes,no,yes,no,yes,yes,yes,
J,"Mittal, A; Moorthy, AK; Bovik, AC",No-Reference Image Quality Assessment in the Spatial Domain,"We propose a natural scene statistic-based distortion-generic blind/no-reference (NR) image quality assessment (IQA) model that operates in the spatial domain. The new model, dubbed blind/referenceless image spatial quality evaluator (BRISQUE) does not compute distortion-specific features, such as ringing, blur, or blocking, but instead uses scene statistics of locally normalized luminance coefficients to quantify possible losses of ""naturalness"" in the image due to the presence of distortions, thereby leading to a holistic measure of quality. The underlying features used derive from the empirical distribution of locally normalized luminances and products of locally normalized luminances under a spatial natural scene statistic model. No transformation to another coordinate frame (DCT, wavelet, etc.) is required, distinguishing it from prior NR IQA approaches. Despite its simplicity, we are able to show that BRISQUE is statistically better than the full-reference peak signal-to-noise ratio and the structural similarity index, and is highly competitive with respect to all present-day distortion-generic NR IQA algorithms. BRISQUE has very low computational complexity, making it well suited for real time applications. BRISQUE features may be used for distortion-identification as well. To illustrate a new practical application of BRISQUE, we describe how a nonblind image denoising algorithm can be augmented with BRISQUE in order to perform blind image denoising. Results show that BRISQUE augmentation leads to performance improvements over state-of-the-art methods. A software release of BRISQUE is available online: http://live.ece.utexas.edu/research/quality/BRISQUE_release.zip for public use and evaluation.",10.1109/TIP.2012.2214050,95,yes,no,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,,
J,"Shin, HC; Roth, HR; Gao, MC; Lu, L; Xu, ZY; Nogues, I; Yao, JH; Mollura, D; Summers, RM","Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning","Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.",10.1109/TMI.2016.2528162,96,yes,no,yes,yes,yes,no,yes,yes,no,no,yes,no,yes,no,no,no,yes,yes,yes,no,no,no,yes,yes,yes,no,yes,yes,yes,yes,,
J,"Larsson, EG; Edfors, O; Tufvesson, F; Marzetta, TL",Massive MIMO for Next Generation Wireless Systems,"Multi-user MIMO offers big advantages over conventional point-to-point MIMO: it works with cheap single-antenna terminals, a rich scattering environment is not required, and resource allocation is simplified because every active terminal utilizes all of the time-frequency bins. However, multi-user MIMO, as originally envisioned, with roughly equal numbers of service antennas and terminals and frequency-division duplex operation, is not a scalable technology. Massive MIMO (also known as large-scale antenna systems, very large MIMO, hyper MIMO, full-dimension MIMO, and ARGOS) makes a clean break with current practice through the use of a large excess of service antennas over active terminals and time-division duplex operation. Extra antennas help by focusing energy into ever smaller regions of space to bring huge improvements in throughput and radiated energy efficiency. Other benefits of massive MIMO include extensive use of inexpensive low-power components, reduced latency, simplification of the MAC layer, and robustness against intentional jamming. The anticipated throughput depends on the propagation environment providing asymptotically orthogonal channels to the terminals, but so far experiments have not disclosed any limitations in this regard. While massive MIMO renders many traditional research problems irrelevant, it uncovers entirely new problems that urgently need attention: the challenge of making many low-cost low-precision components that work effectively together, acquisition and synchronization for newly joined terminals, the exploitation of extra degrees of freedom provided by the excess of service antennas, reducing internal power consumption to achieve total energy efficiency reductions, and finding new deployment scenarios. This article presents an overview of the massive MIMO concept and contemporary research on the topic.",10.1109/MCOM.2014.6736761,97,yes,no,no,yes,yes,no,yes,yes,no,no,yes,no,no,no,no,yes,yes,yes,yes,no,no,no,no,yes,yes,yes,yes,yes,yes,yes,,
J,"Guerrero, JM; Vasquez, JC; Matas, J; de Vicuña, LG; Castilla, M",Hierarchical Control of Droop-Controlled AC and DC Microgrids-A General Approach Toward Standardization,"AC and dc microgrids (MGs) are key elements for integrating renewable and distributed energy resources as well as distributed energy-storage systems. In the last several years, efforts toward the standardization of these MGs have been made. In this sense, this paper presents the hierarchical control derived from ISA-95 and electrical dispatching standards to endow smartness and flexibility to MGs. The hierarchical control proposed consists of three levels: 1) The primary control is based on the droop method, including an output-impedance virtual loop; 2) the secondary control allows the restoration of the deviations produced by the primary control; and 3) the tertiary control manages the power flow between the MG and the external electrical distribution system. Results from a hierarchical-controlled MG are provided to show the feasibility of the proposed approach.",10.1109/TIE.2010.2066534,98,yes,no,yes,no,yes,no,yes,yes,no,no,yes,no,no,no,no,no,yes,no,yes,no,no,no,no,no,no,no,yes,no,no,no,yes,no
J,"Greff, K; Srivastava, RK; Koutník, J; Steunebrink, BR; Schmidhuber, J",LSTM: A Search Space Odyssey,"Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs (approximate to 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.",10.1109/TNNLS.2016.2582924,99,yes,no,yes,yes,yes,no,yes,no,yes,no,yes,yes,yes,no,no,no,yes,yes,yes,no,yes,yes,yes,yes,yes,yes,yes,no,yes,yes,yes,
